"""
JavaScriptæ–‡ä»¶çˆ¬å–å’Œåæ··æ·†ä¸»ç¨‹åº
"""
import sys
import time
import argparse
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime

from .config import ensure_directories, get_directory_structure, SAVE_CHECKPOINT_INTERVAL, BROWSER_ENGINE, USE_EMBEDDED_BROWSER
from ..utils.logger import setup_logger, get_logger
from ..crawlers.static_crawler import StaticJSCrawler
from ..crawlers.dynamic_crawler import DynamicJSCrawler
from .deobfuscator import JSDeobfuscator
from ..utils.utils import save_checkpoint, load_checkpoint, format_file_size

# æ ¹æ®é…ç½®é€‰æ‹©æµè§ˆå™¨å¼•æ“
if USE_EMBEDDED_BROWSER and BROWSER_ENGINE == "playwright":
    try:
        from ..crawlers.playwright_crawler import PlaywrightCrawler
        PLAYWRIGHT_AVAILABLE = True
    except ImportError:
        PLAYWRIGHT_AVAILABLE = False
        logger = get_logger("main")
        logger.warning("Playwrightæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ä¼ ç»ŸSeleniumæ–¹å¼")
else:
    PLAYWRIGHT_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
setup_logger()
logger = get_logger("main")

class JSCrawler:
    """JavaScriptçˆ¬å–å™¨ - é¢å‘ç”¨æˆ·çš„ä¸»è¦æ¥å£"""
    
    def __init__(self, target_url: str):
        self.target_url = target_url
        self.dirs = get_directory_structure(target_url)
        self.output_dir = self.dirs['target_output_dir']  # æ·»åŠ output_dirå±æ€§
        self.manager = JSCrawlerManager(target_url)
    
    def crawl(self, depth: int = 2, wait_time: int = 3, max_workers: int = 2, resume: bool = False) -> Dict[str, Any]:
        """æ‰§è¡Œçˆ¬å–æ“ä½œ"""
        return self.manager.run(
            url=self.target_url,
            max_depth=depth,
            wait_time=wait_time,
            max_workers=max_workers,
            resume=resume
        )

class JSCrawlerManager:
    """JavaScriptçˆ¬å–ç®¡ç†å™¨"""
    
    def __init__(self, target_url: str):
        self.target_url = target_url
        self.dirs = get_directory_structure(target_url)
        ensure_directories(target_url)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.checkpoint_file = self.dirs['checkpoints_dir'] / 'crawler_checkpoint.json'
        self.start_time = time.time()
        # ä¼ é€’output_dirå‚æ•°ç»™çˆ¬è™«ï¼Œä»¥ä¾¿ç”ŸæˆæŠ¥å‘Š
        self.static_crawler = StaticJSCrawler(target_url, self.dirs['target_output_dir'])
        self.dynamic_crawler = DynamicJSCrawler(target_url, self.dirs['target_output_dir'])
        self.deobfuscator = JSDeobfuscator()
        self.checkpoint_data = {}
        
    def load_checkpoint(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®"""
        try:
            checkpoint = load_checkpoint(self.checkpoint_file)
            if checkpoint:
                logger.info("å‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå°†ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­")
                return checkpoint
        except Exception as e:
            logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return None
    
    def save_checkpoint(self, data: Dict[str, Any]) -> bool:
        """ä¿å­˜æ£€æŸ¥ç‚¹æ•°æ®"""
        try:
            self.checkpoint_data.update(data)
            return save_checkpoint(self.checkpoint_data, self.checkpoint_file)
        except Exception as e:
            logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def _is_anti_crawler_detected(self, results: Dict[str, Any]) -> bool:
        """æ£€æµ‹æ˜¯å¦é‡åˆ°åçˆ¬è™«æœºåˆ¶"""
        # å¦‚æœè®¿é—®äº†é¡µé¢ä½†æ²¡æœ‰å‘ç°ä»»ä½•JSæ–‡ä»¶ï¼Œå¯èƒ½æ˜¯åçˆ¬è™«
        if (results.get('visited_pages', 0) > 0 and 
            results.get('total_discovered', 0) == 0 and
            results.get('successful_downloads', 0) == 0):
            return True
        
        # å¦‚æœå‘ç°çš„æ–‡ä»¶æ•°é‡å¾ˆå°‘ä¸”ä¸‹è½½å¤±è´¥ç‡å¾ˆé«˜
        total_discovered = results.get('total_discovered', 0)
        failed_downloads = results.get('failed_downloads', 0)
        if total_discovered > 0 and failed_downloads / total_discovered > 0.8:
            return True
            
        return False
    
    def _is_anti_crawler_error(self, error: Exception) -> bool:
        """æ£€æµ‹æ˜¯å¦æ˜¯åçˆ¬è™«ç›¸å…³çš„é”™è¯¯"""
        error_str = str(error).lower()
        anti_crawler_indicators = [
            '404', '403', '401', 'forbidden', 'not found',
            'access denied', 'blocked', 'captcha', 'verification',
            'too many requests', 'rate limit', 'cloudflare'
        ]
        
        return any(indicator in error_str for indicator in anti_crawler_indicators)
    
    def crawl_static_js(self, url: str, max_depth: int = 2, max_workers: int = 4, resume: bool = False) -> Dict[str, Any]:
        """çˆ¬å–é™æ€JavaScriptæ–‡ä»¶"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹é™æ€JavaScriptæ–‡ä»¶çˆ¬å–")
        logger.info("=" * 60)
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤
            if resume and 'static_completed' in self.checkpoint_data:
                logger.info("é™æ€çˆ¬å–å·²å®Œæˆï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return self.checkpoint_data.get('static_results', {})
            
            # æ‰§è¡Œé™æ€çˆ¬å–
            results = self.static_crawler.crawl_website(url, max_depth, max_workers)
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°åçˆ¬è™«æœºåˆ¶
            if self._is_anti_crawler_detected(results):
                logger.warning("æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶ï¼Œé™æ€çˆ¬å–å¤±è´¥")
                logger.info("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°åŠ¨æ€çˆ¬å–æ¨¡å¼...")
                # æ ‡è®°é™æ€çˆ¬å–å¤±è´¥ï¼Œä½†ä¸ä¿å­˜ä¸ºå®ŒæˆçŠ¶æ€
                self.save_checkpoint({
                    'static_failed_anti_crawler': True,
                    'static_results': results
                })
                return results
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint({
                'static_completed': True,
                'static_results': results
            })
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            logger.info(f"é™æ€çˆ¬å–å®Œæˆ:")
            logger.info(f"  - å‘ç°æ–‡ä»¶æ•°: {results['total_discovered']}")
            logger.info(f"  - æˆåŠŸä¸‹è½½: {results['successful_downloads']}")
            logger.info(f"  - å¤±è´¥ä¸‹è½½: {results['failed_downloads']}")
            logger.info(f"  - è®¿é—®é¡µé¢æ•°: {results['visited_pages']}")
            
            return results
            
        except Exception as e:
            logger.error(f"é™æ€çˆ¬å–å¤±è´¥: {e}")
            # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬è™«ç›¸å…³çš„é”™è¯¯
            if self._is_anti_crawler_error(e):
                logger.warning("æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶å¯¼è‡´çš„é”™è¯¯")
                logger.info("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°åŠ¨æ€çˆ¬å–æ¨¡å¼...")
                self.save_checkpoint({
                    'static_failed_anti_crawler': True
                })
            
            return {
                'total_discovered': 0,
                'successful_downloads': 0,
                'failed_downloads': 0,
                'visited_pages': 0,
                'downloaded_files': [],
                'failed_files': []
            }
    
    def crawl_dynamic_js(self, url: str, wait_time: int = 10, resume: bool = False) -> Dict[str, Any]:
        """çˆ¬å–åŠ¨æ€JavaScriptæ–‡ä»¶"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹åŠ¨æ€JavaScriptæ–‡ä»¶çˆ¬å–")
        logger.info("=" * 60)
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤
            if resume and 'dynamic_completed' in self.checkpoint_data:
                logger.info("åŠ¨æ€çˆ¬å–å·²å®Œæˆï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return self.checkpoint_data.get('dynamic_results', {})
            
            # æ ¹æ®é…ç½®é€‰æ‹©çˆ¬å–æ–¹å¼
            if USE_EMBEDDED_BROWSER and PLAYWRIGHT_AVAILABLE:
                results = self._crawl_with_playwright(url, wait_time)
            else:
                # ä½¿ç”¨ä¼ ç»ŸSeleniumæ–¹å¼
                results = self.dynamic_crawler.crawl_dynamic_js(url, wait_time)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint({
                'dynamic_completed': True,
                'dynamic_results': results
            })
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            logger.info(f"åŠ¨æ€çˆ¬å–å®Œæˆ:")
            logger.info(f"  - å‘ç°æ–‡ä»¶æ•°: {results['total_discovered']}")
            logger.info(f"  - æˆåŠŸä¸‹è½½: {results['successful_downloads']}")
            logger.info(f"  - å¤±è´¥ä¸‹è½½: {results['failed_downloads']}")
            
            return results
            
        except Exception as e:
            logger.error(f"åŠ¨æ€çˆ¬å–å¤±è´¥: {e}")
            return {
                'total_discovered': 0,
                'successful_downloads': 0,
                'failed_downloads': 0,
                'downloaded_files': [],
                'failed_files': []
            }
    
    def _crawl_with_playwright(self, url: str, wait_time: int = 10) -> Dict[str, Any]:
        """ä½¿ç”¨Playwrightè¿›è¡ŒåŠ¨æ€çˆ¬å–"""
        import asyncio
        from .config import PLAYWRIGHT_BROWSER
        
        async def async_crawl():
            async with PlaywrightCrawler(
                target_url=url,
                max_depth=2, 
                wait_time=wait_time,
                browser_type=PLAYWRIGHT_BROWSER
            ) as crawler:
                stats = await crawler.crawl_website(url, self.dirs['original_dir'])
                return {
                    'total_discovered': stats['js_files_found'],
                    'successful_downloads': stats['js_files_downloaded'],
                    'failed_downloads': stats['js_files_failed'],
                    'downloaded_files': [],  # Playwrightå¤„ç†æ–‡ä»¶åˆ—è¡¨çš„æ–¹å¼ä¸åŒ
                    'failed_files': [],
                    'total_size': stats['total_size']
                }
        
        # è¿è¡Œå¼‚æ­¥çˆ¬å–
        return asyncio.run(async_crawl())
    
    def deobfuscate_files(self, max_workers: int = 4, resume: bool = False) -> Dict[str, Any]:
        """åæ··æ·†JavaScriptæ–‡ä»¶"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹JavaScriptæ–‡ä»¶åæ··æ·†")
        logger.info("=" * 60)
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤
            if resume and 'deobfuscation_completed' in self.checkpoint_data:
                logger.info("åæ··æ·†å·²å®Œæˆï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return self.checkpoint_data.get('deobfuscation_results', {})
            
            # æ‰§è¡Œåæ··æ·†
            results = self.deobfuscator.process_all_files(self.dirs, max_workers)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint({
                'deobfuscation_completed': True,
                'deobfuscation_results': results
            })
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            total = results['total']
            logger.info(f"åæ··æ·†å®Œæˆ:")
            logger.info(f"  - æ€»æ–‡ä»¶æ•°: {total['total_files']}")
            logger.info(f"  - å¤„ç†æˆåŠŸ: {total['processed_files']}")
            logger.info(f"  - ç›´æ¥å¤åˆ¶: {total['skipped_files']}")
            logger.info(f"  - å¤„ç†å¤±è´¥: {total['failed_files']}")
            logger.info(f"  - æˆåŠŸç‡: {total.get('success_rate', 0):.1f}%")
            
            return results
            
        except Exception as e:
            logger.error(f"åæ··æ·†å¤±è´¥: {e}")
            return {
                'static': {'total_files': 0, 'processed_files': 0, 'failed_files': 0, 'skipped_files': 0},
                'dynamic': {'total_files': 0, 'processed_files': 0, 'failed_files': 0, 'skipped_files': 0},
                'total': {'total_files': 0, 'processed_files': 0, 'failed_files': 0, 'skipped_files': 0, 'success_rate': 0}
            }
    
    def generate_final_report(self, static_results: Dict[str, Any], 
                            dynamic_results: Dict[str, Any], 
                            deobfuscation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š"""
        end_time = time.time()
        total_time = end_time - self.start_time
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_discovered = static_results['total_discovered'] + dynamic_results['total_discovered']
        total_downloaded = static_results['successful_downloads'] + dynamic_results['successful_downloads']
        total_failed = static_results['failed_downloads'] + dynamic_results['failed_downloads']
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        static_files = static_results.get('downloaded_files', [])
        dynamic_files = dynamic_results.get('downloaded_files', [])
        total_size = sum(f.get('size', 0) for f in static_files + dynamic_files)
        
        # åæ··æ·†ç»Ÿè®¡
        deob_total = deobfuscation_results['total']
        
        # åˆ¤æ–­æ•´ä½“æ˜¯å¦æˆåŠŸ
        success = total_downloaded > 0 or deob_total['processed_files'] > 0
        
        report = {
            'success': success,
            'total_files': total_downloaded,
            'output_dir': self.dirs['target_output_dir'],
            'error': None if success else 'æœªå‘ç°ä»»ä½•JavaScriptæ–‡ä»¶',
            'summary': {
                'start_time': datetime.fromtimestamp(self.start_time).strftime('%Y-%m-%d %H:%M:%S'),
                'end_time': datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S'),
                'total_time': f"{total_time:.2f}ç§’",
                'total_discovered': total_discovered,
                'total_downloaded': total_downloaded,
                'total_failed': total_failed,
                'download_success_rate': f"{(total_downloaded / total_discovered * 100) if total_discovered > 0 else 0:.1f}%",
                'total_size': format_file_size(total_size)
            },
            'static_crawling': {
                'discovered': static_results['total_discovered'],
                'downloaded': static_results['successful_downloads'],
                'failed': static_results['failed_downloads'],
                'pages_visited': static_results.get('visited_pages', 0)
            },
            'dynamic_crawling': {
                'discovered': dynamic_results['total_discovered'],
                'downloaded': dynamic_results['successful_downloads'],
                'failed': dynamic_results['failed_downloads']
            },
            'deobfuscation': {
                'total_files': deob_total['total_files'],
                'processed': deob_total['processed_files'],
                'copied': deob_total['skipped_files'],
                'failed': deob_total['failed_files'],
                'success_rate': f"{deob_total.get('success_rate', 0):.1f}%"
            }
        }
        
        return report
    
    def _consolidate_detailed_reports(self):
        """æ•´åˆæ‰€æœ‰è¯¦ç»†æŠ¥å‘Šåˆ°ä¸»è¾“å‡ºç›®å½•"""
        try:
            from ..utils.report_generator import CrawlReportGenerator
            
            # åˆ›å»ºä¸»æŠ¥å‘Šç”Ÿæˆå™¨
            main_report_generator = CrawlReportGenerator(self.dirs['target_output_dir'])
            
            # æ”¶é›†é™æ€çˆ¬è™«çš„æŠ¥å‘Šæ•°æ®
            if hasattr(self.static_crawler, 'report_generator'):
                static_generator = self.static_crawler.report_generator
                # åˆå¹¶æˆåŠŸæ–‡ä»¶
                for file_info in static_generator.success_files:
                    main_report_generator.add_success_file(file_info)
                # åˆå¹¶å¤±è´¥æ–‡ä»¶
                for file_info in static_generator.failed_files:
                    main_report_generator.add_failed_file(file_info)
                # åˆå¹¶æ—¥å¿—
                for log_entry in static_generator.logs:
                    main_report_generator.add_log(log_entry)
            
            # æ”¶é›†åŠ¨æ€çˆ¬è™«çš„æŠ¥å‘Šæ•°æ®
            if hasattr(self.dynamic_crawler, 'report_generator'):
                dynamic_generator = self.dynamic_crawler.report_generator
                # åˆå¹¶æˆåŠŸæ–‡ä»¶
                for file_info in dynamic_generator.success_files:
                    main_report_generator.add_success_file(file_info)
                # åˆå¹¶å¤±è´¥æ–‡ä»¶
                for file_info in dynamic_generator.failed_files:
                    main_report_generator.add_failed_file(file_info)
                # åˆå¹¶æ—¥å¿—
                for log_entry in dynamic_generator.logs:
                    main_report_generator.add_log(log_entry)
            
            # æ·»åŠ æ•´åˆå®Œæˆçš„æ—¥å¿—
            main_report_generator.add_log("æ‰€æœ‰çˆ¬å–æŠ¥å‘Šå·²æ•´åˆå®Œæˆ")
            
            # ä¿å­˜æ•´åˆåçš„æŠ¥å‘Š
            main_report_generator.save_all_reports()
            
            # æ‰“å°æ•´åˆæŠ¥å‘Šæ‘˜è¦
            summary = main_report_generator.get_summary()
            logger.info(f"æ•´åˆæŠ¥å‘Šæ‘˜è¦: {summary}")
            
        except Exception as e:
            logger.error(f"æ•´åˆè¯¦ç»†æŠ¥å‘Šå¤±è´¥: {e}")
    
    def run(self, url: str, max_depth: int = 2, wait_time: int = 10, 
            max_workers: int = 4, resume: bool = False) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„çˆ¬å–å’Œåæ··æ·†æµç¨‹"""
        logger.info("JavaScriptæ–‡ä»¶çˆ¬å–å’Œåæ··æ·†å·¥å…·å¯åŠ¨")
        logger.info(f"ç›®æ ‡URL: {url}")
        logger.info(f"æœ€å¤§æ·±åº¦: {max_depth}")
        logger.info(f"åŠ¨æ€ç­‰å¾…æ—¶é—´: {wait_time}ç§’")
        logger.info(f"å¹¶è¡Œå·¥ä½œçº¿ç¨‹: {max_workers}")
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            ensure_directories(self.target_url)
            
            # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœéœ€è¦æ¢å¤ï¼‰
            if resume:
                checkpoint = self.load_checkpoint()
                if checkpoint:
                    self.checkpoint_data = checkpoint
            
            # æ­¥éª¤1: é™æ€çˆ¬å–
            static_results = self.crawl_static_js(url, max_depth, max_workers, resume)
            
            # æ£€æŸ¥é™æ€çˆ¬å–æ˜¯å¦å› åçˆ¬è™«æœºåˆ¶å¤±è´¥
            static_failed_anti_crawler = (
                'static_failed_anti_crawler' in self.checkpoint_data or
                self._is_anti_crawler_detected(static_results)
            )
            
            # æ­¥éª¤2: åŠ¨æ€çˆ¬å–
            # å¦‚æœé™æ€çˆ¬å–å¤±è´¥ï¼Œå¼ºåˆ¶æ‰§è¡ŒåŠ¨æ€çˆ¬å–
            if static_failed_anti_crawler:
                logger.info("ç”±äºæ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶ï¼Œå°†å¼ºåˆ¶æ‰§è¡ŒåŠ¨æ€çˆ¬å–")
                dynamic_results = self.crawl_dynamic_js(url, wait_time, resume=False)  # ä¸è·³è¿‡åŠ¨æ€çˆ¬å–
            else:
                dynamic_results = self.crawl_dynamic_js(url, wait_time, resume)
            
            # æ­¥éª¤3: åæ··æ·†
            deobfuscation_results = self.deobfuscate_files(max_workers, resume)
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = self.generate_final_report(
                static_results, dynamic_results, deobfuscation_results
            )
            
            # æ•´åˆæ‰€æœ‰è¯¦ç»†æŠ¥å‘Š
            self._consolidate_detailed_reports()
            
            # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
            self.print_final_report(final_report)
            
            # æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶
            if self.checkpoint_file.exists():
                self.checkpoint_file.unlink()
                logger.info("å·²æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶")
            
            return final_report
            
        except KeyboardInterrupt:
            logger.warning("ç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œå·²ä¿å­˜æ£€æŸ¥ç‚¹")
            return {
                'success': False,
                'error': 'ç”¨æˆ·ä¸­æ–­æ“ä½œ',
                'total_files': 0,
                'output_dir': self.dirs.get('target_output_dir', '')
            }
        except Exception as e:
            logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'total_files': 0,
                'output_dir': self.dirs.get('target_output_dir', '')
            }
    
    def print_final_report(self, report: Dict[str, Any]):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Š"""
        logger.info("=" * 80)
        logger.info("æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 80)
        
        summary = report['summary']
        logger.info(f"å¼€å§‹æ—¶é—´: {summary['start_time']}")
        logger.info(f"ç»“æŸæ—¶é—´: {summary['end_time']}")
        logger.info(f"æ€»è€—æ—¶: {summary['total_time']}")
        logger.info("")
        
        logger.info("æ–‡ä»¶å‘ç°å’Œä¸‹è½½:")
        logger.info(f"  æ€»å‘ç°æ–‡ä»¶æ•°: {summary['total_discovered']}")
        logger.info(f"  æˆåŠŸä¸‹è½½æ•°: {summary['total_downloaded']}")
        logger.info(f"  ä¸‹è½½å¤±è´¥æ•°: {summary['total_failed']}")
        logger.info(f"  ä¸‹è½½æˆåŠŸç‡: {summary['download_success_rate']}")
        logger.info(f"  æ€»æ–‡ä»¶å¤§å°: {summary['total_size']}")
        logger.info("")
        
        static = report['static_crawling']
        logger.info("é™æ€çˆ¬å–:")
        logger.info(f"  å‘ç°æ–‡ä»¶: {static['discovered']}")
        logger.info(f"  ä¸‹è½½æˆåŠŸ: {static['downloaded']}")
        logger.info(f"  ä¸‹è½½å¤±è´¥: {static['failed']}")
        logger.info(f"  è®¿é—®é¡µé¢: {static['pages_visited']}")
        logger.info("")
        
        dynamic = report['dynamic_crawling']
        logger.info("åŠ¨æ€çˆ¬å–:")
        logger.info(f"  å‘ç°æ–‡ä»¶: {dynamic['discovered']}")
        logger.info(f"  ä¸‹è½½æˆåŠŸ: {dynamic['downloaded']}")
        logger.info(f"  ä¸‹è½½å¤±è´¥: {dynamic['failed']}")
        logger.info("")
        
        deob = report['deobfuscation']
        logger.info("åæ··æ·†å¤„ç†:")
        logger.info(f"  æ€»æ–‡ä»¶æ•°: {deob['total_files']}")
        logger.info(f"  åæ··æ·†å¤„ç†: {deob['processed']}")
        logger.info(f"  ç›´æ¥å¤åˆ¶: {deob['copied']}")
        logger.info(f"  å¤„ç†å¤±è´¥: {deob['failed']}")
        logger.info(f"  å¤„ç†æˆåŠŸç‡: {deob['success_rate']}")
        
        logger.info("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='JavaScriptæ–‡ä»¶çˆ¬å–å’Œåæ··æ·†å·¥å…·')
    parser.add_argument('url', help='ç›®æ ‡ç½‘ç«™URL')
    parser.add_argument('-d', '--depth', type=int, default=2, help='çˆ¬å–æ·±åº¦ (é»˜è®¤: 2)')
    parser.add_argument('-w', '--wait', type=int, default=3, help='é¡µé¢ç­‰å¾…æ—¶é—´(ç§’) (é»˜è®¤: 3)')
    parser.add_argument('-t', '--threads', type=int, default=2, help='å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤: 2)')
    parser.add_argument('-r', '--resume', action='store_true', help='ä»æ£€æŸ¥ç‚¹æ¢å¤')
    
    args = parser.parse_args()
    
    # éªŒè¯URL
    if not args.url.startswith(('http://', 'https://')):
        logger.error("URLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´")
        sys.exit(1)
    
    # åˆ›å»ºçˆ¬å–å™¨å¹¶è¿è¡Œ
    crawler = JSCrawler(args.url)
    try:
        result = crawler.crawl(
            depth=args.depth,
            wait_time=args.wait,
            max_workers=args.threads,
            resume=args.resume
        )
        
        # è¾“å‡ºç»“æœ
        if result.get('success'):
            print(f"\nâœ… çˆ¬å–å®Œæˆï¼æ€»å…±å¤„ç†äº† {result.get('total_files', 0)} ä¸ªæ–‡ä»¶")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {result.get('output_dir', crawler.dirs['target_output_dir'])}")
        else:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            sys.exit(1)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()