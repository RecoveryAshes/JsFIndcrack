"""
JavaScriptæ–‡ä»¶çˆ¬å–å’Œåæ··æ·†ä¸»ç¨‹åº
"""
import sys
import time
import argparse
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime

from .config import ensure_directories, get_directory_structure, SAVE_CHECKPOINT_INTERVAL, BROWSER_ENGINE, USE_EMBEDDED_BROWSER
from ..utils.logger import setup_logger, get_logger
from ..crawlers.static_crawler import StaticJSCrawler
from ..crawlers.dynamic_crawler import DynamicJSCrawler
from .deobfuscator import JSDeobfuscator
from ..utils.utils import save_checkpoint, load_checkpoint, format_file_size, calculate_file_hash
from ..utils.parallel_similarity_analyzer import ParallelDeduplicationProcessor

# æ ¹æ®é…ç½®é€‰æ‹©æµè§ˆå™¨å¼•æ“
if USE_EMBEDDED_BROWSER and BROWSER_ENGINE == "playwright":
    try:
        from ..crawlers.playwright_crawler import PlaywrightCrawler
        PLAYWRIGHT_AVAILABLE = True
    except ImportError:
        PLAYWRIGHT_AVAILABLE = False
        # å»¶è¿Ÿloggeråˆå§‹åŒ–ï¼Œé¿å…å¾ªç¯å¯¼å…¥
        def _warn_playwright_unavailable():
            setup_logger()
            logger = get_logger("main")
            _get_logger().warning("Playwrightæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ä¼ ç»ŸSeleniumæ–¹å¼")
        _warn_playwright_unavailable()
else:
    PLAYWRIGHT_AVAILABLE = False

# å»¶è¿Ÿloggeråˆå§‹åŒ–å‡½æ•°
def _get_logger():
    setup_logger()
    return get_logger("main")

class JSCrawler:
    """JavaScriptçˆ¬å–å™¨ - é¢å‘ç”¨æˆ·çš„ä¸»è¦æ¥å£"""
    
    def __init__(self, target_url: str):
        self.target_url = target_url
        self.dirs = get_directory_structure(target_url)
        self.output_dir = self.dirs['target_output_dir']  # æ·»åŠ output_dirå±æ€§
        self.manager = JSCrawlerManager(target_url)
    
    def crawl(self, depth: int = 2, wait_time: int = 3, max_workers: int = 2, playwright_tabs: int = 4, 
              headless: bool = True, mode: str = 'all', resume: bool = False,
              similarity_enabled: bool = True, similarity_threshold: float = 0.8,
              similarity_workers: int = None, auto_similarity: bool = True) -> Dict[str, Any]:
        """æ‰§è¡Œçˆ¬å–æ“ä½œ"""
        return self.manager.run(
            url=self.target_url,
            max_depth=depth,
            wait_time=wait_time,
            max_workers=max_workers,
            playwright_tabs=playwright_tabs,
            headless=headless,
            mode=mode,
            resume=resume,
            similarity_enabled=similarity_enabled,
            similarity_threshold=similarity_threshold,
            similarity_workers=similarity_workers,
            auto_similarity=auto_similarity
        )

class JSCrawlerManager:
    """JavaScriptçˆ¬å–ç®¡ç†å™¨"""
    
    def __init__(self, target_url: str):
        self.target_url = target_url
        self.dirs = get_directory_structure(target_url)
        ensure_directories(target_url)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.checkpoint_file = self.dirs['checkpoints_dir'] / 'crawler_checkpoint.json'
        self.start_time = time.time()
        # ä¼ é€’output_dirå‚æ•°ç»™çˆ¬è™«ï¼Œä»¥ä¾¿ç”ŸæˆæŠ¥å‘Š
        self.static_crawler = StaticJSCrawler(target_url, self.dirs['target_output_dir'])
        self.dynamic_crawler = DynamicJSCrawler(target_url, self.dirs['target_output_dir'])
        self.deobfuscator = JSDeobfuscator()
        self.checkpoint_data = {}
        
    def load_checkpoint(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®"""
        try:
            checkpoint = load_checkpoint(self.checkpoint_file)
            if checkpoint:
                _get_logger().info("å‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå°†ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­")
                return checkpoint
        except Exception as e:
            _get_logger().error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return None
    
    def save_checkpoint(self, data: Dict[str, Any]) -> bool:
        """ä¿å­˜æ£€æŸ¥ç‚¹æ•°æ®"""
        try:
            self.checkpoint_data.update(data)
            return save_checkpoint(self.checkpoint_data, self.checkpoint_file)
        except Exception as e:
            _get_logger().error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def _is_anti_crawler_detected(self, results: Dict[str, Any]) -> bool:
        """æ£€æµ‹æ˜¯å¦é‡åˆ°åçˆ¬è™«æœºåˆ¶"""
        # å¦‚æœè®¿é—®äº†é¡µé¢ä½†æ²¡æœ‰å‘ç°ä»»ä½•JSæ–‡ä»¶ï¼Œå¯èƒ½æ˜¯åçˆ¬è™«
        if (results.get('visited_pages', 0) > 0 and 
            results.get('total_discovered', 0) == 0 and
            results.get('successful_downloads', 0) == 0):
            return True
        
        # å¦‚æœå‘ç°çš„æ–‡ä»¶æ•°é‡å¾ˆå°‘ä¸”ä¸‹è½½å¤±è´¥ç‡å¾ˆé«˜
        total_discovered = results.get('total_discovered', 0)
        failed_downloads = results.get('failed_downloads', 0)
        if total_discovered > 0 and failed_downloads / total_discovered > 0.8:
            return True
            
        return False
    
    def _is_anti_crawler_error(self, error: Exception) -> bool:
        """æ£€æµ‹æ˜¯å¦æ˜¯åçˆ¬è™«ç›¸å…³çš„é”™è¯¯"""
        error_str = str(error).lower()
        anti_crawler_indicators = [
            '404', '403', '401', 'forbidden', 'not found',
            'access denied', 'blocked', 'captcha', 'verification',
            'too many requests', 'rate limit', 'cloudflare'
        ]
        
        return any(indicator in error_str for indicator in anti_crawler_indicators)
    
    def _collect_downloaded_file_hashes(self) -> Dict[str, str]:
        """æ”¶é›†å·²ä¸‹è½½æ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼Œç”¨äºè·¨æ¨¡å¼å»é‡"""
        file_hashes = {}
        original_dir = self.dirs['original_dir']
        
        if original_dir.exists():
            for js_file in original_dir.rglob('*.js'):
                try:
                    file_hash = calculate_file_hash(js_file)
                    file_hashes[file_hash] = str(js_file.name)
                    _get_logger().debug(f"æ”¶é›†æ–‡ä»¶å“ˆå¸Œ: {js_file.name} -> {file_hash}")
                except Exception as e:
                    _get_logger().warning(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {js_file}: {e}")
        
        _get_logger().info(f"æ”¶é›†åˆ° {len(file_hashes)} ä¸ªå·²ä¸‹è½½æ–‡ä»¶çš„å“ˆå¸Œå€¼")
        return file_hashes
    
    def crawl_static_js(self, url: str, max_depth: int = 2, max_workers: int = 4, resume: bool = False) -> Dict[str, Any]:
        """çˆ¬å–é™æ€JavaScriptæ–‡ä»¶"""
        _get_logger().info("=" * 60)
        _get_logger().info("å¼€å§‹é™æ€JavaScriptæ–‡ä»¶çˆ¬å–")
        _get_logger().info("=" * 60)
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤
            if resume and 'static_completed' in self.checkpoint_data:
                _get_logger().info("é™æ€çˆ¬å–å·²å®Œæˆï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return self.checkpoint_data.get('static_results', {})
            
            # æ‰§è¡Œé™æ€çˆ¬å–
            results = self.static_crawler.crawl_website(url, max_depth, max_workers)
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°åçˆ¬è™«æœºåˆ¶
            if self._is_anti_crawler_detected(results):
                _get_logger().warning("æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶ï¼Œé™æ€çˆ¬å–å¤±è´¥")
                _get_logger().info("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°åŠ¨æ€çˆ¬å–æ¨¡å¼...")
                # æ ‡è®°é™æ€çˆ¬å–å¤±è´¥ï¼Œä½†ä¸ä¿å­˜ä¸ºå®ŒæˆçŠ¶æ€
                self.save_checkpoint({
                    'static_failed_anti_crawler': True,
                    'static_results': results
                })
                return results
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint({
                'static_completed': True,
                'static_results': results
            })
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            _get_logger().info(f"é™æ€çˆ¬å–å®Œæˆ:")
            _get_logger().info(f"  - å‘ç°æ–‡ä»¶æ•°: {results['total_discovered']}")
            _get_logger().info(f"  - æˆåŠŸä¸‹è½½: {results['successful_downloads']}")
            _get_logger().info(f"  - å¤±è´¥ä¸‹è½½: {results['failed_downloads']}")
            _get_logger().info(f"  - è®¿é—®é¡µé¢æ•°: {results['visited_pages']}")
            
            return results
            
        except Exception as e:
            _get_logger().error(f"é™æ€çˆ¬å–å¤±è´¥: {e}")
            # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬è™«ç›¸å…³çš„é”™è¯¯
            if self._is_anti_crawler_error(e):
                _get_logger().warning("æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶å¯¼è‡´çš„é”™è¯¯")
                _get_logger().info("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°åŠ¨æ€çˆ¬å–æ¨¡å¼...")
                self.save_checkpoint({
                    'static_failed_anti_crawler': True
                })
            
            return {
                'total_discovered': 0,
                'successful_downloads': 0,
                'failed_downloads': 0,
                'visited_pages': 0,
                'downloaded_files': [],
                'failed_files': []
            }
    
    def crawl_dynamic_js(self, url: str, wait_time: int = 10, playwright_tabs: int = 4, headless: bool = True, resume: bool = False) -> Dict[str, Any]:
        """çˆ¬å–åŠ¨æ€JavaScriptæ–‡ä»¶"""
        _get_logger().info("=" * 60)
        _get_logger().info("å¼€å§‹åŠ¨æ€JavaScriptæ–‡ä»¶çˆ¬å–")
        _get_logger().info("=" * 60)
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤
            if resume and 'dynamic_completed' in self.checkpoint_data:
                _get_logger().info("åŠ¨æ€çˆ¬å–å·²å®Œæˆï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return self.checkpoint_data.get('dynamic_results', {})
            
            # æ”¶é›†å·²ä¸‹è½½æ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼Œç”¨äºè·¨æ¨¡å¼å»é‡
            existing_file_hashes = self._collect_downloaded_file_hashes()
            
            # æ ¹æ®é…ç½®é€‰æ‹©çˆ¬å–æ–¹å¼
            if USE_EMBEDDED_BROWSER and PLAYWRIGHT_AVAILABLE:
                _get_logger().info(f"ä½¿ç”¨Playwrightè¿›è¡ŒåŠ¨æ€çˆ¬å–ï¼Œæœ€å¤§æ ‡ç­¾é¡µæ•°: {playwright_tabs}ï¼Œæ— å¤´æ¨¡å¼: {headless}")
                results = self._crawl_with_playwright(url, wait_time, playwright_tabs, headless, existing_file_hashes)
            else:
                # ä½¿ç”¨ä¼ ç»ŸSeleniumæ–¹å¼
                _get_logger().info("ä½¿ç”¨ä¼ ç»ŸSeleniumè¿›è¡ŒåŠ¨æ€çˆ¬å–")
                # ä¸ºä¼ ç»ŸSeleniumåŠ¨æ€çˆ¬è™«è®¾ç½®é™æ€æ–‡ä»¶å“ˆå¸Œå€¼
                if existing_file_hashes:
                    self.dynamic_crawler.content_hashes.update(existing_file_hashes.keys())
                    self.dynamic_crawler.hash_to_filename.update(existing_file_hashes)
                    self.dynamic_crawler.static_file_hashes.update(existing_file_hashes.keys())
                    _get_logger().info(f"ä¸ºä¼ ç»ŸåŠ¨æ€çˆ¬è™«è®¾ç½®äº† {len(existing_file_hashes)} ä¸ªé™æ€æ–‡ä»¶å“ˆå¸Œ")
                results = self.dynamic_crawler.crawl_dynamic_js(url, wait_time)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint({
                'dynamic_completed': True,
                'dynamic_results': results
            })
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            _get_logger().info(f"åŠ¨æ€çˆ¬å–å®Œæˆ:")
            _get_logger().info(f"  - å‘ç°æ–‡ä»¶æ•°: {results['total_discovered']}")
            _get_logger().info(f"  - æˆåŠŸä¸‹è½½: {results['successful_downloads']}")
            _get_logger().info(f"  - å¤±è´¥ä¸‹è½½: {results['failed_downloads']}")
            
            duplicated_files = results.get('duplicated_files', 0)
            cross_mode_duplicated = results.get('cross_mode_duplicated_files', 0)
            total_duplicated = duplicated_files + cross_mode_duplicated
            
            if total_duplicated > 0:
                _get_logger().info(f"  - é‡å¤æ–‡ä»¶: {total_duplicated} (å·²è·³è¿‡)")
                if cross_mode_duplicated > 0:
                    _get_logger().info(f"    - æ¨¡å¼å†…å»é‡: {duplicated_files}")
                    _get_logger().info(f"    - è·¨æ¨¡å¼å»é‡: {cross_mode_duplicated}")
                    _get_logger().info(f"  - å»é‡æ•ˆæœ: èŠ‚çœäº† {total_duplicated} ä¸ªé‡å¤æ–‡ä»¶çš„ä¸‹è½½ (å…¶ä¸­ {cross_mode_duplicated} ä¸ªä¸ºè·¨æ¨¡å¼å»é‡)")
                else:
                    _get_logger().info(f"  - å»é‡æ•ˆæœ: èŠ‚çœäº† {total_duplicated} ä¸ªé‡å¤æ–‡ä»¶çš„ä¸‹è½½")
            
            return results
            
        except Exception as e:
            _get_logger().error(f"åŠ¨æ€çˆ¬å–å¤±è´¥: {e}")
            return {
                'total_discovered': 0,
                'successful_downloads': 0,
                'failed_downloads': 0,
                'downloaded_files': [],
                'failed_files': []
            }
    
    def _crawl_with_playwright(self, url: str, wait_time: int = 10, playwright_tabs: int = 4, headless: bool = True, existing_file_hashes: Dict[str, str] = None) -> Dict[str, Any]:
        """ä½¿ç”¨Playwrightè¿›è¡ŒåŠ¨æ€çˆ¬å–"""
        import asyncio
        from .config import PLAYWRIGHT_BROWSER
        
        async def async_crawl():
            async with PlaywrightCrawler(
                target_url=url,
                max_depth=2, 
                wait_time=wait_time,
                max_workers=playwright_tabs,  # ä¼ é€’Playwrightæ ‡ç­¾é¡µæ•°é‡æ§åˆ¶å‚æ•°
                browser_type=PLAYWRIGHT_BROWSER,
                headless=headless,  # ä¼ é€’æ— å¤´æ¨¡å¼å‚æ•°
                existing_file_hashes=existing_file_hashes  # ä¼ é€’å·²æœ‰æ–‡ä»¶å“ˆå¸Œ
            ) as crawler:
                stats = await crawler.crawl_website(url, self.dirs['original_dir'])
                return {
                    'total_discovered': stats['js_files_found'],
                    'successful_downloads': stats['js_files_downloaded'],
                    'failed_downloads': stats['js_files_failed'],
                    'duplicated_files': stats.get('duplicated_files', 0),
                    'cross_mode_duplicated_files': stats.get('cross_mode_duplicated_files', 0),
                    'downloaded_files': [],  # Playwrightå¤„ç†æ–‡ä»¶åˆ—è¡¨çš„æ–¹å¼ä¸åŒ
                    'failed_files': [],
                    'total_size': stats['total_size']
                }
        
        # è¿è¡Œå¼‚æ­¥çˆ¬å–
        return asyncio.run(async_crawl())
    
    def run_similarity_analysis(self, similarity_threshold: float = 0.8, 
                               similarity_workers: int = None, resume: bool = False) -> Dict[str, Any]:
        """è¿è¡Œæ™ºèƒ½ç›¸ä¼¼åº¦åˆ†æ"""
        _get_logger().info("=" * 60)
        _get_logger().info("å¼€å§‹æ™ºèƒ½ç›¸ä¼¼åº¦åˆ†æ")
        _get_logger().info("=" * 60)
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤
            if resume and 'similarity_completed' in self.checkpoint_data:
                _get_logger().info("ç›¸ä¼¼åº¦åˆ†æå·²å®Œæˆï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return self.checkpoint_data.get('similarity_results', {})
            
            # æ£€æŸ¥decodeç›®å½•æ˜¯å¦å­˜åœ¨
            decode_dir = self.dirs['decrypted_dir']
            if not decode_dir.exists() or not any(decode_dir.glob('*.js')):
                _get_logger().warning("æœªæ‰¾åˆ°åç¼–è¯‘æ–‡ä»¶ï¼Œè·³è¿‡ç›¸ä¼¼åº¦åˆ†æ")
                return {
                    'success': True,
                    'total_files': 0,
                    'unique_files': 0,
                    'similar_groups': 0,
                    'exact_duplicate_groups': 0,
                    'processing_time_seconds': 0,
                    'output_dir': None
                }
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            similarity_output_dir = self.dirs['target_output_dir'] / f'similarity_analysis_{timestamp}'
            
            # åˆ›å»ºå¹¶è¿è¡Œç›¸ä¼¼åº¦åˆ†æå™¨
            processor = ParallelDeduplicationProcessor(
                similarity_threshold=similarity_threshold,
                max_workers=similarity_workers
            )
            
            _get_logger().info(f"å¼€å§‹åˆ†æ {decode_dir} ä¸­çš„åç¼–è¯‘æ–‡ä»¶")
            _get_logger().info(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
            _get_logger().info(f"å¹¶è¡Œè¿›ç¨‹æ•°: {similarity_workers or 'auto'}")
            
            # æ‰§è¡Œåˆ†æ
            report = processor.process_directory(str(decode_dir), str(similarity_output_dir))
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            similarity_results = {
                'success': True,
                'total_files': report['total_files'],
                'unique_files': report['unique_files'],
                'similar_groups': report['similar_groups'],
                'exact_duplicate_groups': report['exact_duplicate_groups'],
                'processing_time_seconds': report['processing_time_seconds'],
                'files_per_second': report['files_per_second'],
                'total_exact_duplicates': report['total_exact_duplicates'],
                'total_similar_files': report['total_similar_files'],
                'output_dir': str(similarity_output_dir)
            }
            
            self.save_checkpoint({
                'similarity_completed': True,
                'similarity_results': similarity_results
            })
            
            _get_logger().info(f"ç›¸ä¼¼åº¦åˆ†æå®Œæˆï¼")
            _get_logger().info(f"æ€»æ–‡ä»¶æ•°: {report['total_files']}")
            _get_logger().info(f"å”¯ä¸€æ–‡ä»¶æ•°: {report['unique_files']}")
            _get_logger().info(f"ç›¸ä¼¼æ–‡ä»¶ç»„: {report['similar_groups']}")
            _get_logger().info(f"å¤„ç†æ—¶é—´: {report['processing_time_seconds']:.2f} ç§’")
            _get_logger().info(f"ç»“æœä¿å­˜åˆ°: {similarity_output_dir}")
            
            return similarity_results
            
        except Exception as e:
            _get_logger().error(f"ç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'total_files': 0,
                'unique_files': 0,
                'similar_groups': 0,
                'exact_duplicate_groups': 0,
                'processing_time_seconds': 0,
                'output_dir': None
            }

    def deobfuscate_files(self, max_workers: int = 4, resume: bool = False) -> Dict[str, Any]:
        """åæ··æ·†JavaScriptæ–‡ä»¶"""
        _get_logger().info("=" * 60)
        _get_logger().info("å¼€å§‹JavaScriptæ–‡ä»¶åæ··æ·†")
        _get_logger().info("=" * 60)
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤
            if resume and 'deobfuscation_completed' in self.checkpoint_data:
                _get_logger().info("åæ··æ·†å·²å®Œæˆï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return self.checkpoint_data.get('deobfuscation_results', {})
            
            # æ‰§è¡Œåæ··æ·†
            results = self.deobfuscator.process_all_files(self.dirs, max_workers)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint({
                'deobfuscation_completed': True,
                'deobfuscation_results': results
            })
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            total = results['total']
            _get_logger().info(f"åæ··æ·†å®Œæˆ:")
            _get_logger().info(f"  - æ€»æ–‡ä»¶æ•°: {total['total_files']}")
            _get_logger().info(f"  - å¤„ç†æˆåŠŸ: {total['processed_files']}")
            _get_logger().info(f"  - ç›´æ¥å¤åˆ¶: {total['skipped_files']}")
            _get_logger().info(f"  - å¤„ç†å¤±è´¥: {total['failed_files']}")
            _get_logger().info(f"  - æˆåŠŸç‡: {total.get('success_rate', 0):.1f}%")
            
            return results
            
        except Exception as e:
            _get_logger().error(f"åæ··æ·†å¤±è´¥: {e}")
            return {
                'static': {'total_files': 0, 'processed_files': 0, 'failed_files': 0, 'skipped_files': 0},
                'dynamic': {'total_files': 0, 'processed_files': 0, 'failed_files': 0, 'skipped_files': 0},
                'total': {'total_files': 0, 'processed_files': 0, 'failed_files': 0, 'skipped_files': 0, 'success_rate': 0}
            }
    
    def generate_final_report(self, static_results: Dict[str, Any], 
                            dynamic_results: Dict[str, Any], 
                            deobfuscation_results: Dict[str, Any],
                            similarity_results: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š"""
        end_time = time.time()
        total_time = end_time - self.start_time
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_discovered = static_results.get('total_discovered', 0) + dynamic_results.get('total_discovered', 0)
        total_downloaded = static_results.get('successful_downloads', 0) + dynamic_results.get('successful_downloads', 0)
        total_failed = static_results.get('failed_downloads', 0) + dynamic_results.get('failed_downloads', 0)
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        static_files = static_results.get('downloaded_files', [])
        dynamic_files = dynamic_results.get('downloaded_files', [])
        total_size = sum(f.get('size', 0) for f in static_files + dynamic_files)
        
        # åæ··æ·†ç»Ÿè®¡
        deob_total = deobfuscation_results['total']
        
        # åˆ¤æ–­æ•´ä½“æ˜¯å¦æˆåŠŸ
        success = total_downloaded > 0 or deob_total['processed_files'] > 0
        
        report = {
            'success': success,
            'total_files': total_downloaded,
            'output_dir': self.dirs['target_output_dir'],
            'error': None if success else 'æœªå‘ç°ä»»ä½•JavaScriptæ–‡ä»¶',
            'summary': {
                'start_time': datetime.fromtimestamp(self.start_time).strftime('%Y-%m-%d %H:%M:%S'),
                'end_time': datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S'),
                'total_time': f"{total_time:.2f}ç§’",
                'total_discovered': total_discovered,
                'total_downloaded': total_downloaded,
                'total_failed': total_failed,
                'download_success_rate': f"{(total_downloaded / total_discovered * 100) if total_discovered > 0 else 0:.1f}%",
                'total_size': format_file_size(total_size)
            },
            'static_crawling': {
                'discovered': static_results.get('total_discovered', 0),
                'downloaded': static_results.get('successful_downloads', 0),
                'failed': static_results.get('failed_downloads', 0),
                'pages_visited': static_results.get('visited_pages', 0)
            },
            'dynamic_crawling': {
                'discovered': dynamic_results.get('total_discovered', 0),
                'downloaded': dynamic_results.get('successful_downloads', 0),
                'failed': dynamic_results.get('failed_downloads', 0)
            },
            'deobfuscation': {
                'total_files': deob_total['total_files'],
                'processed': deob_total['processed_files'],
                'copied': deob_total['skipped_files'],
                'failed': deob_total['failed_files'],
                'success_rate': f"{deob_total.get('success_rate', 0):.1f}%"
            }
        }
        
        # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if similarity_results and similarity_results.get('success'):
            report['similarity_analysis'] = {
                'total_files': similarity_results.get('total_files', 0),
                'similar_groups': similarity_results.get('similar_groups', 0),
                'unique_files': similarity_results.get('unique_files', 0),
                'deduplication_rate': similarity_results.get('deduplication_rate', '0%'),
                'processing_time': similarity_results.get('processing_time', '0ç§’'),
                'output_dir': similarity_results.get('output_dir', '')
            }
        
        return report
    
    def _consolidate_detailed_reports(self):
        """æ•´åˆæ‰€æœ‰è¯¦ç»†æŠ¥å‘Šåˆ°ä¸»è¾“å‡ºç›®å½•"""
        try:
            from ..utils.report_generator import CrawlReportGenerator
            
            # åˆ›å»ºä¸»æŠ¥å‘Šç”Ÿæˆå™¨
            main_report_generator = CrawlReportGenerator(self.dirs['target_output_dir'])
            
            # æ”¶é›†é™æ€çˆ¬è™«çš„æŠ¥å‘Šæ•°æ®
            if hasattr(self.static_crawler, 'report_generator'):
                static_generator = self.static_crawler.report_generator
                # åˆå¹¶æˆåŠŸæ–‡ä»¶
                for file_info in static_generator.success_files:
                    main_report_generator.add_success_file(file_info)
                # åˆå¹¶å¤±è´¥æ–‡ä»¶
                for file_info in static_generator.failed_files:
                    main_report_generator.add_failed_file(file_info)
                # åˆå¹¶æ—¥å¿—
                for log_entry in static_generator.detailed_logs:
                    main_report_generator.add_log(log_entry)
            
            # æ”¶é›†åŠ¨æ€çˆ¬è™«çš„æŠ¥å‘Šæ•°æ®
            if hasattr(self.dynamic_crawler, 'report_generator'):
                dynamic_generator = self.dynamic_crawler.report_generator
                # åˆå¹¶æˆåŠŸæ–‡ä»¶
                for file_info in dynamic_generator.success_files:
                    main_report_generator.add_success_file(file_info)
                # åˆå¹¶å¤±è´¥æ–‡ä»¶
                for file_info in dynamic_generator.failed_files:
                    main_report_generator.add_failed_file(file_info)
                # åˆå¹¶æ—¥å¿—
                for log_entry in dynamic_generator.detailed_logs:
                    main_report_generator.add_log(log_entry)
            
            # æ·»åŠ æ•´åˆå®Œæˆçš„æ—¥å¿—
            main_report_generator.add_log("æ‰€æœ‰çˆ¬å–æŠ¥å‘Šå·²æ•´åˆå®Œæˆ")
            
            # ä¿å­˜æ•´åˆåçš„æŠ¥å‘Š
            main_report_generator.save_all_reports()
            
            # æ‰“å°æ•´åˆæŠ¥å‘Šæ‘˜è¦
            summary = main_report_generator.generate_summary_report()
            _get_logger().info(f"æ•´åˆæŠ¥å‘Šæ‘˜è¦: {summary['crawl_summary']}")
            
        except Exception as e:
            _get_logger().error(f"æ•´åˆè¯¦ç»†æŠ¥å‘Šå¤±è´¥: {e}")
    
    def run(self, url: str, max_depth: int = 2, wait_time: int = 10, 
            max_workers: int = 4, playwright_tabs: int = 4, headless: bool = True, 
            mode: str = 'all', resume: bool = False, similarity_enabled: bool = True,
            similarity_threshold: float = 0.8, similarity_workers: int = None,
            auto_similarity: bool = True) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„çˆ¬å–å’Œåæ··æ·†æµç¨‹"""
        _get_logger().info("JavaScriptæ–‡ä»¶çˆ¬å–å’Œåæ··æ·†å·¥å…·å¯åŠ¨")
        _get_logger().info(f"ç›®æ ‡URL: {url}")
        _get_logger().info(f"æœ€å¤§æ·±åº¦: {max_depth}")
        _get_logger().info(f"åŠ¨æ€ç­‰å¾…æ—¶é—´: {wait_time}ç§’")
        _get_logger().info(f"å¹¶è¡Œå·¥ä½œçº¿ç¨‹: {max_workers}")
        _get_logger().info(f"Playwrightæ ‡ç­¾é¡µæ•°: {playwright_tabs}")
        _get_logger().info(f"æ— å¤´æ¨¡å¼: {headless}")
        _get_logger().info(f"çˆ¬å–æ¨¡å¼: {mode}")
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            ensure_directories(self.target_url)
            
            # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœéœ€è¦æ¢å¤ï¼‰
            if resume:
                checkpoint = self.load_checkpoint()
                if checkpoint:
                    self.checkpoint_data = checkpoint
            
            static_results = {}
            dynamic_results = {}
            
            # æ ¹æ®æ¨¡å¼é€‰æ‹©æ‰§è¡Œçš„çˆ¬å–ç±»å‹
            if mode in ['static', 'all']:
                # æ­¥éª¤1: é™æ€çˆ¬å–
                static_results = self.crawl_static_js(url, max_depth, max_workers, resume)
            
            if mode in ['dynamic', 'all']:
                # æ£€æŸ¥é™æ€çˆ¬å–æ˜¯å¦å› åçˆ¬è™«æœºåˆ¶å¤±è´¥ï¼ˆä»…åœ¨å…¨æ¨¡å¼ä¸‹æ£€æŸ¥ï¼‰
                static_failed_anti_crawler = False
                if mode == 'all':
                    static_failed_anti_crawler = (
                        'static_failed_anti_crawler' in self.checkpoint_data or
                        self._is_anti_crawler_detected(static_results)
                    )
                
                # æ­¥éª¤2: åŠ¨æ€çˆ¬å–
                # å¦‚æœé™æ€çˆ¬å–å¤±è´¥ï¼Œå¼ºåˆ¶æ‰§è¡ŒåŠ¨æ€çˆ¬å–
                if static_failed_anti_crawler:
                    _get_logger().info("ç”±äºæ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶ï¼Œå°†å¼ºåˆ¶æ‰§è¡ŒåŠ¨æ€çˆ¬å–")
                    dynamic_results = self.crawl_dynamic_js(url, wait_time, playwright_tabs, headless, resume=False)  # ä¸è·³è¿‡åŠ¨æ€çˆ¬å–
                else:
                    dynamic_results = self.crawl_dynamic_js(url, wait_time, playwright_tabs, headless, resume)
            
            # æ­¥éª¤3: åæ··æ·†ï¼ˆå¦‚æœæœ‰æ–‡ä»¶éœ€è¦å¤„ç†ï¼‰
            deobfuscation_results = {}
            if static_results or dynamic_results:
                deobfuscation_results = self.deobfuscate_files(max_workers, resume)
                _get_logger().info(f"åæ··æ·†å®Œæˆï¼Œç»“æœ: {deobfuscation_results}")
            
            # æ­¥éª¤4: æ™ºèƒ½ç›¸ä¼¼åº¦åˆ†æï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰åç¼–è¯‘æ–‡ä»¶ï¼‰
            similarity_results = {}
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶éœ€è¦åˆ†æ
            total_files = deobfuscation_results.get('total', {}).get('total_files', 0)
            _get_logger().info(f"ç›¸ä¼¼åº¦åˆ†ææ£€æŸ¥: similarity_enabled={similarity_enabled}, auto_similarity={auto_similarity}, total_files={total_files}")
            _get_logger().info(f"deobfuscation_resultsç»“æ„: {deobfuscation_results}")
            if similarity_enabled or (auto_similarity and total_files > 0):
                _get_logger().info("å¼€å§‹æ‰§è¡Œç›¸ä¼¼åº¦åˆ†æ...")
                similarity_results = self.run_similarity_analysis(
                    similarity_threshold, similarity_workers, resume
                )
            else:
                _get_logger().info("è·³è¿‡ç›¸ä¼¼åº¦åˆ†æ")
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = self.generate_final_report(
                static_results, dynamic_results, deobfuscation_results, similarity_results
            )
            
            # æ•´åˆæ‰€æœ‰è¯¦ç»†æŠ¥å‘Š
            self._consolidate_detailed_reports()
            
            # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
            self.print_final_report(final_report)
            
            # æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶
            if self.checkpoint_file.exists():
                self.checkpoint_file.unlink()
                _get_logger().info("å·²æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶")
            
            return final_report
            
        except KeyboardInterrupt:
            _get_logger().warning("ç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œå·²ä¿å­˜æ£€æŸ¥ç‚¹")
            return {
                'success': False,
                'error': 'ç”¨æˆ·ä¸­æ–­æ“ä½œ',
                'total_files': 0,
                'output_dir': self.dirs.get('target_output_dir', '')
            }
        except Exception as e:
            _get_logger().error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'total_files': 0,
                'output_dir': self.dirs.get('target_output_dir', '')
            }
    
    def print_final_report(self, report: Dict[str, Any]):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Š"""
        _get_logger().info("=" * 80)
        _get_logger().info("æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š")
        _get_logger().info("=" * 80)
        
        summary = report['summary']
        _get_logger().info(f"å¼€å§‹æ—¶é—´: {summary['start_time']}")
        _get_logger().info(f"ç»“æŸæ—¶é—´: {summary['end_time']}")
        _get_logger().info(f"æ€»è€—æ—¶: {summary['total_time']}")
        _get_logger().info("")
        
        _get_logger().info("æ–‡ä»¶å‘ç°å’Œä¸‹è½½:")
        _get_logger().info(f"  æ€»å‘ç°æ–‡ä»¶æ•°: {summary['total_discovered']}")
        _get_logger().info(f"  æˆåŠŸä¸‹è½½æ•°: {summary['total_downloaded']}")
        _get_logger().info(f"  ä¸‹è½½å¤±è´¥æ•°: {summary['total_failed']}")
        _get_logger().info(f"  ä¸‹è½½æˆåŠŸç‡: {summary['download_success_rate']}")
        _get_logger().info(f"  æ€»æ–‡ä»¶å¤§å°: {summary['total_size']}")
        _get_logger().info("")
        
        static = report['static_crawling']
        _get_logger().info("é™æ€çˆ¬å–:")
        _get_logger().info(f"  å‘ç°æ–‡ä»¶: {static['discovered']}")
        _get_logger().info(f"  ä¸‹è½½æˆåŠŸ: {static['downloaded']}")
        _get_logger().info(f"  ä¸‹è½½å¤±è´¥: {static['failed']}")
        _get_logger().info(f"  è®¿é—®é¡µé¢: {static['pages_visited']}")
        _get_logger().info("")
        
        dynamic = report['dynamic_crawling']
        _get_logger().info("åŠ¨æ€çˆ¬å–:")
        _get_logger().info(f"  å‘ç°æ–‡ä»¶: {dynamic['discovered']}")
        _get_logger().info(f"  ä¸‹è½½æˆåŠŸ: {dynamic['downloaded']}")
        _get_logger().info(f"  ä¸‹è½½å¤±è´¥: {dynamic['failed']}")
        _get_logger().info("")
        
        deob = report['deobfuscation']
        _get_logger().info("åæ··æ·†å¤„ç†:")
        _get_logger().info(f"  æ€»æ–‡ä»¶æ•°: {deob['total_files']}")
        _get_logger().info(f"  åæ··æ·†å¤„ç†: {deob['processed']}")
        _get_logger().info(f"  ç›´æ¥å¤åˆ¶: {deob['copied']}")
        _get_logger().info(f"  å¤„ç†å¤±è´¥: {deob['failed']}")
        _get_logger().info(f"  å¤„ç†æˆåŠŸç‡: {deob['success_rate']}")
        _get_logger().info("")
        
        # æ˜¾ç¤ºç›¸ä¼¼åº¦åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if 'similarity_analysis' in report:
            sim = report['similarity_analysis']
            _get_logger().info("æ™ºèƒ½ç›¸ä¼¼åº¦åˆ†æ:")
            _get_logger().info(f"  åˆ†ææ–‡ä»¶æ•°: {sim['total_files']}")
            _get_logger().info(f"  ç›¸ä¼¼æ–‡ä»¶ç»„: {sim['similar_groups']}")
            _get_logger().info(f"  å”¯ä¸€æ–‡ä»¶æ•°: {sim['unique_files']}")
            _get_logger().info(f"  å»é‡ç‡: {sim['deduplication_rate']}")
            _get_logger().info(f"  å¤„ç†æ—¶é—´: {sim['processing_time']}")
            _get_logger().info(f"  è¾“å‡ºç›®å½•: {sim['output_dir']}")
            _get_logger().info("")
        
        _get_logger().info("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='JavaScriptæ–‡ä»¶çˆ¬å–å’Œåæ··æ·†å·¥å…·')
    parser.add_argument('-u', '--url', dest='url', required=True, help='ç›®æ ‡ç½‘ç«™URL')
    parser.add_argument('-d', '--depth', type=int, default=2, help='çˆ¬å–æ·±åº¦ (é»˜è®¤: 2)')
    parser.add_argument('-w', '--wait', type=int, default=3, help='é¡µé¢ç­‰å¾…æ—¶é—´(ç§’) (é»˜è®¤: 3)')
    parser.add_argument('-t', '--threads', type=int, default=2, help='é™æ€çˆ¬å–å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤: 2)')
    parser.add_argument('--playwright-tabs', type=int, default=4, help='PlaywrightåŒæ—¶æ‰“å¼€çš„æ ‡ç­¾é¡µæ•°é‡ (é»˜è®¤: 4)')
    parser.add_argument('--headless', action='store_true', default=True, help='Playwrightæ— å¤´æ¨¡å¼è¿è¡Œ (é»˜è®¤: True)')
    parser.add_argument('--no-headless', dest='headless', action='store_false', help='Playwrightæœ‰å¤´æ¨¡å¼è¿è¡Œ')
    parser.add_argument('--mode', choices=['static', 'dynamic', 'all'], default='all', 
                       help='çˆ¬å–æ¨¡å¼: static(ä»…é™æ€), dynamic(ä»…åŠ¨æ€), all(å…¨éƒ¨) (é»˜è®¤: all)')
    parser.add_argument('-r', '--resume', action='store_true', help='ä»æ£€æŸ¥ç‚¹æ¢å¤')
    
    # ç›¸ä¼¼åº¦æ£€æµ‹ç›¸å…³å‚æ•°
    parser.add_argument('--similarity', action='store_true', default=True, help='å¯ç”¨æ™ºèƒ½ç›¸ä¼¼åº¦æ£€æµ‹å’Œå»é‡ (é»˜è®¤: True)')
    parser.add_argument('--similarity-threshold', type=float, default=0.8, 
                       help='ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0ï¼Œé»˜è®¤: 0.8)')
    parser.add_argument('--similarity-workers', type=int, default=None,
                       help='ç›¸ä¼¼åº¦åˆ†æå¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--no-similarity', dest='similarity', action='store_false', 
                       help='ç¦ç”¨æ™ºèƒ½ç›¸ä¼¼åº¦æ£€æµ‹å’Œå»é‡')
    parser.add_argument('--no-similarity-auto', action='store_true', 
                       help='ç¦ç”¨åç¼–è¯‘åè‡ªåŠ¨è¿è¡Œç›¸ä¼¼åº¦åˆ†æ')
    
    args = parser.parse_args()
    
    # éªŒè¯URL
    if not args.url.startswith(('http://', 'https://')):
        _get_logger().error("URLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´")
        sys.exit(1)
    
    # åˆ›å»ºçˆ¬å–å™¨å¹¶è¿è¡Œ
    crawler = JSCrawler(args.url)
    try:
        result = crawler.crawl(
            depth=args.depth,
            wait_time=args.wait,
            max_workers=args.threads,
            playwright_tabs=getattr(args, 'playwright_tabs'),
            headless=args.headless,
            mode=args.mode,
            resume=args.resume,
            similarity_enabled=args.similarity,
            similarity_threshold=args.similarity_threshold,
            similarity_workers=args.similarity_workers,
            auto_similarity=not args.no_similarity_auto
        )
        
        # è¾“å‡ºç»“æœ
        if result.get('success'):
            print(f"\nçˆ¬å–å®Œæˆï¼æ€»å…±å¤„ç†äº† {result.get('total_files', 0)} ä¸ªæ–‡ä»¶")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {result.get('output_dir', crawler.dirs['target_output_dir'])}")
        else:
            print(f"\nçˆ¬å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            sys.exit(1)
    except Exception as e:
        _get_logger().error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()