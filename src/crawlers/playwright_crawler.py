#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PlaywrightåŠ¨æ€çˆ¬è™«æ¨¡å— - ä½¿ç”¨å†…ç½®æµè§ˆå™¨å¼•æ“
æ”¯æŒChromiumã€Firefoxå’ŒWebKitå¼•æ“ï¼Œæ— éœ€å¤–éƒ¨æµè§ˆå™¨ä¾èµ–
"""

import asyncio
import time
import re
from pathlib import Path
from typing import Set, Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse
import logging

try:
    from playwright.async_api import async_playwright, Browser, Page, BrowserContext
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

from ..utils.utils import is_javascript_file, generate_file_path, convert_to_utf8, format_file_size


class PlaywrightCrawler:
    """ä½¿ç”¨Playwrightçš„åŠ¨æ€JavaScriptçˆ¬è™«"""
    
    def __init__(self, target_url: str = None, max_depth: int = 2, wait_time: int = 3, 
                 max_workers: int = 4, browser_type: str = "chromium"):
        """
        åˆå§‹åŒ–Playwrightçˆ¬è™«
        
        Args:
            target_url: ç›®æ ‡URL
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦
            wait_time: é¡µé¢ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            max_workers: æœ€å¤§å¹¶å‘æ•°
            browser_type: æµè§ˆå™¨ç±»å‹ ("chromium", "firefox", "webkit")
        """
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError("Playwrightæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install playwright && playwright install")
        
        self.target_url = target_url
        self.max_depth = max_depth
        self.wait_time = wait_time
        self.max_workers = max_workers
        self.browser_type = browser_type.lower()
        
        # éªŒè¯æµè§ˆå™¨ç±»å‹
        if self.browser_type not in ["chromium", "firefox", "webkit"]:
            self.browser_type = "chromium"
        
        self.logger = logging.getLogger(__name__)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'pages_visited': 0,
            'js_files_found': 0,
            'js_files_downloaded': 0,
            'js_files_failed': 0,
            'total_size': 0
        }
        self.download_tasks = []  # å­˜å‚¨ä¸‹è½½ä»»åŠ¡
        
        # å­˜å‚¨å·²è®¿é—®çš„URLå’Œå‘ç°çš„JSæ–‡ä»¶
        self.visited_urls: Set[str] = set()
        self.discovered_js_files: Set[str] = set()
        
        # Playwrightå¯¹è±¡
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self._setup_browser()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self._cleanup_browser()

    async def _setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨"""
        try:
            self.playwright = await async_playwright().start()
            
            # æ ¹æ®ç±»å‹é€‰æ‹©æµè§ˆå™¨
            if self.browser_type == "firefox":
                self.browser = await self.playwright.firefox.launch(
                    headless=False,
                    args=['--no-sandbox', '--disable-dev-shm-usage']
                )
            elif self.browser_type == "webkit":
                self.browser = await self.playwright.webkit.launch(
                    headless=False
                )
            else:  # chromium (é»˜è®¤)
                self.browser = await self.playwright.chromium.launch(
                    headless=False,
                    args=[
                        '--no-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--disable-extensions',
                        '--disable-plugins',
                        '--disable-images',
                        '--disable-javascript-harmony-shipping',
                        '--disable-background-timer-throttling',
                        '--disable-backgrounding-occluded-windows',
                        '--disable-renderer-backgrounding'
                    ]
                )
            
            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                ignore_https_errors=True,
                java_script_enabled=True
            )
            
            self.logger.info(f"âœ… {self.browser_type.title()}æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def _cleanup_browser(self):
        """æ¸…ç†æµè§ˆå™¨èµ„æº"""
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            self.logger.info("ğŸ§¹ æµè§ˆå™¨èµ„æºå·²æ¸…ç†")
        except Exception as e:
            self.logger.error(f"æ¸…ç†æµè§ˆå™¨èµ„æºæ—¶å‡ºé”™: {e}")

    async def _extract_js_files_from_page(self, page: Page, base_url: str) -> Set[str]:
        """ä»é¡µé¢ä¸­æå–JavaScriptæ–‡ä»¶URL"""
        js_files = set()
        
        try:
            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆï¼ˆä¸ç­‰å¾…networkidleï¼ŒåŠ è½½è¿‡ç¨‹ä¸­ä¹Ÿèƒ½æ”¶é›†JSï¼‰
            await page.wait_for_load_state('domcontentloaded', timeout=10000)
            
            # æ‰§è¡ŒJavaScriptæ¥è·å–æ‰€æœ‰scriptæ ‡ç­¾
            script_urls = await page.evaluate("""
                () => {
                    const scripts = Array.from(document.querySelectorAll('script[src]'));
                    return scripts.map(script => script.src).filter(src => src);
                }
            """)
            
            # å¤„ç†ç›¸å¯¹URL
            for url in script_urls:
                if url:
                    absolute_url = urljoin(base_url, url)
                    if self._is_js_file(absolute_url):
                        js_files.add(absolute_url)
            
            # ç›‘å¬ç½‘ç»œè¯·æ±‚ä¸­çš„JSæ–‡ä»¶
            network_js_files = await page.evaluate("""
                () => {
                    return window.jsFiles || [];
                }
            """)
            
            for url in network_js_files:
                if url:
                    absolute_url = urljoin(base_url, url)
                    if self._is_js_file(absolute_url):
                        js_files.add(absolute_url)
                        
        except Exception as e:
            self.logger.warning(f"æå–JSæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        return js_files

    def _is_js_file(self, url: str) -> bool:
        """åˆ¤æ–­URLæ˜¯å¦ä¸ºJavaScriptæ–‡ä»¶"""
        if not url:
            return False
        
        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µ
        clean_url = url.split('?')[0].split('#')[0]
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if clean_url.endswith('.js'):
            return True
        
        # æ£€æŸ¥MIMEç±»å‹ç›¸å…³çš„URLæ¨¡å¼
        js_patterns = [
            r'\.js$',
            r'\.js\?',
            r'/js/',
            r'javascript',
            r'\.min\.js',
            r'application/javascript',
            r'text/javascript'
        ]
        
        return any(re.search(pattern, url, re.IGNORECASE) for pattern in js_patterns)

    async def _start_immediate_downloads(self, js_files: Set[str]):
        """ç«‹å³å¼€å§‹ä¸‹è½½æ–°å‘ç°çš„JSæ–‡ä»¶"""
        for js_url in js_files:
            if js_url not in self.discovered_js_files:
                self.discovered_js_files.add(js_url)
                # åˆ›å»ºä¸‹è½½ä»»åŠ¡ä½†ä¸ç­‰å¾…å®Œæˆ
                task = asyncio.create_task(self._download_js_file_immediate(js_url))
                self.download_tasks.append(task)
                self.logger.debug(f"å·²å¯åŠ¨ä¸‹è½½ä»»åŠ¡: {js_url}")

    async def _download_js_file_immediate(self, url: str) -> Optional[Dict]:
        """ç«‹å³ä¸‹è½½JSæ–‡ä»¶ï¼ˆç”¨äºå¹¶å‘ä¸‹è½½ï¼‰"""
        try:
            # åˆ›å»ºæ–°é¡µé¢ç”¨äºä¸‹è½½
            page = await self.context.new_page()
            
            # ç›´æ¥è®¿é—®JSæ–‡ä»¶URL
            response = await page.goto(url, timeout=10000)
            
            if response and response.status == 200:
                content = await response.text()
                
                # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
                file_path = generate_file_path(url, self.target_url, 'dynamic')
                
                # ä¿å­˜æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                # è½¬æ¢ç¼–ç ä¸ºUTF-8
                convert_to_utf8(file_path)
                
                self.logger.info(f"ä¸‹è½½æˆåŠŸ: {url} -> {file_path}")
                
                await page.close()
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats['js_files_downloaded'] += 1
                self.stats['total_size'] += len(content.encode('utf-8'))
                
                return {
                    'url': url,
                    'path': str(file_path),
                    'size': len(content.encode('utf-8'))
                }
            else:
                self.logger.warning(f"ä¸‹è½½å¤±è´¥: {url} (çŠ¶æ€ç : {response.status if response else 'None'})")
                self.stats['js_files_failed'] += 1
                
            await page.close()
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½JSæ–‡ä»¶å¤±è´¥ {url}: {e}")
            self.stats['js_files_failed'] += 1
        
        return None

    async def _visit_page(self, url: str, depth: int = 0) -> Tuple[Set[str], Set[str]]:
        """è®¿é—®å•ä¸ªé¡µé¢å¹¶æå–JSæ–‡ä»¶å’Œé“¾æ¥"""
        if depth > self.max_depth or url in self.visited_urls:
            return set(), set()
        
        self.visited_urls.add(url)
        js_files = set()
        links = set()
        
        try:
            page = await self.context.new_page()
            
            # è®¾ç½®ç½‘ç»œç›‘å¬æ¥æ•è·JSæ–‡ä»¶è¯·æ±‚
            js_requests = []
            
            async def handle_request(request):
                if self._is_js_file(request.url):
                    js_requests.append(request.url)
            
            page.on('request', handle_request)
            
            self.logger.info(f"æ­£åœ¨è®¿é—®é¡µé¢: {url}")
            
            # è®¿é—®é¡µé¢ï¼ˆä¸ç­‰å¾…networkidleï¼Œè¿™æ ·å¯ä»¥åœ¨åŠ è½½è¿‡ç¨‹ä¸­æ”¶é›†JSæ–‡ä»¶ï¼‰
            response = await page.goto(url, wait_until='domcontentloaded', timeout=10000)
            
            if response and response.status >= 400:
                self.logger.warning(f"é¡µé¢è¿”å›é”™è¯¯çŠ¶æ€: {response.status}")
            
            # çŸ­æš‚ç­‰å¾…è®©åˆå§‹JSåŠ è½½ï¼ˆå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
            await asyncio.sleep(1)
            
            # è§¦å‘å¯èƒ½çš„åŠ¨æ€åŠ è½½
            await page.evaluate("""
                () => {
                    // æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
                    window.scrollTo(0, document.body.scrollHeight);
                    
                    // è§¦å‘å¸¸è§çš„äº‹ä»¶
                    ['click', 'mouseover', 'focus'].forEach(eventType => {
                        document.querySelectorAll('button, a, input').forEach(el => {
                            try {
                                el.dispatchEvent(new Event(eventType, {bubbles: true}));
                            } catch(e) {}
                        });
                    });
                }
            """)
            
            # å†æ¬¡çŸ­æš‚ç­‰å¾…
            await asyncio.sleep(1)
            
            # æå–JSæ–‡ä»¶
            page_js_files = await self._extract_js_files_from_page(page, url)
            js_files.update(page_js_files)
            js_files.update(js_requests)
            
            # ç«‹å³å¼€å§‹ä¸‹è½½æ–°å‘ç°çš„JSæ–‡ä»¶
            await self._start_immediate_downloads(js_files)
            
            # æå–é¡µé¢é“¾æ¥ï¼ˆç”¨äºæ·±åº¦çˆ¬å–ï¼‰
            if depth < self.max_depth:
                try:
                    page_links = await page.evaluate("""
                        () => {
                            const links = Array.from(document.querySelectorAll('a[href]'));
                            return links.map(link => link.href).filter(href => href);
                        }
                    """)
                    
                    base_domain = urlparse(url).netloc
                    for link in page_links:
                        if link and urlparse(link).netloc == base_domain:
                            links.add(link)
                            
                except Exception as e:
                    self.logger.warning(f"æå–é“¾æ¥æ—¶å‡ºé”™: {e}")
            
            await page.close()
            self.stats['pages_visited'] += 1
            
            if js_files:
                self.logger.info(f"åœ¨é¡µé¢ {url} ä¸­å‘ç° {len(js_files)} ä¸ªJavaScriptæ–‡ä»¶")
            
        except Exception as e:
            self.logger.error(f"è®¿é—®é¡µé¢å¤±è´¥ {url}: {e}")
        
        return js_files, links

    async def crawl_website(self, start_url: str, output_dir: Path) -> Dict:
        """çˆ¬å–ç½‘ç«™çš„JavaScriptæ–‡ä»¶"""
        self.logger.info(f"å¼€å§‹åŠ¨æ€çˆ¬å–: {start_url}")
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # å¼€å§‹çˆ¬å–
            to_visit = [(start_url, 0)]
            all_js_files = set()
            
            while to_visit:
                current_url, depth = to_visit.pop(0)
                
                if depth > self.max_depth:
                    continue
                
                js_files, links = await self._visit_page(current_url, depth)
                all_js_files.update(js_files)
                
                # æ·»åŠ æ–°å‘ç°çš„é“¾æ¥åˆ°å¾…è®¿é—®åˆ—è¡¨
                for link in links:
                    if link not in self.visited_urls:
                        to_visit.append((link, depth + 1))
            
            self.stats['js_files_found'] = len(all_js_files)
            self.logger.info(f"å‘ç° {len(all_js_files)} ä¸ªJavaScriptæ–‡ä»¶")
            
            # ç­‰å¾…æ‰€æœ‰å¹¶å‘ä¸‹è½½ä»»åŠ¡å®Œæˆ
            if self.download_tasks:
                self.logger.info(f"ç­‰å¾… {len(self.download_tasks)} ä¸ªä¸‹è½½ä»»åŠ¡å®Œæˆ...")
                results = await asyncio.gather(*self.download_tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        self.stats['js_files_failed'] += 1
                    elif result:
                        self.stats['js_files_downloaded'] += 1
                        self.stats['total_size'] += result.get('size', 0)
            
        except Exception as e:
            self.logger.error(f"åŠ¨æ€çˆ¬å–å¤±è´¥: {e}")
            raise
        
        return self.stats

    async def _download_js_file(self, url: str, output_dir: Path) -> Optional[Dict]:
        """ä¸‹è½½å•ä¸ªJavaScriptæ–‡ä»¶"""
        try:
            # åˆ›å»ºæ–°é¡µé¢ç”¨äºä¸‹è½½
            page = await self.context.new_page()
            
            # ç›´æ¥è®¿é—®JSæ–‡ä»¶URL
            response = await page.goto(url, timeout=10000)
            
            if response and response.status == 200:
                content = await response.text()
                
                # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
                parsed_url = urlparse(url)
                filename = Path(parsed_url.path).name or 'script.js'
                if not filename.endswith('.js'):
                    filename += '.js'
                file_path = output_dir / filename
                
                # ä¿å­˜æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                self.logger.info(f"ä¸‹è½½æˆåŠŸ: {url} -> {file_path}")
                
                await page.close()
                return {
                    'url': url,
                    'path': str(file_path),
                    'size': len(content.encode('utf-8'))
                }
            else:
                self.logger.warning(f"ä¸‹è½½å¤±è´¥: {url} (çŠ¶æ€ç : {response.status if response else 'None'})")
                
            await page.close()
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½JSæ–‡ä»¶å¤±è´¥ {url}: {e}")
        
        return None


async def main():
    """æµ‹è¯•å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python playwright_crawler.py <URL>")
        return
    
    url = sys.argv[1]
    output_dir = Path("test_output")
    
    async with PlaywrightCrawler(max_depth=1, wait_time=3) as crawler:
        stats = await crawler.crawl_website(url, output_dir)
        print(f"çˆ¬å–å®Œæˆ: {stats}")


if __name__ == "__main__":
    asyncio.run(main())