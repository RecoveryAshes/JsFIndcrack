#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PlaywrightåŠ¨æ€çˆ¬è™«æ¨¡å— - ä½¿ç”¨å†…ç½®æµè§ˆå™¨å¼•æ“
æ”¯æŒChromiumã€Firefoxå’ŒWebKitå¼•æ“ï¼Œæ— éœ€å¤–éƒ¨æµè§ˆå™¨ä¾èµ–
"""

import asyncio
import time
import re
import sys
import os
from pathlib import Path
from typing import Set, Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse
import logging
from tqdm.asyncio import tqdm

try:
    from playwright.async_api import async_playwright, Browser, Page, BrowserContext
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    # ä¸ºç±»å‹æ³¨è§£æä¾›å ä½ç¬¦
    Browser = None
    Page = None
    BrowserContext = None

from ..utils.utils import is_supported_file, generate_file_path, convert_to_utf8, format_file_size, get_content_hash, is_duplicate_content, is_file_already_downloaded
from ..utils.logger import get_logger


def get_packaged_browser_path():
    """è·å–æ‰“åŒ…ç¯å¢ƒä¸­çš„æµè§ˆå™¨è·¯å¾„"""
    if getattr(sys, 'frozen', False):
        # åœ¨æ‰“åŒ…ç¯å¢ƒä¸­
        if sys.platform == "darwin":  # macOS
            # åœ¨æ‰“åŒ…çš„å¯æ‰§è¡Œæ–‡ä»¶ä¸­ï¼Œæµè§ˆå™¨ä½äºç›¸å¯¹è·¯å¾„
            base_path = Path(sys._MEIPASS)
            browser_path = base_path / "playwright_browsers" / "chromium-1187" / "chrome-mac" / "Chromium.app" / "Contents" / "MacOS" / "Chromium"
            if browser_path.exists():
                return str(browser_path)
        elif sys.platform.startswith("linux"):
            # Linuxç¯å¢ƒ
            base_path = Path(sys._MEIPASS)
            browser_path = base_path / "playwright_browsers" / "chromium-1187" / "chrome-linux" / "chrome"
            if browser_path.exists():
                return str(browser_path)
        elif sys.platform.startswith("win"):
            # Windowsç¯å¢ƒ
            base_path = Path(sys._MEIPASS)
            browser_path = base_path / "playwright_browsers" / "chromium-1187" / "chrome-win" / "chrome.exe"
            if browser_path.exists():
                return str(browser_path)
    return None


class PlaywrightCrawler:
    """ä½¿ç”¨Playwrightçš„åŠ¨æ€JavaScriptå’ŒSource Mapçˆ¬è™«"""
    
    def __init__(self, target_url: str = None, max_depth: int = 2, wait_time: int = 3, 
                 max_workers: int = 4, browser_type: str = "chromium", headless: bool = True,
                 existing_file_hashes: Dict[str, str] = None):
        """
        åˆå§‹åŒ–Playwrightçˆ¬è™«
        
        Args:
            target_url: ç›®æ ‡URL
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦
            wait_time: é¡µé¢ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆæ§åˆ¶åŒæ—¶æ‰“å¼€çš„æ ‡ç­¾é¡µæ•°é‡ï¼‰
            browser_type: æµè§ˆå™¨ç±»å‹ ("chromium", "firefox", "webkit")
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œ
            existing_file_hashes: å·²æœ‰æ–‡ä»¶çš„å“ˆå¸Œå€¼å­—å…¸ï¼Œç”¨äºè·¨æ¨¡å¼å»é‡
        """
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError("Playwrightæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install playwright && playwright install")
        
        self.target_url = target_url
        self.max_depth = max_depth
        self.wait_time = wait_time
        self.max_workers = max_workers
        self.browser_type = browser_type.lower()
        self.headless = headless
        
        # éªŒè¯æµè§ˆå™¨ç±»å‹
        if self.browser_type not in ["chromium", "firefox", "webkit"]:
            self.browser_type = "chromium"
        
        self.logger = get_logger("playwright_crawler")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'pages_visited': 0,
            'js_files_found': 0,
            'js_files_downloaded': 0,
            'js_files_failed': 0,
            'js_files_duplicated': 0,  # æ–°å¢ï¼šé‡å¤æ–‡ä»¶æ•°é‡
            'js_files_cross_mode_duplicated': 0,  # æ–°å¢ï¼šè·¨æ¨¡å¼é‡å¤æ–‡ä»¶æ•°é‡
            'total_size': 0,
            'start_time': None,
            'end_time': None
        }
        
        # å»é‡ç›¸å…³
        self.content_hashes = set()  # å­˜å‚¨å·²ä¸‹è½½æ–‡ä»¶çš„å†…å®¹å“ˆå¸Œ
        self.hash_to_filename = {}   # å“ˆå¸Œå€¼åˆ°æ–‡ä»¶åçš„æ˜ å°„
        self.download_tasks = []  # å­˜å‚¨ä¸‹è½½ä»»åŠ¡
        
        # åˆå§‹åŒ–å·²æœ‰æ–‡ä»¶å“ˆå¸Œï¼ˆç”¨äºè·¨æ¨¡å¼å»é‡ï¼‰
        self.static_file_hashes = set()  # å­˜å‚¨æ¥è‡ªé™æ€çˆ¬å–çš„æ–‡ä»¶å“ˆå¸Œ
        if existing_file_hashes:
            self.content_hashes.update(existing_file_hashes.keys())
            self.hash_to_filename.update(existing_file_hashes)
            self.static_file_hashes.update(existing_file_hashes.keys())
            self.logger.info(f"åŠ è½½äº† {len(existing_file_hashes)} ä¸ªå·²æœ‰æ–‡ä»¶å“ˆå¸Œï¼Œç”¨äºè·¨æ¨¡å¼å»é‡")
        
        # å­˜å‚¨å·²è®¿é—®çš„URLå’Œå‘ç°çš„JSæ–‡ä»¶
        self.visited_urls: Set[str] = set()
        self.discovered_js_files: Set[str] = set()
        
        # æ ‡ç­¾é¡µç®¡ç†
        self.active_pages = {}  # {page_id: page_object}
        self.page_semaphore = None  # å°†åœ¨__aenter__ä¸­åˆå§‹åŒ–
        self.download_queue = None  # å°†åœ¨__aenter__ä¸­åˆå§‹åŒ–
        self.completed_downloads = set()  # å·²å®Œæˆä¸‹è½½çš„URL
        
        # Playwrightå¯¹è±¡
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        # åˆå§‹åŒ–å¼‚æ­¥å¯¹è±¡
        self.page_semaphore = asyncio.Semaphore(self.max_workers)
        self.download_queue = asyncio.Queue()
        
        await self._setup_browser()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self._cleanup_browser()

    async def _setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨"""
        try:
            self.playwright = await async_playwright().start()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æ‰“åŒ…ç¯å¢ƒä¸­ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨æ‰“åŒ…çš„æµè§ˆå™¨
            packaged_browser_path = get_packaged_browser_path()
            
            # æµè§ˆå™¨å¯åŠ¨å‚æ•°
            browser_args = [
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--disable-extensions',
                '--disable-plugins',
                '--disable-images',
                '--disable-javascript-harmony-shipping',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-renderer-backgrounding'
            ]
            
            # æ ¹æ®ç±»å‹é€‰æ‹©æµè§ˆå™¨
            if self.browser_type == "firefox":
                launch_options = {
                    'headless': self.headless,
                    'args': ['--no-sandbox', '--disable-dev-shm-usage']
                }
                if packaged_browser_path and 'firefox' in packaged_browser_path.lower():
                    launch_options['executable_path'] = packaged_browser_path
                self.browser = await self.playwright.firefox.launch(**launch_options)
                
            elif self.browser_type == "webkit":
                launch_options = {
                    'headless': self.headless
                }
                if packaged_browser_path and 'webkit' in packaged_browser_path.lower():
                    launch_options['executable_path'] = packaged_browser_path
                self.browser = await self.playwright.webkit.launch(**launch_options)
                
            else:  # chromium (é»˜è®¤)
                launch_options = {
                    'headless': self.headless,
                    'args': browser_args
                }
                if packaged_browser_path:
                    launch_options['executable_path'] = packaged_browser_path
                    self.logger.info(f"ä½¿ç”¨æ‰“åŒ…çš„æµè§ˆå™¨: {packaged_browser_path}")
                
                self.browser = await self.playwright.chromium.launch(**launch_options)
            
            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                ignore_https_errors=True,
                java_script_enabled=True
            )
            
            self.logger.info(f"{self.browser_type.title()}æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def _cleanup_browser(self):
        """æ¸…ç†æµè§ˆå™¨èµ„æº"""
        try:
            # å…³é—­æ‰€æœ‰æ´»è·ƒçš„é¡µé¢
            for page_id, page in self.active_pages.items():
                try:
                    await page.close()
                except:
                    pass
            self.active_pages.clear()
            
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            self.logger.info("ğŸ§¹ æµè§ˆå™¨èµ„æºå·²æ¸…ç†")
        except Exception as e:
            self.logger.error(f"æ¸…ç†æµè§ˆå™¨èµ„æºæ—¶å‡ºé”™: {e}")

    async def _download_js_with_tab_control(self, url: str, output_dir: Path) -> Optional[Dict]:
        """ä½¿ç”¨æ ‡ç­¾é¡µæ§åˆ¶ä¸‹è½½JavaScriptæ–‡ä»¶"""
        if url in self.completed_downloads:
            return None
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸºäºæ–‡ä»¶åï¼‰
        if is_file_already_downloaded(url, self.target_url, 'dynamic'):
            parsed_url = urlparse(url)
            filename = Path(parsed_url.path).name or 'script.js'
            self.logger.info(f"è·³è¿‡å·²å­˜åœ¨æ–‡ä»¶: {filename}")
            self.completed_downloads.add(url)
            return None
            
        async with self.page_semaphore:  # æ§åˆ¶åŒæ—¶æ‰“å¼€çš„æ ‡ç­¾é¡µæ•°é‡
            page_id = f"download_{len(self.active_pages)}"
            page = None
            
            try:
                # åˆ›å»ºæ–°é¡µé¢
                page = await self.context.new_page()
                self.active_pages[page_id] = page
                
                # æ˜¾ç¤ºä¸‹è½½å¼€å§‹ä¿¡æ¯
                parsed_url = urlparse(url)
                filename = Path(parsed_url.path).name or 'script.js'
                self.logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ [{self.stats['js_files_downloaded'] + 1}] {filename}")
                self.logger.info(f"URL: {url}")
                self.logger.info(f" æ ‡ç­¾é¡µçŠ¶æ€: {len(self.active_pages)}/{self.max_workers} æ´»è·ƒ")
                
                # ç›´æ¥è®¿é—®JSæ–‡ä»¶URL
                response = await page.goto(url, timeout=10000)
                
                if response and response.status == 200:
                    content = await response.text()
                    content_bytes = content.encode('utf-8')
                    content_size = len(content_bytes)
                    
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
                    content_hash = get_content_hash(content_bytes)
                    if content_hash in self.content_hashes:
                        existing_filename = self.hash_to_filename.get(content_hash, "æœªçŸ¥æ–‡ä»¶")
                        
                        # åˆ¤æ–­æ˜¯è·¨æ¨¡å¼å»é‡è¿˜æ˜¯æ¨¡å¼å†…å»é‡
                        is_cross_mode = content_hash in self.static_file_hashes
                        
                        if is_cross_mode:
                            self.logger.info(f"è·³è¿‡è·¨æ¨¡å¼é‡å¤æ–‡ä»¶: {filename} (ä¸é™æ€çˆ¬å–çš„ {existing_filename} å†…å®¹ç›¸åŒ)")
                            self.stats['js_files_cross_mode_duplicated'] += 1
                        else:
                            self.logger.info(f"è·³è¿‡é‡å¤æ–‡ä»¶: {filename} (ä¸ {existing_filename} å†…å®¹ç›¸åŒ)")
                            self.stats['js_files_duplicated'] += 1
                        
                        self.completed_downloads.add(url)
                        return {
                            'url': url,
                            'path': None,  # æœªä¿å­˜æ–°æ–‡ä»¶
                            'size': content_size,
                            'duplicate': True,
                            'cross_mode_duplicate': is_cross_mode,
                            'original_file': existing_filename
                        }
                    
                    # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
                    if not filename.endswith('.js'):
                        filename += '.js'
                    
                    # é¿å…æ–‡ä»¶åå†²çª
                    counter = 1
                    original_filename = filename
                    while (output_dir / filename).exists():
                        name, ext = original_filename.rsplit('.', 1)
                        filename = f"{name}_{counter}.{ext}"
                        counter += 1
                    
                    file_path = output_dir / filename
                    
                    # ä¿å­˜æ–‡ä»¶
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    
                    # æ›´æ–°å»é‡ä¿¡æ¯
                    self.content_hashes.add(content_hash)
                    self.hash_to_filename[content_hash] = filename
                    
                    self.completed_downloads.add(url)
                    self.stats['js_files_downloaded'] += 1
                    self.stats['total_size'] += content_size
                    
                    # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
                    if content_size < 1024:
                        size_str = f"{content_size} B"
                    elif content_size < 1024 * 1024:
                        size_str = f"{content_size / 1024:.1f} KB"
                    else:
                        size_str = f"{content_size / (1024 * 1024):.1f} MB"
                    
                    self.logger.info(f"ä¸‹è½½å®Œæˆ: {filename} ({size_str})")
                    self.logger.info(f"ä¿å­˜è·¯å¾„: {file_path}")
                    
                    return {
                        'url': url,
                        'path': str(file_path),
                        'size': content_size,
                        'filename': filename,
                        'duplicate': False,
                        'content_hash': content_hash
                    }
                else:
                    self.logger.warning(f"ä¸‹è½½å¤±è´¥: {url} (çŠ¶æ€ç : {response.status if response else 'None'})")
                    self.stats['js_files_failed'] += 1
                    
            except Exception as e:
                self.logger.error(f"ä¸‹è½½JSæ–‡ä»¶å¤±è´¥ {url}: {e}")
                self.stats['js_files_failed'] += 1
            finally:
                # ç«‹å³å…³é—­é¡µé¢é‡Šæ”¾èµ„æº
                if page:
                    try:
                        await page.close()
                        if page_id in self.active_pages:
                            del self.active_pages[page_id]
                        self.logger.debug(f"ğŸ—‘ï¸ å·²å…³é—­æ ‡ç­¾é¡µ: {page_id} (å‰©ä½™æ´»è·ƒæ ‡ç­¾é¡µ: {len(self.active_pages)})")
                    except:
                        pass
        
        return None

    async def _start_controlled_downloads(self, js_files: Set[str], output_dir: Path):
        """å¯åŠ¨å—æ§çš„ä¸‹è½½ä»»åŠ¡"""
        new_files = js_files - self.completed_downloads
        if not new_files:
            return
            
        self.logger.info(f"ğŸš€ å‘ç° {len(new_files)} ä¸ªæ–°çš„JSæ–‡ä»¶ï¼Œå¼€å§‹ä¸‹è½½...")
        
        # åˆ›å»ºä¸‹è½½è¿›åº¦æ¡
        download_progress = tqdm(
            total=len(new_files),
            desc="ğŸ“¥ ä¸‹è½½JSæ–‡ä»¶",
            unit="æ–‡ä»¶",
            dynamic_ncols=True,
            colour="green",
            position=0,
            leave=False,
            ncols=100,
            bar_format='{desc}: {percentage:3.0f}%|{bar}| {n}/{total} [{elapsed}<{remaining}, {postfix}]'
        )
        
        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        download_tasks = []
        for url in new_files:
            task = asyncio.create_task(self._download_js_with_progress(url, output_dir, download_progress))
            download_tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä¸‹è½½å®Œæˆ
        if download_tasks:
            await asyncio.gather(*download_tasks, return_exceptions=True)
        
        download_progress.close()

    async def _download_js_with_progress(self, url: str, output_dir: Path, progress_bar: tqdm) -> Optional[Dict]:
        """å¸¦è¿›åº¦æ¡çš„JSæ–‡ä»¶ä¸‹è½½"""
        try:
            result = await self._download_js_with_tab_control(url, output_dir)
            
            # æ›´æ–°è¿›åº¦æ¡
            if result:
                file_size = result.get('file_size', 0)
                progress_bar.set_postfix({
                    "å·²å®Œæˆ": len(self.completed_downloads),
                    "æœ€æ–°": result.get('filename', '')[:20] + ('...' if len(result.get('filename', '')) > 20 else '')
                })
            
            progress_bar.update(1)
            return result
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            progress_bar.update(1)
            return None

    async def _extract_files_from_page(self, page: Page, base_url: str) -> Set[str]:
        """ä»é¡µé¢ä¸­æå–JavaScriptå’ŒSource Mapæ–‡ä»¶URL"""
        file_urls = set()
        
        try:
            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆï¼ˆä¸ç­‰å¾…networkidleï¼ŒåŠ è½½è¿‡ç¨‹ä¸­ä¹Ÿèƒ½æ”¶é›†JSï¼‰
            await page.wait_for_load_state('domcontentloaded', timeout=10000)
            
            # æ‰§è¡ŒJavaScriptæ¥è·å–æ‰€æœ‰scriptæ ‡ç­¾
            script_urls = await page.evaluate("""
                () => {
                    const scripts = Array.from(document.querySelectorAll('script[src]'));
                    return scripts.map(script => script.src).filter(src => src);
                }
            """)
            
            # å¤„ç†ç›¸å¯¹URL
            for url in script_urls:
                if url:
                    absolute_url = urljoin(base_url, url)
                    if self._is_supported_file(absolute_url):
                        file_urls.add(absolute_url)
            
            # ç›‘å¬ç½‘ç»œè¯·æ±‚ä¸­çš„JSå’ŒMAPæ–‡ä»¶
            network_files = await page.evaluate("""
                () => {
                    return window.jsFiles || [];
                }
            """)
            
            for url in network_files:
                if url:
                    absolute_url = urljoin(base_url, url)
                    if self._is_supported_file(absolute_url):
                        file_urls.add(absolute_url)
                        
        except Exception as e:
            self.logger.warning(f"æå–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        return file_urls

    def _is_supported_file(self, url: str) -> bool:
        """åˆ¤æ–­URLæ˜¯å¦ä¸ºæ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼ˆJavaScriptæˆ–Source Mapï¼‰"""
        if not url:
            return False
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶ç±»å‹æ£€æµ‹å‡½æ•°
        return is_supported_file(url)

    async def _start_immediate_downloads(self, js_files: Set[str]):
        """ç«‹å³å¼€å§‹ä¸‹è½½æ–°å‘ç°çš„JSæ–‡ä»¶"""
        for js_url in js_files:
            if js_url not in self.discovered_js_files:
                self.discovered_js_files.add(js_url)
                # åˆ›å»ºä¸‹è½½ä»»åŠ¡ä½†ä¸ç­‰å¾…å®Œæˆ
                task = asyncio.create_task(self._download_file_immediate(js_url))
                self.download_tasks.append(task)
                self.logger.debug(f"å·²å¯åŠ¨ä¸‹è½½ä»»åŠ¡: {js_url}")

    async def _download_file_immediate(self, url: str) -> Optional[Dict]:
        """ç«‹å³ä¸‹è½½æ–‡ä»¶ï¼ˆç”¨äºå¹¶å‘ä¸‹è½½ï¼‰"""
        try:
            # åˆ›å»ºæ–°é¡µé¢ç”¨äºä¸‹è½½
            page = await self.context.new_page()
            
            # ç›´æ¥è®¿é—®JSæ–‡ä»¶URL
            response = await page.goto(url, timeout=10000)
            
            if response and response.status == 200:
                content = await response.text()
                
                # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
                file_path = generate_file_path(url, self.target_url, 'dynamic')
                
                # ä¿å­˜æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                # è½¬æ¢ç¼–ç ä¸ºUTF-8
                convert_to_utf8(file_path)
                
                self.logger.info(f"ä¸‹è½½æˆåŠŸ: {url} -> {file_path}")
                
                await page.close()
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats['js_files_downloaded'] += 1
                self.stats['total_size'] += len(content.encode('utf-8'))
                
                return {
                    'url': url,
                    'path': str(file_path),
                    'size': len(content.encode('utf-8'))
                }
            else:
                self.logger.warning(f"ä¸‹è½½å¤±è´¥: {url} (çŠ¶æ€ç : {response.status if response else 'None'})")
                self.stats['js_files_failed'] += 1
                
            await page.close()
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½JSæ–‡ä»¶å¤±è´¥ {url}: {e}")
            self.stats['js_files_failed'] += 1
        
        return None

    async def _visit_page(self, url: str, depth: int = 0, output_dir: Path = None) -> Tuple[Set[str], Set[str]]:
        """è®¿é—®å•ä¸ªé¡µé¢å¹¶æå–JSæ–‡ä»¶å’Œé“¾æ¥"""
        if depth > self.max_depth or url in self.visited_urls:
            return set(), set()
        
        self.visited_urls.add(url)
        js_files = set()
        links = set()
        
        try:
            page = await self.context.new_page()
            
            # è®¾ç½®ç½‘ç»œç›‘å¬æ¥æ•è·JSæ–‡ä»¶è¯·æ±‚
            js_requests = []
            
            async def handle_request(request):
                if self._is_supported_file(request.url):
                    js_requests.append(request.url)
                    self.logger.debug(f"ğŸ” å‘ç°æ–‡ä»¶è¯·æ±‚: {request.url}")
            
            page.on('request', handle_request)
            
            # æ˜¾ç¤ºè¯¦ç»†çš„çˆ¬å–ä¿¡æ¯
            depth_indicator = "  " * depth + "â””â”€" if depth > 0 else ""
            self.logger.info(f"ğŸŒ {depth_indicator}æ­£åœ¨çˆ¬å– [æ·±åº¦ {depth}]: {url}")
            self.logger.info(f" çˆ¬å–è¿›åº¦: å·²è®¿é—® {len(self.visited_urls)} ä¸ªé¡µé¢")
            
            # è®¿é—®é¡µé¢ï¼ˆä¸ç­‰å¾…networkidleï¼Œè¿™æ ·å¯ä»¥åœ¨åŠ è½½è¿‡ç¨‹ä¸­æ”¶é›†JSæ–‡ä»¶ï¼‰
            response = await page.goto(url, wait_until='domcontentloaded', timeout=10000)
            
            if response and response.status >= 400:
                self.logger.warning(f"âš ï¸ é¡µé¢è¿”å›é”™è¯¯çŠ¶æ€: {response.status}")
            
            # çŸ­æš‚ç­‰å¾…è®©åˆå§‹JSåŠ è½½ï¼ˆå‡å°‘ç­‰å¾…æ—¶é—´ï¼‰
            await asyncio.sleep(1)
            
            # è§¦å‘å¯èƒ½çš„åŠ¨æ€åŠ è½½
            await page.evaluate("""
                () => {
                    // æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
                    window.scrollTo(0, document.body.scrollHeight);
                    
                    // è§¦å‘å¸¸è§çš„äº‹ä»¶
                    ['click', 'mouseover', 'focus'].forEach(eventType => {
                        document.querySelectorAll('button, a, input').forEach(el => {
                            try {
                                el.dispatchEvent(new Event(eventType, {bubbles: true}));
                            } catch(e) {}
                        });
                    });
                }
            """)
            
            # å†æ¬¡çŸ­æš‚ç­‰å¾…
            await asyncio.sleep(1)
            
            # æå–JSå’ŒMAPæ–‡ä»¶
            page_files = await self._extract_files_from_page(page, url)
            js_files.update(page_files)
            js_files.update(js_requests)
            
            # ç«‹å³å¼€å§‹ä¸‹è½½æ–°å‘ç°çš„æ–‡ä»¶ï¼ˆä½¿ç”¨æ ‡ç­¾é¡µæ§åˆ¶ï¼‰
            if js_files and output_dir:
                await self._start_controlled_downloads(js_files, output_dir)
            
            # æå–é¡µé¢é“¾æ¥ï¼ˆç”¨äºæ·±åº¦çˆ¬å–ï¼‰
            if depth < self.max_depth:
                try:
                    page_links = await page.evaluate("""
                        () => {
                            const links = Array.from(document.querySelectorAll('a[href]'));
                            return links.map(link => link.href).filter(href => href);
                        }
                    """)
                    
                    base_domain = urlparse(url).netloc
                    for link in page_links:
                        if link and urlparse(link).netloc == base_domain:
                            links.add(link)
                            
                except Exception as e:
                    self.logger.warning(f"æå–é“¾æ¥æ—¶å‡ºé”™: {e}")
            
            await page.close()
            self.stats['pages_visited'] += 1
            
            # æ˜¾ç¤ºé¡µé¢çˆ¬å–ç»“æœ
            depth_indicator = "  " * depth + "â””â”€" if depth > 0 else ""
            if js_files:
                self.logger.info(f" {depth_indicator}é¡µé¢åˆ†æå®Œæˆ: å‘ç° {len(js_files)} ä¸ªJSæ–‡ä»¶")
                for js_file in list(js_files)[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                    self.logger.debug(f"  {Path(urlparse(js_file).path).name}")
                if len(js_files) > 3:
                    self.logger.debug(f"   ... è¿˜æœ‰ {len(js_files) - 3} ä¸ªæ–‡ä»¶")
            else:
                self.logger.info(f" {depth_indicator}é¡µé¢åˆ†æå®Œæˆ: æœªå‘ç°JSæ–‡ä»¶")
            
            if links:
                self.logger.info(f"å‘ç° {len(links)} ä¸ªå†…éƒ¨é“¾æ¥ç”¨äºæ·±åº¦çˆ¬å–")
            
        except Exception as e:
            self.logger.error(f"è®¿é—®é¡µé¢å¤±è´¥ {url}: {e}")
        
        return js_files, links

    async def crawl_website(self, start_url: str, output_dir: Path) -> Dict:
        """çˆ¬å–ç½‘ç«™çš„JavaScriptæ–‡ä»¶"""
        self.logger.info(f"ğŸš€ å¼€å§‹åŠ¨æ€çˆ¬å–")
        self.logger.info(f" ç›®æ ‡ç½‘ç«™: {start_url}")
        self.logger.info(f" çˆ¬å–é…ç½®: æœ€å¤§æ·±åº¦ {self.max_depth}, æ ‡ç­¾é¡µæ§åˆ¶ {self.max_workers}")
        self.logger.info(f"ğŸŒ æµè§ˆå™¨æ¨¡å¼: {'æ— å¤´æ¨¡å¼' if self.headless else 'æœ‰å¤´æ¨¡å¼'} ({self.browser_type})")
        self.logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        self.logger.info(f"{'='*60}")
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir.mkdir(parents=True, exist_ok=True)
            self.stats['start_time'] = time.time()
            
            # å¼€å§‹çˆ¬å–
            to_visit = [(start_url, 0)]
            all_js_files = set()
            
            # ä¼°ç®—æ€»é¡µé¢æ•°ï¼ˆåŸºäºæ·±åº¦ï¼‰
            estimated_pages = min(100, sum(10 ** i for i in range(self.max_depth + 1)))
            
            # åˆ›å»ºé¡µé¢è®¿é—®è¿›åº¦æ¡
            page_progress = tqdm(
                total=estimated_pages,
                desc="ğŸŒ é¡µé¢çˆ¬å–",
                unit="é¡µé¢",
                dynamic_ncols=True,
                colour="blue",
                position=0,
                leave=False,
                ncols=100,
                bar_format='{desc}: {percentage:3.0f}%|{bar}| {n}/{total} [{elapsed}<{remaining}, {postfix}]'
            )
            
            while to_visit:
                current_url, depth = to_visit.pop(0)
                
                if depth > self.max_depth:
                    continue
                
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                page_progress.set_description(f"ğŸŒ çˆ¬å–æ·±åº¦{depth}")
                
                js_files, links = await self._visit_page(current_url, depth, output_dir)
                all_js_files.update(js_files)
                
                # æ›´æ–°è¿›åº¦æ¡
                page_progress.update(1)
                page_progress.set_postfix({
                    "å·²è®¿é—®": len(self.visited_urls),
                    "å¾…è®¿é—®": len(to_visit),
                    "JSæ–‡ä»¶": len(all_js_files)
                })
                
                # æ·»åŠ æ–°å‘ç°çš„é“¾æ¥åˆ°å¾…è®¿é—®åˆ—è¡¨
                for link in links:
                    if link not in self.visited_urls:
                        to_visit.append((link, depth + 1))
            
            page_progress.close()
            
            self.stats['js_files_found'] = len(all_js_files)
            self.stats['total_discovered'] = len(all_js_files)  # æ·»åŠ total_discoveredå­—æ®µ
            self.stats['end_time'] = time.time()
            
            # ä¸‹è½½å‰©ä½™çš„JSæ–‡ä»¶
            remaining_files = all_js_files - self.completed_downloads
            if remaining_files:
                self.logger.info(f"ğŸ“¥ ä¸‹è½½å‰©ä½™çš„ {len(remaining_files)} ä¸ªJavaScriptæ–‡ä»¶...")
                await self._start_controlled_downloads(remaining_files, output_dir)
            
            duration = self.stats['end_time'] - self.stats['start_time']
            
            # è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
            total_found = len(all_js_files)
            downloaded = self.stats['js_files_downloaded']
            duplicated = self.stats['js_files_duplicated']
            cross_mode_duplicated = self.stats['js_files_cross_mode_duplicated']
            failed = self.stats['js_files_failed']
            total_duplicated = duplicated + cross_mode_duplicated
            
            self.logger.info(f"çˆ¬å–å®Œæˆ! å‘ç° {total_found} ä¸ªJSæ–‡ä»¶ï¼Œä¸‹è½½ {downloaded} ä¸ªï¼Œé‡å¤ {total_duplicated} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ªï¼Œè€—æ—¶ {duration:.2f}ç§’")
            
            if total_duplicated > 0:
                if cross_mode_duplicated > 0:
                    self.logger.info(f"å»é‡æ•ˆæœ: èŠ‚çœäº† {total_duplicated} ä¸ªé‡å¤æ–‡ä»¶çš„ä¸‹è½½ (å…¶ä¸­ {cross_mode_duplicated} ä¸ªä¸ºè·¨æ¨¡å¼å»é‡)")
                else:
                    self.logger.info(f"å»é‡æ•ˆæœ: èŠ‚çœäº† {total_duplicated} ä¸ªé‡å¤æ–‡ä»¶çš„ä¸‹è½½")
            
        except Exception as e:
            self.logger.error(f"åŠ¨æ€çˆ¬å–å¤±è´¥: {e}")
            raise
        
        # æ·»åŠ å»é‡ç»Ÿè®¡ä¿¡æ¯åˆ°è¿”å›ç»“æœ
        result = self.stats.copy()
        result['duplicated_files'] = self.stats['js_files_duplicated']
        result['cross_mode_duplicated_files'] = self.stats['js_files_cross_mode_duplicated']
        return result




async def main():
    """æµ‹è¯•å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python playwright_crawler.py <URL>")
        return
    
    url = sys.argv[1]
    output_dir = Path("test_output")
    
    async with PlaywrightCrawler(max_depth=1, wait_time=3) as crawler:
        stats = await crawler.crawl_website(url, output_dir)
        print(f"çˆ¬å–å®Œæˆ: {stats}")


if __name__ == "__main__":
    asyncio.run(main())