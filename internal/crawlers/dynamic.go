package crawlers

import (
	"context"
	"crypto/sha256"
	"crypto/tls"
	"encoding/base64"
	"errors"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/RecoveryAshes/JsFIndcrack/internal/models"
	"github.com/RecoveryAshes/JsFIndcrack/internal/utils"
	"github.com/go-rod/rod"
	"github.com/go-rod/rod/lib/launcher"
	"github.com/go-rod/rod/lib/proto"
	"github.com/google/uuid"
)

// é”™è¯¯ç±»å‹å®šä¹‰ (Feature 010-fix-domain-crawl-bugs)
var (
	ErrBrowserCrashed    = errors.New("æµè§ˆå™¨å´©æºƒ")
	ErrMaxRetriesReached = errors.New("å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
	ErrInvalidContent    = errors.New("æ— æ•ˆå†…å®¹,éJSæ–‡ä»¶")
)

// DynamicCrawler åŠ¨æ€çˆ¬å–å™¨(ä½¿ç”¨Rod)
type DynamicCrawler struct {
	browser   *rod.Browser
	config    models.CrawlConfig
	outputDir string
	domain    string

	// HTTPå¤´éƒ¨æä¾›è€…
	headerProvider models.HeaderProvider

	// æ–‡ä»¶å­˜å‚¨
	jsFiles  map[string]*models.JSFile  // URL -> JSFile
	mapFiles map[string]*models.MapFile // URL -> MapFile
	mu       sync.RWMutex               // ä¿æŠ¤maps

	// å…¨å±€æ–‡ä»¶å“ˆå¸Œè¡¨(ç”¨äºè·¨çˆ¬å–å™¨å»é‡)
	globalFileHashes map[string]string // hash -> URL (shared with static crawler)
	globalMu         *sync.RWMutex     // ä¿æŠ¤globalFileHashesçš„äº’æ–¥é”

	// ç»Ÿè®¡
	visitedURLs []string
	stats       models.TaskStats

	// æ–°å¢: è‡ªé€‚åº”æ ‡ç­¾é¡µæ± 
	pagePool        *PagePool
	resourceMonitor *ResourceMonitor
	urlQueue        *URLQueue

	// æ ‡ç­¾é¡µIDæ˜ å°„ (ç”¨äºæ—¥å¿—æ˜¾ç¤º)
	pageIDs   map[*rod.Page]int
	pageIDsMu sync.RWMutex
	nextPageID int

	// æµè§ˆå™¨ä¼šè¯ç®¡ç† (Feature 010-fix-domain-crawl-bugs)
	browserRetryCount int // å½“å‰æµè§ˆå™¨é‡å¯æ¬¡æ•°
	maxBrowserRetries int // æœ€å¤§æµè§ˆå™¨é‡å¯æ¬¡æ•°(é»˜è®¤3)

	// Workeræ´»è·ƒè®¡æ•°å™¨(ç”¨äºæ£€æµ‹æ‰€æœ‰workerç©ºé—²)
	activeWorkers int32 // ä½¿ç”¨atomicæ“ä½œ
	workersMu     sync.Mutex

	ctx    context.Context
	cancel context.CancelFunc
}

// NewDynamicCrawler åˆ›å»ºåŠ¨æ€çˆ¬å–å™¨
func NewDynamicCrawler(config models.CrawlConfig, outputDir string, domain string, globalFileHashes map[string]string, globalMu *sync.RWMutex, headerProvider models.HeaderProvider) *DynamicCrawler {
	ctx, cancel := context.WithCancel(context.Background())

	dc := &DynamicCrawler{
		config:            config,
		outputDir:         outputDir,
		domain:            domain,
		headerProvider:    headerProvider,
		jsFiles:           make(map[string]*models.JSFile),
		mapFiles:          make(map[string]*models.MapFile),
		globalFileHashes:  globalFileHashes,
		globalMu:          globalMu,
		visitedURLs:       make([]string, 0),
		stats:             models.TaskStats{},
		pageIDs:           make(map[*rod.Page]int),
		nextPageID:        1,
		browserRetryCount: 0, // åˆå§‹åŒ–é‡è¯•è®¡æ•°
		maxBrowserRetries: 3, // é»˜è®¤æœ€å¤šé‡å¯3æ¬¡
		ctx:               ctx,
		cancel:            cancel,
	}

	return dc
}

// Crawl å¼€å§‹åŠ¨æ€çˆ¬å– (Feature 010-fix-domain-crawl-bugs: T029-T032)
// æ”¯æŒæµè§ˆå™¨å´©æºƒè‡ªåŠ¨é‡å¯,æœ€å¤šé‡è¯•3æ¬¡
func (dc *DynamicCrawler) Crawl(targetURL string) error {
	startTime := time.Now()

	// éªŒè¯å…¥å£URL
	if targetURL == "" {
		return fmt.Errorf("å…¥å£URLä¸ºç©º,æ— æ³•å¼€å§‹çˆ¬å–")
	}

	// éªŒè¯URLæ ¼å¼
	parsedTestURL, err := url.Parse(targetURL)
	if err != nil || parsedTestURL.Scheme == "" || parsedTestURL.Host == "" {
		return fmt.Errorf("å…¥å£URLæ ¼å¼æ— æ•ˆ: %s", targetURL)
	}

	utils.Infof("ğŸŒ åŠ¨æ€çˆ¬å–æ¨¡å¼å¯åŠ¨(è‡ªé€‚åº”æ ‡ç­¾é¡µæ± )")
	utils.Infof("ç›®æ ‡URL: %s", targetURL)
	utils.Infof("ç­‰å¾…æ—¶é—´: %dç§’", dc.config.WaitTime)
	utils.Infof("æœ€å¤§æ·±åº¦: %d", dc.config.Depth)

	// è§£æç›®æ ‡URLè·å–åŸŸå
	parsedURL, err := url.Parse(targetURL)
	if err != nil {
		return fmt.Errorf("è§£æç›®æ ‡URLå¤±è´¥: %w", err)
	}
	targetDomain := parsedURL.Host

	// åˆå§‹åŒ–ResourceMonitor (åœ¨é‡è¯•å¾ªç¯å¤–,é¿å…é‡å¤åˆ›å»º)
	resourceConfig := ResourceMonitorConfig{
		SafetyReserveMemory: 1024 * 1024 * 1024, // 1GB
		SafetyThreshold:     500 * 1024 * 1024,  // 500MB
		CPULoadThreshold:    80,                 // 80%
		MaxTabsLimit:        16,                 // 16ä¸ªæ ‡ç­¾é¡µ
		TabMemoryUsage:      100 * 1024 * 1024,  // 100MB per tab
	}
	dc.resourceMonitor = NewResourceMonitor(resourceConfig)
	dc.resourceMonitor.StartMonitoring(1 * time.Second)
	defer dc.resourceMonitor.StopMonitoring()

	// åˆå§‹åŒ–URLQueue (åœ¨é‡è¯•å¾ªç¯å¤–,ä¿æŒvisitedURLsçŠ¶æ€ - T033)
	dc.urlQueue = NewURLQueue(targetDomain, dc.config.AllowCrossDomain, dc.config.Depth)
	defer dc.urlQueue.Close()

	// å°†å…¥å£URLæ·»åŠ åˆ°é˜Ÿåˆ—
	err = dc.urlQueue.Push(targetURL, 0)
	if err != nil {
		return fmt.Errorf("æ·»åŠ å…¥å£URLå¤±è´¥: %w", err)
	}

	// T030: æµè§ˆå™¨å´©æºƒé‡è¯•å¾ªç¯ (æœ€å¤š3æ¬¡)
	for dc.browserRetryCount = 0; dc.browserRetryCount <= dc.maxBrowserRetries; dc.browserRetryCount++ {
		// å¯åŠ¨æµè§ˆå™¨
		if err := dc.launchBrowser(); err != nil {
			utils.Errorf("æµè§ˆå™¨å¯åŠ¨å¤±è´¥(é‡è¯•%d/%d): %v", dc.browserRetryCount, dc.maxBrowserRetries, err)
			if dc.browserRetryCount == dc.maxBrowserRetries {
				return fmt.Errorf("æµè§ˆå™¨å¯åŠ¨å¤±è´¥,å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: %w", err)
			}
			// T032: æµè§ˆå™¨é‡å¯Warnæ—¥å¿—
			utils.Warnf("æµè§ˆå™¨å¯åŠ¨å¤±è´¥,å‡†å¤‡é‡å¯(é‡è¯•%d/%d)", dc.browserRetryCount+1, dc.maxBrowserRetries)
			time.Sleep(2 * time.Second) // ç­‰å¾…2ç§’åé‡è¯•
			continue
		}

		// è®°å½•è¯ä¹¦è·³è¿‡ä¿¡æ¯ (WARNçº§åˆ«æ—¥å¿—)
		utils.Warnf("æµè§ˆå™¨å·²é…ç½®ä¸ºè·³è¿‡HTTPSè¯ä¹¦éªŒè¯,é€‚ç”¨äºå†…ç½‘/å¼€å‘ç¯å¢ƒçš„è‡ªç­¾åè¯ä¹¦")

		// T029: è°ƒç”¨crawlWithBrowseræ‰§è¡Œçˆ¬å–é€»è¾‘
		err = dc.crawlWithBrowser(targetURL, targetDomain)

		// å…³é—­æµè§ˆå™¨
		dc.closeBrowser()

		// T030: æ£€æµ‹æµè§ˆå™¨å´©æºƒ
		if errors.Is(err, ErrBrowserCrashed) {
			dc.stats.BrowserRestarts++ // è®°å½•é‡å¯æ¬¡æ•°
			utils.Warnf("æµè§ˆå™¨å´©æºƒ,å‡†å¤‡é‡å¯(é‡è¯•%d/%d)", dc.browserRetryCount+1, dc.maxBrowserRetries)

			// å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°,è¿”å›é”™è¯¯
			if dc.browserRetryCount == dc.maxBrowserRetries {
				return fmt.Errorf("æµè§ˆå™¨å´©æºƒ,å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: %w", ErrMaxRetriesReached)
			}

			time.Sleep(2 * time.Second) // ç­‰å¾…2ç§’åé‡å¯
			continue                    // ç»§ç»­é‡è¯•å¾ªç¯
		}

		// å…¶ä»–é”™è¯¯æˆ–æˆåŠŸå®Œæˆ,é€€å‡ºé‡è¯•å¾ªç¯
		if err != nil {
			return err
		}
		break // æˆåŠŸå®Œæˆ,é€€å‡ºé‡è¯•å¾ªç¯
	}

	// è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
	duration := time.Since(startTime)
	dc.stats.Duration = duration.Seconds()

	utils.Infof("âœ… åŠ¨æ€çˆ¬å–å®Œæˆ")
	utils.Infof("è®¿é—®URLæ•°: %d", dc.stats.VisitedURLs)
	utils.Infof("ä¸‹è½½æ–‡ä»¶æ•°: %d", dc.stats.DynamicFiles)
	utils.Infof("å¤±è´¥æ–‡ä»¶æ•°: %d", dc.stats.FailedFiles)
	if dc.stats.BrowserRestarts > 0 {
		utils.Infof("æµè§ˆå™¨é‡å¯æ¬¡æ•°: %d", dc.stats.BrowserRestarts)
	}
	utils.Infof("æ€»è€—æ—¶: %.2fç§’", dc.stats.Duration)

	return nil
}

// crawlWithBrowser åœ¨æµè§ˆå™¨å®ä¾‹ä¸­æ‰§è¡Œçˆ¬å–é€»è¾‘ (T029, T031)
// è¿”å›ErrBrowserCrashedè¡¨ç¤ºæµè§ˆå™¨å´©æºƒ,éœ€è¦é‡å¯
func (dc *DynamicCrawler) crawlWithBrowser(targetURL string, targetDomain string) (err error) {
	// T031: ä½¿ç”¨deferæ•è·panic,è½¬æ¢ä¸ºErrBrowserCrashed
	defer func() {
		if r := recover(); r != nil {
			utils.Errorf("æµè§ˆå™¨æ“ä½œpanic: %v", r)
			err = ErrBrowserCrashed
		}
	}()

	// åˆå§‹åŒ–PagePool (æ¯æ¬¡æµè§ˆå™¨é‡å¯éƒ½éœ€è¦é‡æ–°åˆ›å»º)
	dc.pagePool = NewPagePool(dc.browser, dc.resourceMonitor, dc.urlQueue, dc.ctx)
	defer dc.pagePool.Close()

	// T039 [EC2]: è®¡ç®—åˆå§‹workeræ•°é‡ä¸ºmin(16, resourceMonitor.CalculateMaxTabs())
	maxWorkerLimit := 16
	initialMaxTabs := dc.resourceMonitor.CalculateMaxTabs()
	maxWorkers := maxWorkerLimit
	if initialMaxTabs < maxWorkerLimit {
		maxWorkers = initialMaxTabs
	}
	if maxWorkers < 1 {
		maxWorkers = 1
	}

	// T041 [EC2]: workerå¯åŠ¨Debugæ—¥å¿—
	utils.Debugf("åŠ¨æ€çˆ¬å–å¯åŠ¨: åˆå§‹workeræ•°é‡=%d, å¯ç”¨æ ‡ç­¾é¡µæ•°=%d, æœ€å¤§é™åˆ¶=%d",
		maxWorkers, initialMaxTabs, maxWorkerLimit)

	utils.Infof("å¼€å§‹çˆ¬å–,åˆå§‹æ ‡ç­¾é¡µæ•°: 1")

	// T040 [EC2]: æ·»åŠ goroutine,æ¯5ç§’æ£€æŸ¥èµ„æºå¹¶è°ƒç”¨pagePool.AdjustSize
	adjustCtx, adjustCancel := context.WithCancel(dc.ctx)
	defer adjustCancel()

	go func() {
		ticker := time.NewTicker(5 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-adjustCtx.Done():
				return
			case <-ticker.C:
				// è·å–é˜Ÿåˆ—ä¸­å¾…å¤„ç†URLæ•°é‡
				pendingCount := dc.urlQueue.PendingCount()
				// è°ƒç”¨PagePoolçš„åŠ¨æ€è°ƒæ•´æ–¹æ³•
				dc.pagePool.AdjustSize(pendingCount)
			}
		}
	}()

	// æ·»åŠ ç›‘æ§goroutine,æ£€æµ‹æ‰€æœ‰workerç©ºé—²ä¸”é˜Ÿåˆ—ä¸ºç©ºæ—¶å…³é—­é˜Ÿåˆ—
	go func() {
		ticker := time.NewTicker(500 * time.Millisecond)
		defer ticker.Stop()

		for {
			select {
			case <-adjustCtx.Done():
				return
			case <-ticker.C:
				// æ£€æŸ¥æ˜¯å¦æ‰€æœ‰workeréƒ½ç©ºé—²ä¸”é˜Ÿåˆ—ä¸ºç©º
				activeCount := atomic.LoadInt32(&dc.activeWorkers)
				pendingCount := dc.urlQueue.PendingCount()

				if activeCount == 0 && pendingCount == 0 {
					// æ‰€æœ‰workerç©ºé—²ä¸”é˜Ÿåˆ—ä¸ºç©º,å…³é—­é˜Ÿåˆ—
					utils.Debugf("æ£€æµ‹åˆ°æ‰€æœ‰workerç©ºé—²ä¸”é˜Ÿåˆ—ä¸ºç©º,å…³é—­é˜Ÿåˆ—")
					dc.urlQueue.Close()
					return
				}
			}
		}
	}()

	// Worker poolæ¨¡å¼å¤„ç†URLé˜Ÿåˆ—
	var wg sync.WaitGroup

	// åˆå§‹åŒ–æ´»è·ƒworkeræ•°é‡
	atomic.StoreInt32(&dc.activeWorkers, int32(maxWorkers))

	for i := 0; i < maxWorkers; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			dc.worker(workerID)
		}(i)
	}

	wg.Wait()

	return nil
}

// worker Worker goroutine,ä»é˜Ÿåˆ—æ‹‰å–URLå¹¶çˆ¬å–
func (dc *DynamicCrawler) worker(workerID int) {
	for {
		// Workerè¿›å…¥ç©ºé—²çŠ¶æ€(ç­‰å¾…URL)
		atomic.AddInt32(&dc.activeWorkers, -1)

		// ä»é˜Ÿåˆ—è·å–URL
		urlStr, depth, ok := dc.urlQueue.Pop(dc.ctx)
		if !ok {
			// é˜Ÿåˆ—å·²å…³é—­æˆ–contextå–æ¶ˆ
			return
		}

		// Workerè¿›å…¥å·¥ä½œçŠ¶æ€
		atomic.AddInt32(&dc.activeWorkers, 1)

		// æ£€æŸ¥é˜Ÿåˆ—é•¿åº¦,åŠ¨æ€è°ƒæ•´æ ‡ç­¾é¡µæ± å¤§å°
		pendingCount := dc.urlQueue.PendingCount()
		dc.pagePool.AdjustSize(pendingCount)

		// çˆ¬å–é¡µé¢
		err := dc.crawlPage(urlStr, depth)
		if err != nil {
			utils.Warnf("Worker %d çˆ¬å–å¤±è´¥ [%s]: %v", workerID, urlStr, err)
		}

		// ä¸åœ¨è¿™é‡Œæ£€æŸ¥é€€å‡ºæ¡ä»¶,è®©Popé˜»å¡ç­‰å¾…æ–°URL
		// å½“é˜Ÿåˆ—å…³é—­æ—¶,Popä¼šè¿”å›ok=false,workerè‡ªç„¶é€€å‡º
	}
}

// launchBrowser å¯åŠ¨æµè§ˆå™¨
func (dc *DynamicCrawler) launchBrowser() error {
	// é…ç½®launcher
	l := launcher.New()

	if dc.config.Headless {
		l = l.Headless(true)
	} else {
		l = l.Headless(false)
	}

	// Bug #1ä¿®å¤: æ·»åŠ è¯ä¹¦å¿½ç•¥å‚æ•°,å…è®¸è®¿é—®è‡ªç­¾åã€è¿‡æœŸæˆ–ä¸»æœºåä¸åŒ¹é…çš„HTTPSç«™ç‚¹
	// å‚è€ƒ: research.md - TLSè¯ä¹¦éªŒè¯ä¿®å¤æ–¹æ¡ˆ
	l = l.Set("ignore-certificate-errors")
	utils.Debugf("æµè§ˆå™¨å¯åŠ¨å‚æ•°: --ignore-certificate-errors (è·³è¿‡TLSè¯ä¹¦éªŒè¯)")

	// å¯åŠ¨æµè§ˆå™¨
	controlURL, err := l.Launch()
	if err != nil {
		return fmt.Errorf("å¯åŠ¨æµè§ˆå™¨å¤±è´¥: %w", err)
	}

	// è¿æ¥åˆ°æµè§ˆå™¨
	dc.browser = rod.New().ControlURL(controlURL)
	if err := dc.browser.Connect(); err != nil {
		return fmt.Errorf("è¿æ¥æµè§ˆå™¨å¤±è´¥: %w", err)
	}

	utils.Debugf("æµè§ˆå™¨å·²å¯åŠ¨: %s", controlURL)
	return nil
}

// closeBrowser å…³é—­æµè§ˆå™¨
func (dc *DynamicCrawler) closeBrowser() {
	if dc.browser != nil {
		dc.cancel()
		dc.browser.MustClose()
		utils.Debugf("æµè§ˆå™¨å·²å…³é—­")
	}
}

// setupNetworkIntercept è®¾ç½®ç½‘ç»œè¯·æ±‚æ‹¦æˆª
func (dc *DynamicCrawler) setupNetworkIntercept(page *rod.Page) error {
	// åˆ†é…å¹¶æ³¨å†Œé¡µé¢ID
	dc.pageIDsMu.Lock()
	pageID := dc.nextPageID
	dc.pageIDs[page] = pageID
	dc.nextPageID++
	dc.pageIDsMu.Unlock()

	// å¯ç”¨ç½‘ç»œåŸŸ
	router := page.HijackRequests()

	router.MustAdd("*", func(ctx *rod.Hijack) {
		// åº”ç”¨è‡ªå®šä¹‰HTTPå¤´éƒ¨
		if dc.headerProvider != nil {
			headers, err := dc.headerProvider.GetHeaders()
			if err != nil {
				utils.Warnf("è·å–HTTPå¤´éƒ¨å¤±è´¥: %v", err)
			} else {
				for name, values := range headers {
					if len(values) > 0 {
						ctx.Request.Req().Header.Set(name, values[0])
					}
				}
			}
		}

		// è®©æµè§ˆå™¨ç»§ç»­å¤„ç†è¯·æ±‚(ä¸æ‹¦æˆª,åªç›‘å¬å“åº”)
		ctx.ContinueRequest(&proto.FetchContinueRequest{})
	})

	// ç›‘å¬å“åº”å®Œæˆäº‹ä»¶æ¥æ•è·JSæ–‡ä»¶
	go page.EachEvent(func(e *proto.NetworkResponseReceived) {
		// æ£€æŸ¥æ˜¯å¦ä¸ºJavaScriptæ–‡ä»¶
		resp := e.Response
		if resp.MIMEType == "application/javascript" || resp.MIMEType == "text/javascript" ||
			strings.HasSuffix(resp.URL, ".js") {
			utils.Debugf("æ£€æµ‹åˆ°JSå“åº”: %s", resp.URL)

			// è·å–å“åº”ä½“
			body, err := proto.NetworkGetResponseBody{RequestID: e.RequestID}.Call(page)
			if err != nil {
				utils.Warnf("è·å–å“åº”ä½“å¤±è´¥ [%s]: %v", resp.URL, err)
				return
			}

			var content []byte
			if body.Base64Encoded {
				content, err = base64.StdEncoding.DecodeString(body.Body)
				if err != nil {
					utils.Warnf("è§£ç Base64å¤±è´¥ [%s]: %v", resp.URL, err)
					return
				}
			} else {
				content = []byte(body.Body)
			}

			// ä¸‹è½½JSæ–‡ä»¶,ä¼ å…¥é¡µé¢ID
			contentType := resp.MIMEType
			if contentType == "" {
				contentType = "application/javascript"
			}
			if err := dc.downloadJSFileWithPageID(resp.URL, content, contentType, pageID); err != nil {
				utils.Warnf("ä¸‹è½½JSæ–‡ä»¶å¤±è´¥ [%s]: %v", resp.URL, err)
			}
		}
	})()

	go router.Run()

	return nil
}

// crawlPage çˆ¬å–å•ä¸ªé¡µé¢
func (dc *DynamicCrawler) crawlPage(pageURL string, depth int) (err error) {
	// T030-T031 [US2]: æ·»åŠ defer+recoveræœºåˆ¶æ•è·panic,è®°å½•ç»“æ„åŒ–é”™è¯¯æ—¥å¿—
	defer func() {
		if r := recover(); r != nil {
			// æ•è·panicå¹¶è½¬æ¢ä¸ºerror
			err = fmt.Errorf("é¡µé¢çˆ¬å–panic: %v", r)

			// T031: è®°å½•ç»“æ„åŒ–é”™è¯¯æ—¥å¿—(åŒ…å«URLã€é”™è¯¯ç±»å‹ã€å †æ ˆè·Ÿè¸ª)
			utils.Errorf("æ•è·panic: URL=%s, æ·±åº¦=%d, é”™è¯¯=%v, ç±»å‹=panicæ¢å¤", pageURL, depth, r)

			// å¢åŠ å¤±è´¥è®¡æ•°
			dc.mu.Lock()
			dc.stats.FailedFiles++
			dc.mu.Unlock()
		}
	}()

	// æ ‡è®°ä¸ºå·²è®¿é—®
	dc.urlQueue.MarkVisited(pageURL)

	// è®°å½•è®¿é—®
	dc.mu.Lock()
	dc.visitedURLs = append(dc.visitedURLs, pageURL)
	dc.stats.VisitedURLs++
	dc.mu.Unlock()

	utils.Debugf("è®¿é—®é¡µé¢: %s (æ·±åº¦: %d)", pageURL, depth)

	// ä»PagePoolè·å–æ ‡ç­¾é¡µ
	page, pageErr := dc.pagePool.AcquirePage(dc.ctx)
	if pageErr != nil {
		utils.Errorf("è·å–æ ‡ç­¾é¡µå¤±è´¥ [%s]: %v", pageURL, pageErr)
		dc.stats.FailedFiles++
		return pageErr
	}
	defer dc.pagePool.ReleasePage(page)

	// è®¾ç½®ç½‘ç»œæ‹¦æˆª,æ•è·åŠ¨æ€åŠ è½½çš„JSæ–‡ä»¶
	// ä½¿ç”¨LoadResponseè®©æµè§ˆå™¨å¤„ç†è¯·æ±‚(æµè§ˆå™¨å·²é…ç½®--ignore-certificate-errors)
	if interceptErr := dc.setupNetworkIntercept(page); interceptErr != nil {
		utils.Warnf("è®¾ç½®ç½‘ç»œæ‹¦æˆªå¤±è´¥ [%s]: %v", pageURL, interceptErr)
	}

	// å¯¼èˆªåˆ°ç›®æ ‡URL
	if navErr := page.Navigate(pageURL); navErr != nil {
		utils.Errorf("å¯¼èˆªå¤±è´¥ [%s]: %v", pageURL, navErr)
		dc.stats.FailedFiles++
		return navErr
	}

	// ç­‰å¾…é¡µé¢åŠ è½½
	if loadErr := page.WaitLoad(); loadErr != nil {
		utils.Errorf("ç­‰å¾…é¡µé¢åŠ è½½å¤±è´¥ [%s]: %v", pageURL, loadErr)
		return loadErr
	}

	// é¢å¤–ç­‰å¾…æ—¶é—´(ç­‰å¾…åŠ¨æ€JSåŠ è½½)
	time.Sleep(time.Duration(dc.config.WaitTime) * time.Second)

	utils.Debugf("é¡µé¢åŠ è½½å®Œæˆ: %s", pageURL)

	// æå–é¡µé¢é“¾æ¥(å¦‚æœæœªè¾¾åˆ°æœ€å¤§æ·±åº¦)
	if depth < dc.config.Depth {
		// åˆ›å»ºURLExtractor
		parsedURL, _ := url.Parse(pageURL)
		extractor := NewURLExtractor(dc.urlQueue, parsedURL.Host, dc.config.AllowCrossDomain, dc.config.Depth)

		// ä»é¡µé¢æå–é“¾æ¥
		extractedCount, extractErr := extractor.ExtractFromPage(page, pageURL, depth)
		if extractErr != nil {
			utils.Warnf("æå–é“¾æ¥å¤±è´¥ [%s]: %v", pageURL, extractErr)
		} else if extractedCount > 0 {
			utils.Infof("ä»é¡µé¢æå–äº† %d ä¸ªé“¾æ¥: %s", extractedCount, pageURL)

			// è®°å½•å½“å‰çŠ¶æ€
			currentTabs := dc.pagePool.CurrentSize()
			pendingURLs := dc.urlQueue.PendingCount()
			maxTabs := dc.pagePool.MaxSize()
			utils.Infof("å½“å‰æ ‡ç­¾é¡µ: %d, å¾…çˆ¬URLæ•°: %d, æœ€å¤§é™åˆ¶: %d", currentTabs, pendingURLs, maxTabs)
		}
	}

	return nil
}

// downloadJSFile ä¸‹è½½å¹¶ä¿å­˜JavaScriptæ–‡ä»¶
func (dc *DynamicCrawler) downloadJSFile(fileURL string, content []byte, contentType string) error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	// æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
	if _, exists := dc.jsFiles[fileURL]; exists {
		utils.Debugf("æ–‡ä»¶å·²å­˜åœ¨,è·³è¿‡: %s", fileURL)
		return nil
	}

	// è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
	hash := fmt.Sprintf("%x", sha256.Sum256(content))

	// å…ˆæ£€æŸ¥å…¨å±€å“ˆå¸Œè¡¨(è·¨çˆ¬å–å™¨å»é‡)
	if dc.globalFileHashes != nil && dc.globalMu != nil {
		dc.globalMu.RLock()
		if existingURL, exists := dc.globalFileHashes[hash]; exists {
			dc.globalMu.RUnlock()
			utils.Debugf("å‘ç°å…¨å±€é‡å¤æ–‡ä»¶(å“ˆå¸Œç›¸åŒ): %s (ä¸ %s ç›¸åŒ)", fileURL, existingURL)

			// åˆ›å»ºä¸€ä¸ªæ ‡è®°ä¸ºé‡å¤çš„JSFileå¯¹è±¡,ä½†ä¸ä¿å­˜åˆ°ç£ç›˜
			jsFile := &models.JSFile{
				ID:           uuid.New().String(),
				URL:          fileURL,
				FilePath:     "", // ä¸ä¿å­˜æ–‡ä»¶
				Hash:         hash,
				Size:         int64(len(content)),
				Extension:    filepath.Ext(fileURL),
				ContentType:  contentType,
				SourceURL:    fileURL,
				CrawlMode:    models.ModeDynamic,
				Depth:        0,
				IsObfuscated: false,
				IsDuplicate:  true,
				DownloadedAt: time.Now(),
				HasMapFile:   false,
			}
			dc.jsFiles[fileURL] = jsFile
			return nil
		}
		dc.globalMu.RUnlock()
	}

	// æ£€æŸ¥æœ¬åœ°å“ˆå¸Œå»é‡
	for _, existingFile := range dc.jsFiles {
		if existingFile.Hash == hash {
			utils.Debugf("å‘ç°é‡å¤æ–‡ä»¶(å“ˆå¸Œç›¸åŒ): %s", fileURL)
			dc.jsFiles[fileURL] = existingFile
			existingFile.IsDuplicate = true
			return nil
		}
	}

	// ç”Ÿæˆæ–‡ä»¶è·¯å¾„
	filePath, err := dc.generateFilePath(fileURL, "encode/js")
	if err != nil {
		return fmt.Errorf("ç”Ÿæˆæ–‡ä»¶è·¯å¾„å¤±è´¥: %w", err)
	}

	// ç¡®ä¿ç›®å½•å­˜åœ¨
	if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {
		return fmt.Errorf("åˆ›å»ºç›®å½•å¤±è´¥: %w", err)
	}

	// å†™å…¥æ–‡ä»¶
	if err := os.WriteFile(filePath, content, 0644); err != nil {
		return fmt.Errorf("å†™å…¥æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// åˆ›å»ºJSFileå¯¹è±¡
	jsFile := &models.JSFile{
		ID:           uuid.New().String(),
		URL:          fileURL,
		FilePath:     filePath,
		Hash:         hash,
		Size:         int64(len(content)),
		Extension:    filepath.Ext(fileURL),
		ContentType:  contentType,
		SourceURL:    fileURL,
		CrawlMode:    models.ModeDynamic,
		Depth:        0, // TODO: è·Ÿè¸ªå®é™…æ·±åº¦
		IsObfuscated: false,
		DownloadedAt: time.Now(),
		HasMapFile:   false,
	}

	dc.jsFiles[fileURL] = jsFile
	dc.stats.DynamicFiles++
	dc.stats.TotalFiles++
	dc.stats.TotalSize += int64(len(content))

	// æ·»åŠ åˆ°å…¨å±€å“ˆå¸Œè¡¨
	if dc.globalFileHashes != nil && dc.globalMu != nil {
		dc.globalMu.Lock()
		dc.globalFileHashes[hash] = fileURL
		dc.globalMu.Unlock()
	}

	utils.Infof("ğŸ“¥ ä¸‹è½½æˆåŠŸ: %s (%d bytes) - %s", filepath.Base(filePath), len(content), fileURL)

	// æ£€æŸ¥æ˜¯å¦æœ‰Source Map
	dc.checkAndDownloadSourceMap(fileURL, content)

	return nil
}

// downloadJSFileWithPageID ä¸‹è½½JSæ–‡ä»¶å¹¶ä¿å­˜(å¸¦é¡µé¢IDæ˜¾ç¤º)
func (dc *DynamicCrawler) downloadJSFileWithPageID(fileURL string, content []byte, contentType string, pageID int) error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	// æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
	if _, exists := dc.jsFiles[fileURL]; exists {
		utils.Debugf("æ–‡ä»¶å·²å­˜åœ¨,è·³è¿‡: %s", fileURL)
		return nil
	}

	// è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
	hash := fmt.Sprintf("%x", sha256.Sum256(content))

	// å…ˆæ£€æŸ¥å…¨å±€å“ˆå¸Œè¡¨(è·¨çˆ¬å–å™¨å»é‡)
	if dc.globalFileHashes != nil && dc.globalMu != nil {
		dc.globalMu.RLock()
		if existingURL, exists := dc.globalFileHashes[hash]; exists {
			dc.globalMu.RUnlock()
			utils.Debugf("å‘ç°å…¨å±€é‡å¤æ–‡ä»¶(å“ˆå¸Œç›¸åŒ): %s (ä¸ %s ç›¸åŒ)", fileURL, existingURL)

			// åˆ›å»ºä¸€ä¸ªæ ‡è®°ä¸ºé‡å¤çš„JSFileå¯¹è±¡,ä½†ä¸ä¿å­˜åˆ°ç£ç›˜
			jsFile := &models.JSFile{
				ID:           uuid.New().String(),
				URL:          fileURL,
				FilePath:     "", // ä¸ä¿å­˜æ–‡ä»¶
				Hash:         hash,
				Size:         int64(len(content)),
				Extension:    filepath.Ext(fileURL),
				ContentType:  contentType,
				SourceURL:    fileURL,
				CrawlMode:    models.ModeDynamic,
				Depth:        0,
				IsObfuscated: false,
				IsDuplicate:  true,
				DownloadedAt: time.Now(),
				HasMapFile:   false,
			}
			dc.jsFiles[fileURL] = jsFile
			return nil
		}
		dc.globalMu.RUnlock()
	}

	// æ£€æŸ¥æœ¬åœ°å“ˆå¸Œå»é‡
	for _, existingFile := range dc.jsFiles {
		if existingFile.Hash == hash {
			utils.Debugf("å‘ç°é‡å¤æ–‡ä»¶(å“ˆå¸Œç›¸åŒ): %s", fileURL)
			dc.jsFiles[fileURL] = existingFile
			existingFile.IsDuplicate = true
			return nil
		}
	}

	// ç”Ÿæˆæ–‡ä»¶è·¯å¾„
	filePath, err := dc.generateFilePath(fileURL, "encode/js")
	if err != nil {
		return fmt.Errorf("ç”Ÿæˆæ–‡ä»¶è·¯å¾„å¤±è´¥: %w", err)
	}

	// ç¡®ä¿ç›®å½•å­˜åœ¨
	if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {
		return fmt.Errorf("åˆ›å»ºç›®å½•å¤±è´¥: %w", err)
	}

	// å†™å…¥æ–‡ä»¶
	if err := os.WriteFile(filePath, content, 0644); err != nil {
		return fmt.Errorf("å†™å…¥æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// åˆ›å»ºJSFileå¯¹è±¡
	jsFile := &models.JSFile{
		ID:           uuid.New().String(),
		URL:          fileURL,
		FilePath:     filePath,
		Hash:         hash,
		Size:         int64(len(content)),
		Extension:    filepath.Ext(fileURL),
		ContentType:  contentType,
		SourceURL:    fileURL,
		CrawlMode:    models.ModeDynamic,
		Depth:        0, // TODO: è·Ÿè¸ªå®é™…æ·±åº¦
		IsObfuscated: false,
		DownloadedAt: time.Now(),
		HasMapFile:   false,
	}

	dc.jsFiles[fileURL] = jsFile
	dc.stats.DynamicFiles++
	dc.stats.TotalFiles++
	dc.stats.TotalSize += int64(len(content))

	// æ·»åŠ åˆ°å…¨å±€å“ˆå¸Œè¡¨
	if dc.globalFileHashes != nil && dc.globalMu != nil {
		dc.globalMu.Lock()
		dc.globalFileHashes[hash] = fileURL
		dc.globalMu.Unlock()
	}

	// å¸¦æ ‡ç­¾é¡µIDçš„æ—¥å¿—
	utils.Infof("ğŸ“¥ ä¸‹è½½æˆåŠŸ [æ ‡ç­¾é¡µ#%d]: %s (%d bytes) - %s", pageID, filepath.Base(filePath), len(content), fileURL)

	// æ£€æŸ¥æ˜¯å¦æœ‰Source Map
	dc.checkAndDownloadSourceMap(fileURL, content)

	return nil
}

// checkAndDownloadSourceMap æ£€æŸ¥å¹¶ä¸‹è½½Source Mapæ–‡ä»¶
func (dc *DynamicCrawler) checkAndDownloadSourceMap(jsURL string, jsContent []byte) {
	// åœ¨æ–‡ä»¶å†…å®¹ä¸­æŸ¥æ‰¾sourceMappingURLæ³¨é‡Š
	content := string(jsContent)

	// æŸ¥æ‰¾ //# sourceMappingURL=xxx.map
	if idx := strings.Index(content, "sourceMappingURL="); idx != -1 {
		start := idx + len("sourceMappingURL=")
		end := strings.IndexAny(content[start:], "\n\r ")
		if end == -1 {
			end = len(content) - start
		}

		mapURL := strings.TrimSpace(content[start : start+end])

		// æ„é€ å®Œæ•´URL
		baseURL, _ := url.Parse(jsURL)
		fullMapURL, err := baseURL.Parse(mapURL)
		if err == nil {
			utils.Infof("ğŸ—ºï¸  å‘ç°Source Map: %s", fullMapURL.String())

			// ä¸‹è½½Source Mapæ–‡ä»¶
			dc.downloadSourceMapFile(fullMapURL.String())
		}
	}
}

// downloadSourceMapFile ä¸‹è½½Source Mapæ–‡ä»¶
// æ³¨æ„: è°ƒç”¨æ­¤å‡½æ•°å‰è°ƒç”¨è€…å¿…é¡»å·²æŒæœ‰ dc.mu é”
func (dc *DynamicCrawler) downloadSourceMapFile(mapURL string) {
	// æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½ (ä¸éœ€è¦é¢å¤–åŠ é”,è°ƒç”¨è€…å·²æŒæœ‰é”)
	if _, exists := dc.mapFiles[mapURL]; exists {
		utils.Debugf("Source Mapæ–‡ä»¶å·²å­˜åœ¨,è·³è¿‡: %s", mapURL)
		return
	}

	// ä¸´æ—¶é‡Šæ”¾é”ä»¥æ‰§è¡ŒHTTPè¯·æ±‚(é¿å…é˜»å¡å…¶ä»–æ“ä½œ)
	dc.mu.Unlock()
	defer dc.mu.Lock()

	// HTTPè¶…æ—¶æ—¶é—´ç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶çš„ wait_time å€¼(ç§’)
	httpTimeout := time.Duration(dc.config.WaitTime) * time.Second

	// å‘èµ·HTTPè¯·æ±‚ä¸‹è½½
	client := &http.Client{
		Timeout: httpTimeout,
		Transport: &http.Transport{
			TLSClientConfig: &tls.Config{
				InsecureSkipVerify: true,
			},
		},
	}

	resp, err := client.Get(mapURL)
	if err != nil {
		utils.Warnf("ä¸‹è½½Source Mapå¤±è´¥ [%s]: %v", mapURL, err)
		return
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		utils.Warnf("ä¸‹è½½Source Mapå¤±è´¥ [%s]: HTTP %d", mapURL, resp.StatusCode)
		return
	}

	// è¯»å–å†…å®¹
	content, err := io.ReadAll(resp.Body)
	if err != nil {
		utils.Warnf("è¯»å–Source Mapå†…å®¹å¤±è´¥ [%s]: %v", mapURL, err)
		return
	}

	// ç”Ÿæˆæ–‡ä»¶è·¯å¾„ (ä¿å­˜åˆ° encode/map/{domain}/ ç›®å½•)
	filePath, err := dc.generateFilePath(mapURL, "encode/map")
	if err != nil {
		utils.Warnf("ç”ŸæˆSource Mapæ–‡ä»¶è·¯å¾„å¤±è´¥ [%s]: %v", mapURL, err)
		return
	}

	// ç¡®ä¿ç›®å½•å­˜åœ¨
	if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {
		utils.Warnf("åˆ›å»ºSource Mapç›®å½•å¤±è´¥: %v", err)
		return
	}

	// å†™å…¥æ–‡ä»¶
	if err := os.WriteFile(filePath, content, 0644); err != nil {
		utils.Warnf("å†™å…¥Source Mapæ–‡ä»¶å¤±è´¥: %v", err)
		return
	}

	// æ³¨æ„: æ­¤æ—¶é”å·²ç»è¢«é‡æ–°è·å–(defer dc.mu.Lock())
	// åˆ›å»ºMapFileå¯¹è±¡
	mapFile := &models.MapFile{
		ID:           uuid.New().String(),
		URL:          mapURL,
		FilePath:     filePath,
		Size:         int64(len(content)),
		DownloadedAt: time.Now(),
	}

	dc.mapFiles[mapURL] = mapFile
	dc.stats.MapFiles++

	utils.Infof("ğŸ“¥ ä¸‹è½½Source MapæˆåŠŸ: %s (%d bytes)", filepath.Base(filePath), len(content))
}

// isJavaScriptURL åˆ¤æ–­æ˜¯å¦ä¸ºJavaScriptæ–‡ä»¶URL
func (dc *DynamicCrawler) isJavaScriptURL(urlStr string) bool {
	urlStr = strings.ToLower(urlStr)

	// æ£€æŸ¥æ‰©å±•å
	for _, ext := range models.JSFileExtensions {
		if strings.HasSuffix(urlStr, ext) {
			return true
		}
	}

	// æ£€æŸ¥å¸¸è§JSæ¨¡å¼
	if strings.Contains(urlStr, ".js?") ||
		strings.Contains(urlStr, ".mjs?") ||
		strings.Contains(urlStr, ".jsx?") {
		return true
	}

	// æ£€æŸ¥Content-Type (å¦‚æœå¯ç”¨)
	return false
}

// generateFilePath ç”Ÿæˆæœ¬åœ°æ–‡ä»¶è·¯å¾„
// è·¯å¾„æ ¼å¼: output/{target_domain}/encode/js/{source_domain}/filename.js
// ä¾‹å¦‚: output/www.baidu.com/encode/js/map.baidu.com/app.js
func (dc *DynamicCrawler) generateFilePath(fileURL string, subdir string) (string, error) {
	parsed, err := url.Parse(fileURL)
	if err != nil {
		return "", err
	}

	// ä½¿ç”¨URLè·¯å¾„ä½œä¸ºæ–‡ä»¶å
	filename := filepath.Base(parsed.Path)
	if filename == "" || filename == "." {
		filename = "index.js"
	}

	// è·å–JSæ–‡ä»¶çš„æ¥æºåŸŸå
	sourceDomain := parsed.Host
	if sourceDomain == "" {
		sourceDomain = "unknown"
	}

	// æ„é€ å®Œæ•´è·¯å¾„: output/{target_domain}/encode/js/{source_domain}/filename
	// åœ¨jsç›®å½•ä¸‹æŒ‰æ¥æºåŸŸååˆ†ç±»
	fullPath := filepath.Join(dc.outputDir, dc.domain, subdir, sourceDomain, filename)

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨,æ·»åŠ ç¼–å·
	if _, err := os.Stat(fullPath); err == nil {
		ext := filepath.Ext(filename)
		base := strings.TrimSuffix(filename, ext)
		for i := 1; ; i++ {
			newPath := filepath.Join(dc.outputDir, dc.domain, subdir, sourceDomain, fmt.Sprintf("%s_%d%s", base, i, ext))
			if _, err := os.Stat(newPath); os.IsNotExist(err) {
				fullPath = newPath
				break
			}
		}
	}

	return fullPath, nil
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (dc *DynamicCrawler) GetStats() models.TaskStats {
	dc.mu.RLock()
	defer dc.mu.RUnlock()
	return dc.stats
}

// GetJSFiles è·å–æ‰€æœ‰ä¸‹è½½çš„JSæ–‡ä»¶
func (dc *DynamicCrawler) GetJSFiles() []*models.JSFile {
	dc.mu.RLock()
	defer dc.mu.RUnlock()

	files := make([]*models.JSFile, 0, len(dc.jsFiles))
	for _, f := range dc.jsFiles {
		files = append(files, f)
	}
	return files
}

// Reset é‡ç½®çˆ¬å–å™¨çŠ¶æ€,ç”¨äºæ‰¹é‡çˆ¬å–åœºæ™¯
//
// èŒè´£:
//   - æ¸…ç©ºURLé˜Ÿåˆ—(è°ƒç”¨URLQueue.Reset)
//   - é‡ç½®æ ‡ç­¾é¡µæ± åˆ°1ä¸ªæ ‡ç­¾é¡µ(è°ƒç”¨PagePool.Reset)
//   - æ¸…ç©ºå†…éƒ¨çŠ¶æ€(jsFiles, mapFiles, visitedURLs, stats)
//
// ä½¿ç”¨åœºæ™¯:
//   - æ‰¹é‡çˆ¬å–(-få‚æ•°)ä¸­,æ¯ä¸ªç›®æ ‡å®Œæˆåè°ƒç”¨
//   - ç¡®ä¿ç›®æ ‡é—´çš„å®Œå…¨éš”ç¦»,æ— URLæˆ–æ–‡ä»¶æ±¡æŸ“
//
// æ³¨æ„:
//   - ä¸é‡ç½®å…¨å±€æ–‡ä»¶å“ˆå¸Œè¡¨(globalFileHashes),å› ä¸ºéœ€è¦è·¨ç›®æ ‡å»é‡
//   - ä¸å…³é—­æµè§ˆå™¨,å¤ç”¨åŒä¸€æµè§ˆå™¨å®ä¾‹
func (dc *DynamicCrawler) Reset() error {
	dc.mu.Lock()
	defer dc.mu.Unlock()

	// é‡ç½®URLé˜Ÿåˆ—
	if dc.urlQueue != nil {
		dc.urlQueue.Reset()
	}

	// é‡ç½®æ ‡ç­¾é¡µæ± åˆ°1ä¸ªæ ‡ç­¾é¡µ
	if dc.pagePool != nil {
		if err := dc.pagePool.Reset(); err != nil {
			return fmt.Errorf("é‡ç½®æ ‡ç­¾é¡µæ± å¤±è´¥: %w", err)
		}
	}

	// æ¸…ç©ºå†…éƒ¨çŠ¶æ€
	dc.jsFiles = make(map[string]*models.JSFile)
	dc.mapFiles = make(map[string]*models.MapFile)
	dc.visitedURLs = make([]string, 0)
	dc.stats = models.TaskStats{}

	utils.Debugf("åŠ¨æ€çˆ¬å–å™¨çŠ¶æ€å·²é‡ç½®")
	return nil
}
