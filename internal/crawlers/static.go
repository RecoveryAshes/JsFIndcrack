package crawlers

import (
	"bytes"
	"compress/flate"
	"compress/gzip"
	"crypto/sha256"
	"crypto/tls"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"time"

	"github.com/RecoveryAshes/JsFIndcrack/internal/models"
	"github.com/RecoveryAshes/JsFIndcrack/internal/utils"
	"github.com/andybalholm/brotli"
	"github.com/gocolly/colly/v2"
	"github.com/google/uuid"
)

// StaticCrawler é™æ€çˆ¬å–å™¨(ä½¿ç”¨Colly)
type StaticCrawler struct {
	collector *colly.Collector
	config    models.CrawlConfig
	outputDir string
	domain    string

	// HTTPå¤´éƒ¨æä¾›è€…
	headerProvider models.HeaderProvider

	// æ–‡ä»¶å­˜å‚¨
	jsFiles  map[string]*models.JSFile  // URL -> JSFile
	mapFiles map[string]*models.MapFile // URL -> MapFile
	mu       sync.RWMutex               // ä¿æŠ¤maps

	// å…¨å±€æ–‡ä»¶å“ˆå¸Œè¡¨(ç”¨äºè·¨çˆ¬å–å™¨å»é‡)
	globalFileHashes map[string]string // hash -> URL (shared with dynamic crawler)
	globalMu         *sync.RWMutex     // ä¿æŠ¤globalFileHashesçš„äº’æ–¥é”

	// URLé˜Ÿåˆ—ç®¡ç†(æ›¿ä»£visitedURLs)
	urlQueue *URLQueue

	// èµ„æºç›‘æ§å™¨
	resourceMonitor *ResourceMonitor

	// ç»Ÿè®¡
	stats models.TaskStats
}

// NewStaticCrawler åˆ›å»ºé™æ€çˆ¬å–å™¨
func NewStaticCrawler(config models.CrawlConfig, outputDir string, domain string, globalFileHashes map[string]string, globalMu *sync.RWMutex, headerProvider models.HeaderProvider) *StaticCrawler {
	// Bug #1ä¿®å¤ (T012, T013): åˆ›å»ºè‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯,ç¦ç”¨TLSè¯ä¹¦éªŒè¯
	// å‚è€ƒ: research.md - é™æ€çˆ¬å–è¯ä¹¦ä¿®å¤æ–¹æ¡ˆ
	// HTTPè¶…æ—¶æ—¶é—´ç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶çš„ wait_time å€¼(ç§’)
	httpTimeout := time.Duration(config.WaitTime) * time.Second

	httpClient := &http.Client{
		Transport: &http.Transport{
			TLSClientConfig: &tls.Config{
				InsecureSkipVerify: true, // è·³è¿‡è¯ä¹¦éªŒè¯,å…è®¸è®¿é—®è‡ªç­¾åã€è¿‡æœŸæˆ–ä¸»æœºåä¸åŒ¹é…çš„HTTPSç«™ç‚¹
			},
		},
		Timeout: httpTimeout,
	}
	utils.Debugf("é™æ€çˆ¬å–å™¨: HTTPè¶…æ—¶è®¾ç½®ä¸º %d ç§’ (wait_time=%d)", int(httpTimeout.Seconds()), config.WaitTime)

	// åˆ›å»ºColly collector
	// T053: ä¸ä½¿ç”¨colly.MaxDepth,æ”¹ä¸ºåº”ç”¨å±‚æ‰‹åŠ¨ç®¡ç†æ·±åº¦
	// æ³¨æ„: å¿…é¡»è®¾ç½® colly.AllowURLRevisit(false) æ¥ç¦ç”¨Collyçš„å†…éƒ¨åŸŸåæ£€æŸ¥
	c := colly.NewCollector(
		// colly.MaxDepth(config.Depth), // ç§»é™¤è‡ªåŠ¨æ·±åº¦é™åˆ¶
		colly.Async(true),
		// ä¸è®¾ç½®AllowedDomains,å®Œå…¨ç”±åº”ç”¨å±‚æ§åˆ¶åŸŸåè®¿é—®
	)

	// è®¾ç½®è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯ (åŒ…å«TLSé…ç½®å’Œè¶…æ—¶è®¾ç½®)
	c.SetClient(httpClient)
	utils.Debugf("é™æ€çˆ¬å–å™¨: TLSè¯ä¹¦éªŒè¯å·²ç¦ç”¨,é€‚ç”¨äºå†…ç½‘/å¼€å‘ç¯å¢ƒçš„è‡ªç­¾åè¯ä¹¦")

	// æ ¹æ®AllowCrossDomainé…ç½®å†³å®šæ˜¯å¦é™åˆ¶åŸŸå
	// æ³¨æ„: ä¸ä½¿ç”¨Collyçš„AllowedDomains,å› ä¸ºå®ƒä¼šå¯¼è‡´"Forbidden"é”™è¯¯
	// åŸå› : Collyçš„AllowedDomainsæ˜¯ç²¾ç¡®åŒ¹é…,ä¼šé˜»æ­¢å­åŸŸåè®¿é—®(å¦‚ www.baidu.com -> xueshu.baidu.com)
	// æ”¹ä¸ºåœ¨OnRequestå›è°ƒä¸­æ‰‹åŠ¨æ£€æŸ¥åŸŸå,æä¾›æ›´çµæ´»çš„æ§åˆ¶
	if !config.AllowCrossDomain {
		utils.Debugf("é™æ€çˆ¬å–å™¨: å°†åœ¨åº”ç”¨å±‚æ£€æŸ¥åŸŸåé™åˆ¶ (ç›®æ ‡åŸŸå: %s)", domain)
	} else {
		utils.Debugf("é™æ€çˆ¬å–å™¨: å…è®¸è·¨åŸŸçˆ¬å– (æ— åŸŸåé™åˆ¶)")
	}

	// åˆå§‹åŒ–URLé˜Ÿåˆ—
	urlQueue := NewURLQueue(domain, config.AllowCrossDomain, config.Depth)

	// åˆå§‹åŒ–èµ„æºç›‘æ§å™¨
	resourceMonitor := NewResourceMonitor(ResourceMonitorConfig{
		SafetyReserveMemory: int64(config.SafetyReserveMemory) * 1024 * 1024, // MBè½¬å­—èŠ‚
		SafetyThreshold:     int64(config.SafetyThreshold) * 1024 * 1024,
		CPULoadThreshold:    config.CPULoadThreshold,
		MaxTabsLimit:        config.MaxTabsLimit,
		TabMemoryUsage:      100 * 1024 * 1024, // 100MB per worker
	})

	// å¯åŠ¨èµ„æºç›‘æ§(1ç§’é‡‡æ ·é—´éš”)
	resourceMonitor.StartMonitoring(1 * time.Second)

	// åˆå§‹å¹¶å‘æ•°è®¾ä¸ºé…ç½®çš„MaxWorkers
	initialWorkers := config.MaxWorkers
	if initialWorkers < 1 {
		initialWorkers = 1
	}

	// é…ç½®å¹¶å‘é™åˆ¶
	// æ— å»¶è¿Ÿ,æœ€å¤§åŒ–çˆ¬å–é€Ÿåº¦
	if err := c.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: initialWorkers,
		Delay:       0, // æ— å»¶è¿Ÿ
	}); err != nil {
		utils.Warnf("è®¾ç½®å¹¶å‘é™åˆ¶å¤±è´¥: %v", err)
	}

	utils.Debugf("é™æ€çˆ¬å–å™¨å¹¶å‘ä¼˜åŒ–: åˆå§‹å¹¶å‘=%d, æœ€å¤§é™åˆ¶=%d, æ— å»¶è¿Ÿ",
		initialWorkers, config.MaxTabsLimit)

	// è®¾ç½®è¶…æ—¶
	c.SetRequestTimeout(30 * time.Second)

	// T059-T061: åº”ç”¨è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯åˆ°Colly,ä»¥å¯ç”¨TLSè·³è¿‡éªŒè¯
	c.WithTransport(httpClient.Transport)

	sc := &StaticCrawler{
		collector:        c,
		config:           config,
		outputDir:        outputDir,
		domain:           domain,
		headerProvider:   headerProvider,
		jsFiles:          make(map[string]*models.JSFile),
		mapFiles:         make(map[string]*models.MapFile),
		globalFileHashes: globalFileHashes,
		globalMu:         globalMu,
		urlQueue:         urlQueue,
		resourceMonitor:  resourceMonitor,
		stats:            models.TaskStats{},
	}

	// è®¾ç½®å›è°ƒ
	sc.setupCallbacks()

	return sc
}

// setupCallbacks è®¾ç½®Collyå›è°ƒ
func (sc *StaticCrawler) setupCallbacks() {
	// æå–é¡µé¢é“¾æ¥(ç”¨äºæ·±åº¦çˆ¬å–å¯¼èˆª)
	// T053: æ‰‹åŠ¨ç®¡ç†æ·±åº¦æ£€æŸ¥,ä»…å¯¹é¡µé¢é“¾æ¥åº”ç”¨æ·±åº¦é™åˆ¶
	sc.collector.OnHTML("a[href]", func(e *colly.HTMLElement) {
		link := e.Request.AbsoluteURL(e.Attr("href"))

		// æ£€æŸ¥URLæœ‰æ•ˆæ€§
		if link == "" || !strings.HasPrefix(link, "http") {
			return
		}

		// æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
		if sc.urlQueue.IsVisited(link) {
			return
		}

		// æ‰‹åŠ¨æ·±åº¦æ£€æŸ¥: åªå¯¹é¡µé¢é“¾æ¥æ£€æŸ¥æ·±åº¦
		currentDepth := e.Request.Depth
		if currentDepth >= sc.config.Depth {
			utils.Debugf("é¡µé¢æ·±åº¦è¾¾åˆ°é™åˆ¶: %s (æ·±åº¦=%d, é™åˆ¶=%d)", link, currentDepth, sc.config.Depth)
			return
		}

		// æ‰‹åŠ¨åŸŸåæ£€æŸ¥(å¦‚æœAllowCrossDomain=false)
		if !sc.config.AllowCrossDomain {
			parsedURL, err := url.Parse(link)
			if err == nil && parsedURL.Host != sc.domain {
				utils.Debugf("è·³è¿‡è·¨åŸŸé“¾æ¥: %s (ç›®æ ‡åŸŸå: %s)", link, sc.domain)
				return
			}
		}

		// æ ‡è®°å·²è®¿é—®
		sc.urlQueue.MarkVisited(link)

		// è®¿é—®é“¾æ¥(ç”¨äºé¡µé¢å¯¼èˆª)
		if err := e.Request.Visit(link); err != nil {
			// åªåœ¨éForbiddené”™è¯¯æ—¶è®°å½•æ—¥å¿—
			if !strings.Contains(err.Error(), "Forbidden") {
				utils.Debugf("è®¿é—®é“¾æ¥å¤±è´¥ [%s]: %v", link, err)
			}
		}
	})

	// æå–scriptæ ‡ç­¾ä¸­çš„JavaScriptæ–‡ä»¶(JSèµ„æº)
	// T056: JSæ–‡ä»¶ä¸æ£€æŸ¥æ·±åº¦,æ— æ¡ä»¶è®¿é—®(æ·±åº¦è±å…)
	sc.collector.OnHTML("script[src]", func(e *colly.HTMLElement) {
		jsURL := e.Request.AbsoluteURL(e.Attr("src"))
		if sc.isJavaScriptURL(jsURL) {
			utils.Debugf("å‘ç°JSæ–‡ä»¶: %s", jsURL)

			// æ— æ¡ä»¶è®¿é—®JSæ–‡ä»¶,ä¸æ£€æŸ¥æ·±åº¦(æ·±åº¦è±å…)
			if err := e.Request.Visit(jsURL); err != nil {
				utils.Warnf("è®¿é—®JSæ–‡ä»¶å¤±è´¥ [%s]: %v", jsURL, err)
			}
		}
	})

	// æå–å†…è”scriptæ ‡ç­¾
	sc.collector.OnHTML("script:not([src])", func(e *colly.HTMLElement) {
		// ä¿å­˜å†…è”è„šæœ¬
		content := e.Text
		if len(content) > 100 { // åªä¿å­˜æœ‰å®è´¨å†…å®¹çš„è„šæœ¬
			utils.Debugf("å‘ç°å†…è”è„šæœ¬,é•¿åº¦: %d", len(content))
			// TODO: ä¿å­˜å†…è”è„šæœ¬
		}
	})

	// å¤„ç†å“åº”
	// T013: é›†æˆisValidJavaScriptå†…å®¹æ£€æµ‹,ç»•è¿‡å‡404å“åº”
	sc.collector.OnResponse(func(r *colly.Response) {
		requestURL := r.Request.URL.String()

		// å¦‚æœæ˜¯JavaScriptæ–‡ä»¶,è¿›è¡Œå†…å®¹æ£€æµ‹åä¸‹è½½
		if sc.isJavaScriptURL(requestURL) {
			contentType := r.Headers.Get("Content-Type")
			contentEncoding := r.Headers.Get("Content-Encoding")

			// è§£å‹å“åº”ä½“(å¦‚æœæœ‰å‹ç¼©)
			body := r.Body
			if contentEncoding != "" {
				decompressed, err := decompressResponse(contentEncoding, r.Body)
				if err != nil {
					utils.Warnf("è§£å‹å“åº”å¤±è´¥ [%s] (ç¼–ç =%s): %v", requestURL, contentEncoding, err)
					// è§£å‹å¤±è´¥,ä»ç„¶å°è¯•ä½¿ç”¨åŸå§‹body
				} else {
					body = decompressed
					utils.Debugf("æˆåŠŸè§£å‹å“åº” [%s]: åŸå§‹=%d bytes, è§£å‹å=%d bytes", requestURL, len(r.Body), len(body))
				}
			}

			// æ™ºèƒ½å†…å®¹æ£€æµ‹: æ— è®ºHTTPçŠ¶æ€ç å¦‚ä½•,éƒ½æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºæœ‰æ•ˆJS
			if isValidJavaScript(contentType, body) {
				// å†…å®¹æ£€æµ‹é€šè¿‡,ä¸‹è½½æ–‡ä»¶
				if err := sc.downloadJSFile(requestURL, body, contentType); err != nil {
					utils.Warnf("ä¸‹è½½JSæ–‡ä»¶å¤±è´¥ [%s]: %v", requestURL, err)
					sc.stats.FailedFiles++
				} else {
					// HTTPé”™è¯¯ä½†å†…å®¹æœ‰æ•ˆçš„æƒ…å†µ,è®°å½•FakeHTTPErrors
					if r.StatusCode >= 400 {
						sc.stats.FakeHTTPErrors++
						utils.Debugf("æ£€æµ‹åˆ°å‡HTTPé”™è¯¯ [%s]: çŠ¶æ€ç %dä½†å†…å®¹æœ‰æ•ˆ", requestURL, r.StatusCode)
					}
				}
			} else {
				// T014: å†…å®¹æ£€æµ‹å¤±è´¥,è®°å½•ä¸ºFailedFiles
				utils.Infof("è®¿é—®JSæ–‡ä»¶å¤±è´¥ [%s]: å†…å®¹æ£€æµ‹å¤±è´¥,éæœ‰æ•ˆJavaScriptæ–‡ä»¶", requestURL)
				sc.stats.FailedFiles++
			}
		}
	})

	// é”™è¯¯å¤„ç†
	sc.collector.OnError(func(r *colly.Response, err error) {
		// å¦‚æœæ˜¯Forbiddené”™è¯¯ä¸”é…ç½®å…è®¸è·¨åŸŸ,åˆ™å¿½ç•¥(è¿™æ˜¯Collyçš„å†…éƒ¨åŸŸåæ£€æŸ¥å¯¼è‡´çš„è¯¯æŠ¥)
		if strings.Contains(err.Error(), "Forbidden") && sc.config.AllowCrossDomain {
			utils.Debugf("å¿½ç•¥Collyçš„åŸŸåæ£€æŸ¥é”™è¯¯ [%s]: %v (é…ç½®å…è®¸è·¨åŸŸ)", r.Request.URL, err)
			return
		}

		utils.Errorf("çˆ¬å–é”™è¯¯ [%s]: %v", r.Request.URL, err)
		sc.stats.FailedFiles++
	})

	// è®¿é—®å‰
	sc.collector.OnRequest(func(r *colly.Request) {
		// æ‰‹åŠ¨åŸŸåæ£€æŸ¥(å¦‚æœAllowCrossDomain=false)
		if !sc.config.AllowCrossDomain {
			parsedURL, err := url.Parse(r.URL.String())
			if err == nil {
				// æ£€æŸ¥æ˜¯å¦ä¸ºåŒä¸€åŸŸå
				if parsedURL.Host != sc.domain {
					utils.Debugf("æ‹’ç»è·¨åŸŸè¯·æ±‚: %s (ç›®æ ‡åŸŸå: %s)", r.URL.String(), sc.domain)
					r.Abort()
					return
				}
			}
		}

		// T054: åˆ¤æ–­æ˜¯å¦ä¸ºJavaScriptèµ„æº
		if IsJavaScriptResource(r.URL.String()) {
			// T055: å¯¹JSèµ„æºè¯·æ±‚è®¾ç½®contextæ ‡è®°,è·³è¿‡æ·±åº¦æ£€æŸ¥
			r.Ctx.Put("is_js_resource", "true")
			// T057: æ·»åŠ DEBUGçº§åˆ«æ—¥å¿—è®°å½•JSæ–‡ä»¶è±å…è¡Œä¸º
			utils.Debugf("æ£€æµ‹åˆ°JSèµ„æº,è±å…æ·±åº¦é™åˆ¶: %s", r.URL.String())
		}

		// åº”ç”¨è‡ªå®šä¹‰HTTPå¤´éƒ¨
		if sc.headerProvider != nil {
			headers, err := sc.headerProvider.GetHeaders()
			if err != nil {
				utils.Warnf("è·å–HTTPå¤´éƒ¨å¤±è´¥: %v", err)
			} else {
				for name, values := range headers {
					if len(values) > 0 {
						r.Headers.Set(name, values[0])
					}
				}
			}
		}

		utils.Debugf("è®¿é—®: %s", r.URL.String())
		sc.mu.Lock()
		sc.stats.VisitedURLs++
		sc.mu.Unlock()

		// åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°(åŸºäºé˜Ÿåˆ—é•¿åº¦å’Œèµ„æºé™åˆ¶)
		sc.adjustConcurrency()
	})
}

// Crawl å¼€å§‹çˆ¬å–
func (sc *StaticCrawler) Crawl(targetURL string) error {
	startTime := time.Now()

	utils.Infof("ğŸ” é™æ€çˆ¬å–æ¨¡å¼å¯åŠ¨")
	utils.Infof("ç›®æ ‡URL: %s", targetURL)
	utils.Infof("æœ€å¤§æ·±åº¦: %d", sc.config.Depth)
	utils.Infof("å¹¶å‘æ•°: %d", sc.config.MaxWorkers)

	// è®¿é—®ç›®æ ‡URL
	if err := sc.collector.Visit(targetURL); err != nil {
		return fmt.Errorf("è®¿é—®ç›®æ ‡URLå¤±è´¥: %w", err)
	}

	// æ·»åŠ è¿›åº¦ç›‘æ§goroutine
	done := make(chan struct{})
	go func() {
		ticker := time.NewTicker(5 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-done:
				return
			case <-ticker.C:
				sc.mu.RLock()
				visitedCount := sc.stats.VisitedURLs
				downloadedCount := sc.stats.StaticFiles
				failedCount := sc.stats.FailedFiles
				sc.mu.RUnlock()

				utils.Infof("è¿›åº¦: å·²è®¿é—® %d ä¸ªURL, å·²ä¸‹è½½ %d ä¸ªæ–‡ä»¶, å¤±è´¥ %d ä¸ª",
					visitedCount, downloadedCount, failedCount)
			}
		}
	}()

	// ä½¿ç”¨å¸¦è¶…æ—¶çš„Wait,é¿å…æ— é™ç­‰å¾…
	// åˆ›å»ºä¸€ä¸ªgoroutineæ¥æ‰§è¡ŒWait
	waitDone := make(chan struct{})
	go func() {
		sc.collector.Wait()
		close(waitDone)
	}()

	// è®¾ç½®å…¨å±€è¶…æ—¶: æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
	globalTimeout := 5 * time.Minute
	select {
	case <-waitDone:
		// æ­£å¸¸å®Œæˆ
		utils.Debugf("é™æ€çˆ¬å–æ­£å¸¸å®Œæˆ")
	case <-time.After(globalTimeout):
		// è¶…æ—¶
		utils.Warnf("é™æ€çˆ¬å–è¶…æ—¶(ç­‰å¾…%v),å¼ºåˆ¶ç»“æŸ", globalTimeout)
	}

	close(done) // é€šçŸ¥ç›‘æ§goroutineé€€å‡º

	duration := time.Since(startTime)
	sc.stats.Duration = duration.Seconds()

	utils.Infof("âœ… é™æ€çˆ¬å–å®Œæˆ")
	utils.Infof("è®¿é—®URLæ•°: %d", sc.stats.VisitedURLs)
	utils.Infof("ä¸‹è½½æ–‡ä»¶æ•°: %d", sc.stats.StaticFiles)
	utils.Infof("å¤±è´¥æ–‡ä»¶æ•°: %d", sc.stats.FailedFiles)
	utils.Infof("æ€»è€—æ—¶: %.2fç§’", sc.stats.Duration)

	return nil
}

// downloadJSFile ä¸‹è½½å¹¶ä¿å­˜JavaScriptæ–‡ä»¶
// å¤„ç†æµç¨‹:
//  1. æ£€æŸ¥URLæ˜¯å¦å·²ä¸‹è½½ (é¿å…é‡å¤ä¸‹è½½)
//  2. è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
//  3. æ£€æŸ¥å…¨å±€å“ˆå¸Œè¡¨ (è·¨çˆ¬å–å™¨å»é‡)
//  4. æ£€æŸ¥æœ¬åœ°å“ˆå¸Œè¡¨ (çˆ¬å–å™¨å†…å»é‡)
//  5. ç”Ÿæˆæ–‡ä»¶è·¯å¾„å¹¶ä¿å­˜åˆ°ç£ç›˜
//  6. åˆ›å»ºJSFileå…ƒæ•°æ®å¯¹è±¡
//  7. æ·»åŠ åˆ°å…¨å±€å“ˆå¸Œè¡¨
//  8. æ£€æŸ¥å¹¶ä¸‹è½½Source Mapæ–‡ä»¶
//
// å‚æ•°:
//   - fileURL: JavaScriptæ–‡ä»¶çš„å®Œæ•´URL
//   - content: æ–‡ä»¶å†…å®¹ (å­—èŠ‚æ•°ç»„)
//   - contentType: HTTP Content-Typeå¤´éƒ¨
//
// è¿”å›: é”™è¯¯ä¿¡æ¯ (å¦‚æœå¤±è´¥)
func (sc *StaticCrawler) downloadJSFile(fileURL string, content []byte, contentType string) error {
	sc.mu.Lock()
	defer sc.mu.Unlock()

	// æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
	if _, exists := sc.jsFiles[fileURL]; exists {
		utils.Debugf("æ–‡ä»¶å·²å­˜åœ¨,è·³è¿‡: %s", fileURL)
		return nil
	}

	// è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
	hash := calculateHash(content)

	// å…ˆæ£€æŸ¥å…¨å±€å“ˆå¸Œè¡¨(è·¨çˆ¬å–å™¨å»é‡)
	if sc.globalFileHashes != nil && sc.globalMu != nil {
		sc.globalMu.RLock()
		if existingURL, exists := sc.globalFileHashes[hash]; exists {
			sc.globalMu.RUnlock()
			utils.Debugf("å‘ç°å…¨å±€é‡å¤æ–‡ä»¶(å“ˆå¸Œç›¸åŒ): %s (ä¸ %s ç›¸åŒ)", fileURL, existingURL)

			// åˆ›å»ºä¸€ä¸ªæ ‡è®°ä¸ºé‡å¤çš„JSFileå¯¹è±¡,ä½†ä¸ä¿å­˜åˆ°ç£ç›˜
			jsFile := &models.JSFile{
				ID:           uuid.New().String(),
				URL:          fileURL,
				FilePath:     "", // ä¸ä¿å­˜æ–‡ä»¶
				Hash:         hash,
				Size:         int64(len(content)),
				Extension:    filepath.Ext(fileURL),
				ContentType:  contentType,
				SourceURL:    fileURL,
				CrawlMode:    models.ModeStatic,
				Depth:        0,
				IsObfuscated: false,
				IsDuplicate:  true,
				DownloadedAt: time.Now(),
				HasMapFile:   false,
			}
			sc.jsFiles[fileURL] = jsFile
			return nil
		}
		sc.globalMu.RUnlock()
	}

	// æ£€æŸ¥æœ¬åœ°å“ˆå¸Œå»é‡
	for _, existingFile := range sc.jsFiles {
		if existingFile.Hash == hash {
			utils.Debugf("å‘ç°é‡å¤æ–‡ä»¶(å“ˆå¸Œç›¸åŒ): %s", fileURL)
			sc.jsFiles[fileURL] = existingFile
			existingFile.IsDuplicate = true
			return nil
		}
	}

	// ç”Ÿæˆæ–‡ä»¶è·¯å¾„
	filePath, err := sc.generateFilePath(fileURL, "encode/js")
	if err != nil {
		return fmt.Errorf("ç”Ÿæˆæ–‡ä»¶è·¯å¾„å¤±è´¥: %w", err)
	}

	// ç¡®ä¿ç›®å½•å­˜åœ¨
	if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {
		return fmt.Errorf("åˆ›å»ºç›®å½•å¤±è´¥: %w", err)
	}

	// å†™å…¥æ–‡ä»¶
	if err := os.WriteFile(filePath, content, 0644); err != nil {
		return fmt.Errorf("å†™å…¥æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// åˆ›å»ºJSFileå¯¹è±¡
	jsFile := &models.JSFile{
		ID:           uuid.New().String(),
		URL:          fileURL,
		FilePath:     filePath,
		Hash:         hash,
		Size:         int64(len(content)),
		Extension:    filepath.Ext(fileURL),
		ContentType:  contentType,
		SourceURL:    fileURL,
		CrawlMode:    models.ModeStatic,
		Depth:        0, // TODO: è·Ÿè¸ªå®é™…æ·±åº¦
		IsObfuscated: false,
		DownloadedAt: time.Now(),
		HasMapFile:   false,
	}

	sc.jsFiles[fileURL] = jsFile
	sc.stats.StaticFiles++
	sc.stats.TotalFiles++
	sc.stats.TotalSize += int64(len(content))

	// æ·»åŠ åˆ°å…¨å±€å“ˆå¸Œè¡¨
	if sc.globalFileHashes != nil && sc.globalMu != nil {
		sc.globalMu.Lock()
		sc.globalFileHashes[hash] = fileURL
		sc.globalMu.Unlock()
	}

	utils.Infof("ğŸ“¥ ä¸‹è½½æˆåŠŸ: %s (%d bytes) - %s", filepath.Base(filePath), len(content), fileURL)

	// æ£€æŸ¥æ˜¯å¦æœ‰Source Map
	sc.checkAndDownloadSourceMap(fileURL, content)

	return nil
}

// checkAndDownloadSourceMap æ£€æŸ¥å¹¶ä¸‹è½½Source Mapæ–‡ä»¶
func (sc *StaticCrawler) checkAndDownloadSourceMap(jsURL string, jsContent []byte) {
	// åœ¨æ–‡ä»¶å†…å®¹ä¸­æŸ¥æ‰¾sourceMappingURLæ³¨é‡Š
	content := string(jsContent)

	// æŸ¥æ‰¾ //# sourceMappingURL=xxx.map
	if idx := strings.Index(content, "sourceMappingURL="); idx != -1 {
		start := idx + len("sourceMappingURL=")
		end := strings.IndexAny(content[start:], "\n\r ")
		if end == -1 {
			end = len(content) - start
		}

		mapURL := strings.TrimSpace(content[start : start+end])

		// æ„é€ å®Œæ•´URL
		baseURL, _ := url.Parse(jsURL)
		fullMapURL, err := baseURL.Parse(mapURL)
		if err == nil {
			utils.Infof("ğŸ—ºï¸  å‘ç°Source Map: %s", fullMapURL.String())

			// ä¸‹è½½Source Mapæ–‡ä»¶
			sc.downloadSourceMapFile(fullMapURL.String())
		}
	}
}

// downloadSourceMapFile ä¸‹è½½Source Mapæ–‡ä»¶
// æ³¨æ„: è°ƒç”¨æ­¤å‡½æ•°å‰è°ƒç”¨è€…å¿…é¡»å·²æŒæœ‰ sc.mu é”
func (sc *StaticCrawler) downloadSourceMapFile(mapURL string) {
	// æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½ (ä¸éœ€è¦é¢å¤–åŠ é”,è°ƒç”¨è€…å·²æŒæœ‰é”)
	if _, exists := sc.mapFiles[mapURL]; exists {
		utils.Debugf("Source Mapæ–‡ä»¶å·²å­˜åœ¨,è·³è¿‡: %s", mapURL)
		return
	}

	// ä¸´æ—¶é‡Šæ”¾é”ä»¥æ‰§è¡ŒHTTPè¯·æ±‚(é¿å…é˜»å¡å…¶ä»–æ“ä½œ)
	sc.mu.Unlock()
	defer sc.mu.Lock()

	// å‘èµ·HTTPè¯·æ±‚ä¸‹è½½
	client := &http.Client{
		Timeout: 30 * time.Second,
		Transport: &http.Transport{
			TLSClientConfig: &tls.Config{
				InsecureSkipVerify: true,
			},
		},
	}

	resp, err := client.Get(mapURL)
	if err != nil {
		utils.Warnf("ä¸‹è½½Source Mapå¤±è´¥ [%s]: %v", mapURL, err)
		return
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		utils.Warnf("ä¸‹è½½Source Mapå¤±è´¥ [%s]: HTTP %d", mapURL, resp.StatusCode)
		return
	}

	// è¯»å–å†…å®¹
	content, err := io.ReadAll(resp.Body)
	if err != nil {
		utils.Warnf("è¯»å–Source Mapå†…å®¹å¤±è´¥ [%s]: %v", mapURL, err)
		return
	}

	// ç”Ÿæˆæ–‡ä»¶è·¯å¾„ (ä¿å­˜åˆ° encode/map/{domain}/ ç›®å½•)
	filePath, err := sc.generateFilePath(mapURL, "encode/map")
	if err != nil {
		utils.Warnf("ç”ŸæˆSource Mapæ–‡ä»¶è·¯å¾„å¤±è´¥ [%s]: %v", mapURL, err)
		return
	}

	// ç¡®ä¿ç›®å½•å­˜åœ¨
	if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {
		utils.Warnf("åˆ›å»ºSource Mapç›®å½•å¤±è´¥: %v", err)
		return
	}

	// å†™å…¥æ–‡ä»¶
	if err := os.WriteFile(filePath, content, 0644); err != nil {
		utils.Warnf("å†™å…¥Source Mapæ–‡ä»¶å¤±è´¥: %v", err)
		return
	}

	// æ³¨æ„: æ­¤æ—¶é”å·²ç»è¢«é‡æ–°è·å–(defer sc.mu.Lock())
	// åˆ›å»ºMapFileå¯¹è±¡
	mapFile := &models.MapFile{
		ID:           uuid.New().String(),
		URL:          mapURL,
		FilePath:     filePath,
		Size:         int64(len(content)),
		DownloadedAt: time.Now(),
	}

	sc.mapFiles[mapURL] = mapFile
	sc.stats.MapFiles++

	utils.Infof("ğŸ“¥ ä¸‹è½½Source MapæˆåŠŸ: %s (%d bytes)", filepath.Base(filePath), len(content))
}

// isJavaScriptURL åˆ¤æ–­æ˜¯å¦ä¸ºJavaScriptæ–‡ä»¶URL
func (sc *StaticCrawler) isJavaScriptURL(urlStr string) bool {
	urlStr = strings.ToLower(urlStr)

	// æ£€æŸ¥æ‰©å±•å
	for _, ext := range models.JSFileExtensions {
		if strings.HasSuffix(urlStr, ext) {
			return true
		}
	}

	// æ£€æŸ¥å¸¸è§JSæ¨¡å¼
	if strings.Contains(urlStr, ".js?") ||
		strings.Contains(urlStr, ".mjs?") ||
		strings.Contains(urlStr, ".jsx?") {
		return true
	}

	return false
}

// IsJavaScriptResource åˆ¤æ–­URLæ˜¯å¦ä¸ºJavaScriptèµ„æºæ–‡ä»¶
// ç”¨äºæ·±åº¦è±å…é€»è¾‘: JSèµ„æºä¸å—æ·±åº¦é™åˆ¶å½±å“
//
// åˆ¤æ–­è§„åˆ™:
//   - æ–¹æ³•1: æ£€æŸ¥æ–‡ä»¶æ‰©å±•å (.js, .mjs)
//   - æ–¹æ³•2: æ£€æŸ¥URLè·¯å¾„ç‰¹å¾ (/js/, /javascript/, /scripts/, .min.js)
//
// è¿”å›: trueè¡¨ç¤ºURLæ˜¯JSèµ„æº,åº”è±å…æ·±åº¦é™åˆ¶
func IsJavaScriptResource(urlStr string) bool {
	// è½¬æ¢ä¸ºå°å†™ä»¥æ”¯æŒå¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
	lowerURL := strings.ToLower(urlStr)

	// æ–¹æ³•1: æ£€æŸ¥æ–‡ä»¶æ‰©å±•å (.js, .mjs)
	// éœ€è¦å¤„ç†æŸ¥è¯¢å‚æ•°,å¦‚ app.js?v=1.0
	if strings.HasSuffix(lowerURL, ".js") || strings.HasSuffix(lowerURL, ".mjs") {
		return true
	}
	// æ£€æŸ¥æ˜¯å¦åŒ…å«.js?æˆ–.mjs?æ¨¡å¼
	if strings.Contains(lowerURL, ".js?") || strings.Contains(lowerURL, ".mjs?") {
		return true
	}

	// æ–¹æ³•2: æ£€æŸ¥URLè·¯å¾„ç‰¹å¾ (å¦‚ /static/js/, /assets/scripts/)
	jsPathPatterns := []string{"/js/", "/javascript/", "/scripts/", ".min.js"}
	for _, pattern := range jsPathPatterns {
		if strings.Contains(lowerURL, pattern) {
			return true
		}
	}

	return false
}

// generateFilePath ç”Ÿæˆæœ¬åœ°æ–‡ä»¶è·¯å¾„
// è·¯å¾„æ ¼å¼: output/{target_domain}/encode/js/{source_domain}/filename.js
// ä¾‹å¦‚: output/www.baidu.com/encode/js/map.baidu.com/app.js
func (sc *StaticCrawler) generateFilePath(fileURL string, subdir string) (string, error) {
	parsed, err := url.Parse(fileURL)
	if err != nil {
		return "", err
	}

	// ä½¿ç”¨URLè·¯å¾„ä½œä¸ºæ–‡ä»¶å
	filename := filepath.Base(parsed.Path)
	if filename == "" || filename == "." {
		filename = "index.js"
	}

	// è·å–JSæ–‡ä»¶çš„æ¥æºåŸŸå
	sourceDomain := parsed.Host
	if sourceDomain == "" {
		sourceDomain = "unknown"
	}

	// æ„é€ å®Œæ•´è·¯å¾„: output/{target_domain}/encode/js/{source_domain}/filename
	// åœ¨jsç›®å½•ä¸‹æŒ‰æ¥æºåŸŸååˆ†ç±»
	fullPath := filepath.Join(sc.outputDir, sc.domain, subdir, sourceDomain, filename)

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨,æ·»åŠ ç¼–å·
	if _, err := os.Stat(fullPath); err == nil {
		ext := filepath.Ext(filename)
		base := strings.TrimSuffix(filename, ext)
		for i := 1; ; i++ {
			newPath := filepath.Join(sc.outputDir, sc.domain, subdir, sourceDomain, fmt.Sprintf("%s_%d%s", base, i, ext))
			if _, err := os.Stat(newPath); os.IsNotExist(err) {
				fullPath = newPath
				break
			}
		}
	}

	return fullPath, nil
}

// calculateHash è®¡ç®—SHA-256å“ˆå¸Œ
func calculateHash(data []byte) string {
	hash := sha256.Sum256(data)
	return fmt.Sprintf("%x", hash)
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (sc *StaticCrawler) GetStats() models.TaskStats {
	sc.mu.RLock()
	defer sc.mu.RUnlock()
	return sc.stats
}

// GetJSFiles è·å–æ‰€æœ‰ä¸‹è½½çš„JSæ–‡ä»¶
func (sc *StaticCrawler) GetJSFiles() []*models.JSFile {
	sc.mu.RLock()
	defer sc.mu.RUnlock()

	files := make([]*models.JSFile, 0, len(sc.jsFiles))
	for _, f := range sc.jsFiles {
		files = append(files, f)
	}
	return files
}

// adjustConcurrency åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°(åŸºäºé˜Ÿåˆ—é•¿åº¦å’Œèµ„æºé™åˆ¶)
// ç­–ç•¥:
//   - åŸºäºResourceMonitorè®¡ç®—çš„maxTabsä½œä¸ºå¹¶å‘ä¸Šé™
//   - æ ¹æ®å¾…çˆ¬URLæ•°é‡(PendingCount)æŒ‰éœ€è°ƒæ•´
//   - å¹¶å‘æ•° = min(PendingCount, maxTabs)
func (sc *StaticCrawler) adjustConcurrency() {
	// è·å–èµ„æºé™åˆ¶çš„æœ€å¤§å¹¶å‘æ•°
	maxTabs := sc.resourceMonitor.CalculateMaxTabs()

	// è·å–å¾…çˆ¬URLæ•°é‡
	pendingCount := sc.urlQueue.PendingCount()

	// è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°: min(å¾…çˆ¬URLæ•°, èµ„æºé™åˆ¶)
	// è‡³å°‘ä¿æŒ1ä¸ªå¹¶å‘
	optimalWorkers := 1
	if pendingCount > 0 {
		if pendingCount < maxTabs {
			optimalWorkers = pendingCount
		} else {
			optimalWorkers = maxTabs
		}
	}

	// æ›´æ–°Collyçš„å¹¶å‘é™åˆ¶
	if err := sc.collector.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: optimalWorkers,
		Delay:       0, // æ— å»¶è¿Ÿ
	}); err != nil {
		utils.Warnf("æ›´æ–°å¹¶å‘é™åˆ¶å¤±è´¥: %v", err)
		return
	}

	utils.Debugf("é™æ€çˆ¬å–å™¨å¹¶å‘è°ƒæ•´: å¾…çˆ¬URL=%d, å½“å‰å¹¶å‘=%d, æœ€å¤§é™åˆ¶=%d",
		pendingCount, optimalWorkers, maxTabs)
}

// Reset é‡ç½®çˆ¬å–å™¨çŠ¶æ€,ç”¨äºæ‰¹é‡çˆ¬å–åœºæ™¯
//
// èŒè´£:
//   - æ¸…ç©ºURLé˜Ÿåˆ—(è°ƒç”¨URLQueue.Reset)
//   - æ¸…ç©ºå†…éƒ¨çŠ¶æ€(jsFiles, mapFiles, stats)
//   - é‡æ–°åˆ›å»ºColly collectorå®ä¾‹
//
// ä½¿ç”¨åœºæ™¯:
//   - æ‰¹é‡çˆ¬å–(-få‚æ•°)ä¸­,æ¯ä¸ªç›®æ ‡å®Œæˆåè°ƒç”¨
//   - ç¡®ä¿ç›®æ ‡é—´çš„å®Œå…¨éš”ç¦»,æ— URLæˆ–æ–‡ä»¶æ±¡æŸ“
//
// æ³¨æ„:
//   - ä¸é‡ç½®å…¨å±€æ–‡ä»¶å“ˆå¸Œè¡¨(globalFileHashes),å› ä¸ºéœ€è¦è·¨ç›®æ ‡å»é‡
//   - ä¸é‡ç½®ResourceMonitor,å› ä¸ºæ˜¯å…¨å±€èµ„æºç›‘æ§
//   - éœ€è¦é‡æ–°åˆ›å»ºcollector,å› ä¸ºCollyçš„è®¿é—®å†å²æ— æ³•æ¸…ç©º
func (sc *StaticCrawler) Reset() error {
	sc.mu.Lock()
	defer sc.mu.Unlock()

	// é‡ç½®URLé˜Ÿåˆ—
	if sc.urlQueue != nil {
		sc.urlQueue.Reset()
	}

	// æ¸…ç©ºå†…éƒ¨çŠ¶æ€
	sc.jsFiles = make(map[string]*models.JSFile)
	sc.mapFiles = make(map[string]*models.MapFile)
	sc.stats = models.TaskStats{}

	// é‡æ–°åˆ›å»ºcollectorå®ä¾‹
	sc.collector = colly.NewCollector(
		// colly.MaxDepth(sc.config.Depth), // ç§»é™¤è‡ªåŠ¨æ·±åº¦é™åˆ¶,ä½¿ç”¨åº”ç”¨å±‚æ‰‹åŠ¨ç®¡ç†
		colly.Async(true),
	)

	// ä¸ä½¿ç”¨AllowedDomains,æ”¹ä¸ºåœ¨OnRequestä¸­æ‰‹åŠ¨æ£€æŸ¥
	// å‚è§setupCallbacks()ä¸­çš„åŸŸåæ£€æŸ¥é€»è¾‘

	// è®¾ç½®è¶…æ—¶
	sc.collector.SetRequestTimeout(30 * time.Second)

	// é…ç½®åˆå§‹å¹¶å‘é™åˆ¶
	initialWorkers := sc.config.MaxWorkers
	if initialWorkers < 1 {
		initialWorkers = 1
	}
	if err := sc.collector.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: initialWorkers,
		Delay:       0, // æ— å»¶è¿Ÿ
	}); err != nil {
		utils.Warnf("è®¾ç½®å¹¶å‘é™åˆ¶å¤±è´¥: %v", err)
	}

	// é‡æ–°è®¾ç½®å›è°ƒ
	sc.setupCallbacks()

	utils.Debugf("é™æ€çˆ¬å–å™¨çŠ¶æ€å·²é‡ç½®")
	return nil
}

// isValidJavaScript æ£€æµ‹HTTPå“åº”å†…å®¹æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JavaScriptæ–‡ä»¶
// ç”¨äºç»•è¿‡åçˆ¬è™«çš„å‡404å“åº”(è¿”å›404ä½†bodyåŒ…å«çœŸå®JSä»£ç )
// å‚æ•°:
//
//	contentType: HTTPå“åº”å¤´Content-Type
//	body: å“åº”ä½“å†…å®¹
//
// è¿”å›: æ˜¯å¦ä¸ºæœ‰æ•ˆJavaScriptæ–‡ä»¶
// å¥‘çº¦å‚è€ƒ: contracts/module-contracts.md - å†…å®¹æ£€æµ‹å¥‘çº¦
func isValidJavaScript(contentType string, body []byte) bool {
	// 1. Content-Typeæ£€æµ‹: æœ€å¯é çš„æŒ‡æ ‡
	if strings.Contains(strings.ToLower(contentType), "javascript") {
		return true
	}

	// 2. å†…å®¹ç‰¹å¾æ£€æµ‹(æ£€æŸ¥å‰1KB,é¿å…æ€§èƒ½é—®é¢˜)
	sample := body
	if len(body) > 1024 {
		sample = body[:1024]
	}

	// JavaScriptå…³é”®å­—åˆ—è¡¨
	jsKeywords := []string{"function", "var", "const", "let", "class", "import", "export", "=>"}
	matchCount := 0
	for _, keyword := range jsKeywords {
		if strings.Contains(string(sample), keyword) {
			matchCount++
		}
	}

	// è‡³å°‘åŒ¹é…2ä¸ªå…³é”®å­—æ‰è®¤ä¸ºæ˜¯JS(é¿å…è¯¯åˆ¤,å¦‚HTMLä¸­å¶å°”å‡ºç°"function"å­—æ ·)
	return matchCount >= 2
}

// decompressResponse æ ¹æ®Content-Encodingå¤´éƒ¨è§£å‹å“åº”ä½“
// æ”¯æŒ gzip, deflate, br (Brotli) ä¸‰ç§å‹ç¼©æ ¼å¼
func decompressResponse(contentEncoding string, body []byte) ([]byte, error) {
	encoding := strings.ToLower(strings.TrimSpace(contentEncoding))

	switch encoding {
	case "gzip":
		// GZIPè§£å‹
		reader, err := gzip.NewReader(bytes.NewReader(body))
		if err != nil {
			return nil, fmt.Errorf("gzipè§£å‹å¤±è´¥: %w", err)
		}
		defer reader.Close()

		decompressed, err := io.ReadAll(reader)
		if err != nil {
			return nil, fmt.Errorf("gzipè¯»å–å¤±è´¥: %w", err)
		}
		return decompressed, nil

	case "deflate":
		// Deflateè§£å‹
		reader := flate.NewReader(bytes.NewReader(body))
		defer reader.Close()

		decompressed, err := io.ReadAll(reader)
		if err != nil {
			return nil, fmt.Errorf("deflateè¯»å–å¤±è´¥: %w", err)
		}
		return decompressed, nil

	case "br":
		// Brotliè§£å‹
		reader := brotli.NewReader(bytes.NewReader(body))
		decompressed, err := io.ReadAll(reader)
		if err != nil {
			return nil, fmt.Errorf("brotliè¯»å–å¤±è´¥: %w", err)
		}
		return decompressed, nil

	case "":
		// æ²¡æœ‰å‹ç¼©,ç›´æ¥è¿”å›åŸå§‹å†…å®¹
		return body, nil

	default:
		// æœªçŸ¥ç¼–ç ,è¿”å›è­¦å‘Šä½†ä»ç„¶è¿”å›åŸå§‹å†…å®¹
		utils.Warnf("æœªçŸ¥çš„Content-Encoding: %s", contentEncoding)
		return body, nil
	}
}
