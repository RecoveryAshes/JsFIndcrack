package crawlers

import (
	"crypto/sha256"
	"fmt"
	"net/url"
	"os"
	"path/filepath"
	"runtime"
	"strings"
	"sync"
	"time"

	"github.com/RecoveryAshes/JsFIndcrack/internal/models"
	"github.com/RecoveryAshes/JsFIndcrack/internal/utils"
	"github.com/gocolly/colly/v2"
	"github.com/google/uuid"
)

// StaticCrawler é™æ€çˆ¬å–å™¨(ä½¿ç”¨Colly)
type StaticCrawler struct {
	collector *colly.Collector
	config    models.CrawlConfig
	outputDir string
	domain    string

	// HTTPå¤´éƒ¨æä¾›è€…
	headerProvider models.HeaderProvider

	// æ–‡ä»¶å­˜å‚¨
	jsFiles  map[string]*models.JSFile  // URL -> JSFile
	mapFiles map[string]*models.MapFile // URL -> MapFile
	mu       sync.RWMutex               // ä¿æŠ¤maps

	// å…¨å±€æ–‡ä»¶å“ˆå¸Œè¡¨(ç”¨äºè·¨çˆ¬å–å™¨å»é‡)
	globalFileHashes map[string]string // hash -> URL (shared with dynamic crawler)
	globalMu         *sync.RWMutex     // ä¿æŠ¤globalFileHashesçš„äº’æ–¥é”

	// ç»Ÿè®¡
	visitedURLs []string
	stats       models.TaskStats
}

// NewStaticCrawler åˆ›å»ºé™æ€çˆ¬å–å™¨
func NewStaticCrawler(config models.CrawlConfig, outputDir string, domain string, globalFileHashes map[string]string, globalMu *sync.RWMutex, headerProvider models.HeaderProvider) *StaticCrawler {
	// åˆ›å»ºColly collector
	c := colly.NewCollector(
		colly.MaxDepth(config.Depth),
		colly.Async(true),
		colly.AllowedDomains(domain),
	)

	// åŠ¨æ€è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
	// ç­–ç•¥: MaxWorkers * min(CPUæ ¸å¿ƒæ•°, 4)
	// é¿å…åœ¨ä½æ ¸å¿ƒæœºå™¨ä¸Šè¿‡åº¦å¹¶å‘,åŒæ—¶å……åˆ†åˆ©ç”¨å¤šæ ¸
	optimalWorkers := calculateOptimalWorkers(config.MaxWorkers)

	// é…ç½®å¹¶å‘é™åˆ¶
	c.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: optimalWorkers,
		Delay:       1 * time.Second,
	})

	utils.Debugf("é™æ€çˆ¬å–å™¨å¹¶å‘ä¼˜åŒ–: é…ç½®=%d, CPUæ ¸å¿ƒ=%d, æœ€ä¼˜å¹¶å‘=%d",
		config.MaxWorkers, runtime.NumCPU(), optimalWorkers)

	// è®¾ç½®è¶…æ—¶
	c.SetRequestTimeout(30 * time.Second)

	sc := &StaticCrawler{
		collector:        c,
		config:           config,
		outputDir:        outputDir,
		domain:           domain,
		headerProvider:   headerProvider,
		jsFiles:          make(map[string]*models.JSFile),
		mapFiles:         make(map[string]*models.MapFile),
		globalFileHashes: globalFileHashes,
		globalMu:         globalMu,
		visitedURLs:      make([]string, 0),
		stats:            models.TaskStats{},
	}

	// è®¾ç½®å›è°ƒ
	sc.setupCallbacks()

	return sc
}

// setupCallbacks è®¾ç½®Collyå›è°ƒ
func (sc *StaticCrawler) setupCallbacks() {
	// æå–scriptæ ‡ç­¾ä¸­çš„JavaScriptæ–‡ä»¶
	sc.collector.OnHTML("script[src]", func(e *colly.HTMLElement) {
		jsURL := e.Request.AbsoluteURL(e.Attr("src"))
		if sc.isJavaScriptURL(jsURL) {
			utils.Debugf("å‘ç°JSæ–‡ä»¶: %s", jsURL)

			// è®¿é—®JSæ–‡ä»¶URLä»¥ä¸‹è½½
			e.Request.Visit(jsURL)
		}
	})

	// æå–å†…è”scriptæ ‡ç­¾
	sc.collector.OnHTML("script:not([src])", func(e *colly.HTMLElement) {
		// ä¿å­˜å†…è”è„šæœ¬
		content := e.Text
		if len(content) > 100 { // åªä¿å­˜æœ‰å®è´¨å†…å®¹çš„è„šæœ¬
			utils.Debugf("å‘ç°å†…è”è„šæœ¬,é•¿åº¦: %d", len(content))
			// TODO: ä¿å­˜å†…è”è„šæœ¬
		}
	})

	// å¤„ç†å“åº”
	sc.collector.OnResponse(func(r *colly.Response) {
		requestURL := r.Request.URL.String()

		// å¦‚æœæ˜¯JavaScriptæ–‡ä»¶,ä¸‹è½½å¹¶ä¿å­˜
		if sc.isJavaScriptURL(requestURL) {
			sc.downloadJSFile(requestURL, r.Body, r.Headers.Get("Content-Type"))
		}
	})

	// é”™è¯¯å¤„ç†
	sc.collector.OnError(func(r *colly.Response, err error) {
		utils.Errorf("çˆ¬å–é”™è¯¯ [%s]: %v", r.Request.URL, err)
		sc.stats.FailedFiles++
	})

	// è®¿é—®å‰
	sc.collector.OnRequest(func(r *colly.Request) {
		// åº”ç”¨è‡ªå®šä¹‰HTTPå¤´éƒ¨
		if sc.headerProvider != nil {
			headers, err := sc.headerProvider.GetHeaders()
			if err != nil {
				utils.Warnf("è·å–HTTPå¤´éƒ¨å¤±è´¥: %v", err)
			} else {
				for name, values := range headers {
					if len(values) > 0 {
						r.Headers.Set(name, values[0])
					}
				}
			}
		}

		utils.Debugf("è®¿é—®: %s", r.URL.String())
		sc.mu.Lock()
		sc.visitedURLs = append(sc.visitedURLs, r.URL.String())
		sc.stats.VisitedURLs++
		sc.mu.Unlock()
	})
}

// Crawl å¼€å§‹çˆ¬å–
func (sc *StaticCrawler) Crawl(targetURL string) error {
	startTime := time.Now()

	utils.Infof("ğŸ” é™æ€çˆ¬å–æ¨¡å¼å¯åŠ¨")
	utils.Infof("ç›®æ ‡URL: %s", targetURL)
	utils.Infof("æœ€å¤§æ·±åº¦: %d", sc.config.Depth)
	utils.Infof("å¹¶å‘æ•°: %d", sc.config.MaxWorkers)

	// è®¿é—®ç›®æ ‡URL
	if err := sc.collector.Visit(targetURL); err != nil {
		return fmt.Errorf("è®¿é—®ç›®æ ‡URLå¤±è´¥: %w", err)
	}

	// ç­‰å¾…æ‰€æœ‰å¼‚æ­¥è¯·æ±‚å®Œæˆ
	sc.collector.Wait()

	duration := time.Since(startTime)
	sc.stats.Duration = duration.Seconds()

	utils.Infof("âœ… é™æ€çˆ¬å–å®Œæˆ")
	utils.Infof("è®¿é—®URLæ•°: %d", sc.stats.VisitedURLs)
	utils.Infof("ä¸‹è½½æ–‡ä»¶æ•°: %d", sc.stats.StaticFiles)
	utils.Infof("å¤±è´¥æ–‡ä»¶æ•°: %d", sc.stats.FailedFiles)
	utils.Infof("æ€»è€—æ—¶: %.2fç§’", sc.stats.Duration)

	return nil
}

// downloadJSFile ä¸‹è½½å¹¶ä¿å­˜JavaScriptæ–‡ä»¶
// å¤„ç†æµç¨‹:
//  1. æ£€æŸ¥URLæ˜¯å¦å·²ä¸‹è½½ (é¿å…é‡å¤ä¸‹è½½)
//  2. è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
//  3. æ£€æŸ¥å…¨å±€å“ˆå¸Œè¡¨ (è·¨çˆ¬å–å™¨å»é‡)
//  4. æ£€æŸ¥æœ¬åœ°å“ˆå¸Œè¡¨ (çˆ¬å–å™¨å†…å»é‡)
//  5. ç”Ÿæˆæ–‡ä»¶è·¯å¾„å¹¶ä¿å­˜åˆ°ç£ç›˜
//  6. åˆ›å»ºJSFileå…ƒæ•°æ®å¯¹è±¡
//  7. æ·»åŠ åˆ°å…¨å±€å“ˆå¸Œè¡¨
//  8. æ£€æŸ¥å¹¶ä¸‹è½½Source Mapæ–‡ä»¶
//
// å‚æ•°:
//   - fileURL: JavaScriptæ–‡ä»¶çš„å®Œæ•´URL
//   - content: æ–‡ä»¶å†…å®¹ (å­—èŠ‚æ•°ç»„)
//   - contentType: HTTP Content-Typeå¤´éƒ¨
//
// è¿”å›: é”™è¯¯ä¿¡æ¯ (å¦‚æœå¤±è´¥)
func (sc *StaticCrawler) downloadJSFile(fileURL string, content []byte, contentType string) error {
	sc.mu.Lock()
	defer sc.mu.Unlock()

	// æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
	if _, exists := sc.jsFiles[fileURL]; exists {
		utils.Debugf("æ–‡ä»¶å·²å­˜åœ¨,è·³è¿‡: %s", fileURL)
		return nil
	}

	// è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
	hash := calculateHash(content)

	// å…ˆæ£€æŸ¥å…¨å±€å“ˆå¸Œè¡¨(è·¨çˆ¬å–å™¨å»é‡)
	if sc.globalFileHashes != nil && sc.globalMu != nil {
		sc.globalMu.RLock()
		if existingURL, exists := sc.globalFileHashes[hash]; exists {
			sc.globalMu.RUnlock()
			utils.Debugf("å‘ç°å…¨å±€é‡å¤æ–‡ä»¶(å“ˆå¸Œç›¸åŒ): %s (ä¸ %s ç›¸åŒ)", fileURL, existingURL)

			// åˆ›å»ºä¸€ä¸ªæ ‡è®°ä¸ºé‡å¤çš„JSFileå¯¹è±¡,ä½†ä¸ä¿å­˜åˆ°ç£ç›˜
			jsFile := &models.JSFile{
				ID:           uuid.New().String(),
				URL:          fileURL,
				FilePath:     "", // ä¸ä¿å­˜æ–‡ä»¶
				Hash:         hash,
				Size:         int64(len(content)),
				Extension:    filepath.Ext(fileURL),
				ContentType:  contentType,
				SourceURL:    fileURL,
				CrawlMode:    models.ModeStatic,
				Depth:        0,
				IsObfuscated: false,
				IsDuplicate:  true,
				DownloadedAt: time.Now(),
				HasMapFile:   false,
			}
			sc.jsFiles[fileURL] = jsFile
			return nil
		}
		sc.globalMu.RUnlock()
	}

	// æ£€æŸ¥æœ¬åœ°å“ˆå¸Œå»é‡
	for _, existingFile := range sc.jsFiles {
		if existingFile.Hash == hash {
			utils.Debugf("å‘ç°é‡å¤æ–‡ä»¶(å“ˆå¸Œç›¸åŒ): %s", fileURL)
			sc.jsFiles[fileURL] = existingFile
			existingFile.IsDuplicate = true
			return nil
		}
	}

	// ç”Ÿæˆæ–‡ä»¶è·¯å¾„
	filePath, err := sc.generateFilePath(fileURL, "encode/js")
	if err != nil {
		return fmt.Errorf("ç”Ÿæˆæ–‡ä»¶è·¯å¾„å¤±è´¥: %w", err)
	}

	// ç¡®ä¿ç›®å½•å­˜åœ¨
	if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {
		return fmt.Errorf("åˆ›å»ºç›®å½•å¤±è´¥: %w", err)
	}

	// å†™å…¥æ–‡ä»¶
	if err := os.WriteFile(filePath, content, 0644); err != nil {
		return fmt.Errorf("å†™å…¥æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// åˆ›å»ºJSFileå¯¹è±¡
	jsFile := &models.JSFile{
		ID:           uuid.New().String(),
		URL:          fileURL,
		FilePath:     filePath,
		Hash:         hash,
		Size:         int64(len(content)),
		Extension:    filepath.Ext(fileURL),
		ContentType:  contentType,
		SourceURL:    fileURL,
		CrawlMode:    models.ModeStatic,
		Depth:        0, // TODO: è·Ÿè¸ªå®é™…æ·±åº¦
		IsObfuscated: false,
		DownloadedAt: time.Now(),
		HasMapFile:   false,
	}

	sc.jsFiles[fileURL] = jsFile
	sc.stats.StaticFiles++
	sc.stats.TotalFiles++
	sc.stats.TotalSize += int64(len(content))

	// æ·»åŠ åˆ°å…¨å±€å“ˆå¸Œè¡¨
	if sc.globalFileHashes != nil && sc.globalMu != nil {
		sc.globalMu.Lock()
		sc.globalFileHashes[hash] = fileURL
		sc.globalMu.Unlock()
	}

	utils.Infof("ğŸ“¥ ä¸‹è½½æˆåŠŸ: %s (%d bytes)", filepath.Base(filePath), len(content))

	// æ£€æŸ¥æ˜¯å¦æœ‰Source Map
	sc.checkAndDownloadSourceMap(fileURL, content)

	return nil
}

// checkAndDownloadSourceMap æ£€æŸ¥å¹¶ä¸‹è½½Source Mapæ–‡ä»¶
func (sc *StaticCrawler) checkAndDownloadSourceMap(jsURL string, jsContent []byte) {
	// åœ¨æ–‡ä»¶å†…å®¹ä¸­æŸ¥æ‰¾sourceMappingURLæ³¨é‡Š
	content := string(jsContent)

	// æŸ¥æ‰¾ //# sourceMappingURL=xxx.map
	if idx := strings.Index(content, "sourceMappingURL="); idx != -1 {
		start := idx + len("sourceMappingURL=")
		end := strings.IndexAny(content[start:], "\n\r ")
		if end == -1 {
			end = len(content) - start
		}

		mapURL := strings.TrimSpace(content[start : start+end])

		// æ„é€ å®Œæ•´URL
		baseURL, _ := url.Parse(jsURL)
		fullMapURL, err := baseURL.Parse(mapURL)
		if err == nil {
			utils.Infof("ğŸ—ºï¸  å‘ç°Source Map: %s", fullMapURL.String())
			// TODO: ä¸‹è½½Source Mapæ–‡ä»¶
			sc.stats.MapFiles++
		}
	}
}

// isJavaScriptURL åˆ¤æ–­æ˜¯å¦ä¸ºJavaScriptæ–‡ä»¶URL
func (sc *StaticCrawler) isJavaScriptURL(urlStr string) bool {
	urlStr = strings.ToLower(urlStr)

	// æ£€æŸ¥æ‰©å±•å
	for _, ext := range models.JSFileExtensions {
		if strings.HasSuffix(urlStr, ext) {
			return true
		}
	}

	// æ£€æŸ¥å¸¸è§JSæ¨¡å¼
	if strings.Contains(urlStr, ".js?") ||
		strings.Contains(urlStr, ".mjs?") ||
		strings.Contains(urlStr, ".jsx?") {
		return true
	}

	return false
}

// generateFilePath ç”Ÿæˆæœ¬åœ°æ–‡ä»¶è·¯å¾„
func (sc *StaticCrawler) generateFilePath(fileURL string, subdir string) (string, error) {
	parsed, err := url.Parse(fileURL)
	if err != nil {
		return "", err
	}

	// ä½¿ç”¨URLè·¯å¾„ä½œä¸ºæ–‡ä»¶å
	filename := filepath.Base(parsed.Path)
	if filename == "" || filename == "." {
		filename = "index.js"
	}

	// æ„é€ å®Œæ•´è·¯å¾„: output/domain/encode/js/filename
	fullPath := filepath.Join(sc.outputDir, sc.domain, subdir, filename)

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨,æ·»åŠ ç¼–å·
	if _, err := os.Stat(fullPath); err == nil {
		ext := filepath.Ext(filename)
		base := strings.TrimSuffix(filename, ext)
		for i := 1; ; i++ {
			newPath := filepath.Join(sc.outputDir, sc.domain, subdir, fmt.Sprintf("%s_%d%s", base, i, ext))
			if _, err := os.Stat(newPath); os.IsNotExist(err) {
				fullPath = newPath
				break
			}
		}
	}

	return fullPath, nil
}

// calculateHash è®¡ç®—SHA-256å“ˆå¸Œ
func calculateHash(data []byte) string {
	hash := sha256.Sum256(data)
	return fmt.Sprintf("%x", hash)
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (sc *StaticCrawler) GetStats() models.TaskStats {
	sc.mu.RLock()
	defer sc.mu.RUnlock()
	return sc.stats
}

// GetJSFiles è·å–æ‰€æœ‰ä¸‹è½½çš„JSæ–‡ä»¶
func (sc *StaticCrawler) GetJSFiles() []*models.JSFile {
	sc.mu.RLock()
	defer sc.mu.RUnlock()

	files := make([]*models.JSFile, 0, len(sc.jsFiles))
	for _, f := range sc.jsFiles {
		files = append(files, f)
	}
	return files
}

// calculateOptimalWorkers åŠ¨æ€è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
// æ ¹æ®CPUæ ¸å¿ƒæ•°å’Œé…ç½®å€¼æ™ºèƒ½è°ƒæ•´å¹¶å‘æ•°
// ç­–ç•¥:
//   - å•æ ¸/åŒæ ¸: ä½¿ç”¨é…ç½®å€¼ (é¿å…è¿‡åº¦å¹¶å‘)
//   - 4æ ¸åŠä»¥ä¸Š: é…ç½®å€¼ * min(CPUæ ¸å¿ƒæ•°/2, 4)
//   - æœ€å¤§ä¸è¶…è¿‡ é…ç½®å€¼ * 4
func calculateOptimalWorkers(configWorkers int) int {
	numCPU := runtime.NumCPU()

	// åŸºç¡€å€¼
	baseWorkers := configWorkers
	if baseWorkers < 1 {
		baseWorkers = 2 // é»˜è®¤æœ€å°å¹¶å‘
	}

	// æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
	switch {
	case numCPU <= 2:
		// ä½æ ¸å¿ƒæœºå™¨: ä¿æŒé…ç½®å€¼
		return baseWorkers
	case numCPU <= 4:
		// ä¸­ç­‰æ ¸å¿ƒ: é…ç½®å€¼ * 2
		return baseWorkers * 2
	case numCPU <= 8:
		// å¤šæ ¸: é…ç½®å€¼ * 3
		return baseWorkers * 3
	default:
		// é«˜æ ¸å¿ƒ: é…ç½®å€¼ * 4 (é¿å…è¿‡åº¦å¹¶å‘)
		return baseWorkers * 4
	}
}
