package core

import (
	"fmt"
	"net/url"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/RecoveryAshes/JsFIndcrack/internal/crawlers"
	"github.com/RecoveryAshes/JsFIndcrack/internal/models"
	"github.com/RecoveryAshes/JsFIndcrack/internal/utils"
)

// Crawler ä¸»çˆ¬å–å™¨åè°ƒå™¨
type Crawler struct {
	config    models.CrawlConfig
	targetURL string
	domain    string
	outputDir string
	mode      string

	// HTTPå¤´éƒ¨æä¾›è€…
	headerProvider models.HeaderProvider

	// çˆ¬å–å™¨å®ä¾‹
	staticCrawler  *crawlers.StaticCrawler
	dynamicCrawler *crawlers.DynamicCrawler

	// åæ··æ·†å™¨
	deobfuscator *Deobfuscator

	// å…¨å±€æ–‡ä»¶å»é‡(è·¨æ¨¡å¼)
	fileHashes map[string]string // hash -> URL
	mu         sync.RWMutex

	// ç»Ÿè®¡ä¿¡æ¯
	stats models.TaskStats
}

// NewCrawler åˆ›å»ºä¸»çˆ¬å–å™¨
func NewCrawler(targetURL string, config models.CrawlConfig, outputDir string, mode string, headerProvider models.HeaderProvider) (*Crawler, error) {
	// è§£æURLè·å–åŸŸå
	parsedURL, err := url.Parse(targetURL)
	if err != nil {
		return nil, fmt.Errorf("è§£æURLå¤±è´¥: %w", err)
	}

	domain := parsedURL.Host
	if domain == "" {
		return nil, fmt.Errorf("æ— æ³•ä»URLä¸­æå–åŸŸå: %s", targetURL)
	}

	return &Crawler{
		config:         config,
		targetURL:      targetURL,
		domain:         domain,
		outputDir:      outputDir,
		mode:           mode,
		headerProvider: headerProvider,
		deobfuscator:   NewDeobfuscator(),
		fileHashes:     make(map[string]string),
		stats:          models.TaskStats{},
	}, nil
}

// Crawl æ‰§è¡Œçˆ¬å–ä»»åŠ¡
// æ‰§è¡Œæµç¨‹:
//  1. åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
//  2. æ ¹æ®æ¨¡å¼æ‰§è¡Œçˆ¬å– (static/dynamic/all)
//  3. åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
//  4. æ‰§è¡Œåæ··æ·†å¤„ç†
//  5. ç”Ÿæˆçˆ¬å–æŠ¥å‘Š
//
// è¿”å›: é”™è¯¯ä¿¡æ¯ (å¦‚æœå¤±è´¥)
func (c *Crawler) Crawl() error {
	startTime := time.Now()

	utils.Infof("ğŸš€ å¼€å§‹çˆ¬å–ä»»åŠ¡")
	utils.Infof("ç›®æ ‡URL: %s", c.targetURL)
	utils.Infof("åŸŸå: %s", c.domain)
	utils.Infof("çˆ¬å–æ¨¡å¼: %s", c.mode)
	utils.Infof("è¾“å‡ºç›®å½•: %s", c.outputDir)

	// åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
	if err := c.setupOutputDirectories(); err != nil {
		return fmt.Errorf("åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: %w", err)
	}

	// æ ¹æ®æ¨¡å¼æ‰§è¡Œçˆ¬å–
	switch c.mode {
	case "static":
		if err := c.runStaticCrawl(); err != nil {
			return err
		}
	case "dynamic":
		if err := c.runDynamicCrawl(); err != nil {
			return err
		}
	case "all":
		// å…ˆé™æ€ååŠ¨æ€,é™æ€å¤±è´¥ä¸å½±å“åŠ¨æ€çˆ¬å–
		if err := c.runStaticCrawl(); err != nil {
			utils.Warnf("é™æ€çˆ¬å–å¤±è´¥,ç»§ç»­åŠ¨æ€çˆ¬å–: %v", err)
		}
		if err := c.runDynamicCrawl(); err != nil {
			utils.Warnf("åŠ¨æ€çˆ¬å–å¤±è´¥: %v", err)
		}
	default:
		return fmt.Errorf("æ— æ•ˆçš„çˆ¬å–æ¨¡å¼: %s", c.mode)
	}

	// åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
	c.mergeStats()

	// æ‰§è¡Œåæ··æ·†
	allFiles := c.GetAllFiles()
	if len(allFiles) > 0 {
		utils.Infof("ğŸ”§ å¼€å§‹åæ··æ·†å¤„ç†...")
		successCount, failCount, err := c.deobfuscator.DeobfuscateAll(allFiles, c.outputDir)
		if err != nil {
			utils.Warnf("åæ··æ·†è¿‡ç¨‹å‡ºç°é”™è¯¯: %v", err)
		}
		utils.Infof("âœ… åæ··æ·†å®Œæˆ: æˆåŠŸ %d, å¤±è´¥ %d", successCount, failCount)
	}

	duration := time.Since(startTime)
	c.stats.Duration = duration.Seconds()

	// ç”Ÿæˆçˆ¬å–æŠ¥å‘Š
	reporter := utils.NewReporter(c.outputDir, c.domain)
	if err := reporter.GenerateReport(c.targetURL, c.stats, allFiles, []string{}, c.config); err != nil {
		utils.Warnf("ç”ŸæˆæŠ¥å‘Šå¤±è´¥: %v", err)
	}

	utils.Infof("âœ… çˆ¬å–ä»»åŠ¡å®Œæˆ")
	utils.Infof("æ€»æ–‡ä»¶æ•°: %d", c.stats.TotalFiles)
	utils.Infof("æ€»è€—æ—¶: %.2fç§’", c.stats.Duration)

	return nil
}

// setupOutputDirectories åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
func (c *Crawler) setupOutputDirectories() error {
	// ä¸»è¾“å‡ºç›®å½•: output/domain/
	basePath := filepath.Join(c.outputDir, c.domain)

	// åˆ›å»ºå­ç›®å½•ç»“æ„
	dirs := []string{
		filepath.Join(basePath, "encode", "js"),  // æ··æ·†JSæ–‡ä»¶
		filepath.Join(basePath, "encode", "map"), // Source Mapæ–‡ä»¶
		filepath.Join(basePath, "decode", "js"),  // åæ··æ·†JSæ–‡ä»¶
		filepath.Join(basePath, "similarity"),    // ç›¸ä¼¼åº¦åˆ†æç»“æœ
		filepath.Join(basePath, "reports"),       // æŠ¥å‘Šæ–‡ä»¶
		filepath.Join(basePath, "checkpoints"),   // æ£€æŸ¥ç‚¹æ–‡ä»¶
	}

	for _, dir := range dirs {
		if err := os.MkdirAll(dir, 0755); err != nil {
			return fmt.Errorf("åˆ›å»ºç›®å½•å¤±è´¥ [%s]: %w", dir, err)
		}
		utils.Debugf("åˆ›å»ºç›®å½•: %s", dir)
	}

	utils.Infof("âœ… è¾“å‡ºç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ: %s", basePath)
	return nil
}

// runStaticCrawl æ‰§è¡Œé™æ€çˆ¬å–
func (c *Crawler) runStaticCrawl() error {
	utils.Infof("ğŸ” é™æ€çˆ¬å–æ¨¡å¼å¯åŠ¨")

	c.staticCrawler = crawlers.NewStaticCrawler(c.config, c.outputDir, c.domain, c.fileHashes, &c.mu, c.headerProvider)

	if err := c.staticCrawler.Crawl(c.targetURL); err != nil {
		return fmt.Errorf("é™æ€çˆ¬å–å¤±è´¥: %w", err)
	}

	// æ³¨æ„: æ–‡ä»¶å“ˆå¸Œå·²åœ¨çˆ¬å–è¿‡ç¨‹ä¸­æ·»åŠ åˆ°å…¨å±€å“ˆå¸Œè¡¨

	utils.Infof("âœ… é™æ€çˆ¬å–å®Œæˆ")
	return nil
}

// runDynamicCrawl æ‰§è¡ŒåŠ¨æ€çˆ¬å–
func (c *Crawler) runDynamicCrawl() error {
	utils.Infof("ğŸŒ åŠ¨æ€çˆ¬å–æ¨¡å¼å¯åŠ¨")

	c.dynamicCrawler = crawlers.NewDynamicCrawler(c.config, c.outputDir, c.domain, c.fileHashes, &c.mu, c.headerProvider)

	if err := c.dynamicCrawler.Crawl(c.targetURL); err != nil {
		return fmt.Errorf("åŠ¨æ€çˆ¬å–å¤±è´¥: %w", err)
	}

	// æ³¨æ„: æ–‡ä»¶å“ˆå¸Œå·²åœ¨çˆ¬å–è¿‡ç¨‹ä¸­æ·»åŠ åˆ°å…¨å±€å“ˆå¸Œè¡¨
	// è·¨æ¨¡å¼å»é‡å·²åœ¨åŠ¨æ€çˆ¬å–å™¨çš„downloadJSFileä¸­å®Œæˆ,ä¸éœ€è¦é¢å¤–å¤„ç†

	utils.Infof("âœ… åŠ¨æ€çˆ¬å–å®Œæˆ")
	return nil
}

// updateFileHashes æ›´æ–°å…¨å±€æ–‡ä»¶å“ˆå¸Œè¡¨
func (c *Crawler) updateFileHashes(files []*models.JSFile) {
	c.mu.Lock()
	defer c.mu.Unlock()

	for _, file := range files {
		if file.IsDuplicate {
			continue // è·³è¿‡å·²æ ‡è®°ä¸ºé‡å¤çš„æ–‡ä»¶
		}

		// æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå“ˆå¸Œ
		if existingURL, exists := c.fileHashes[file.Hash]; exists {
			utils.Debugf("å‘ç°é‡å¤æ–‡ä»¶: %s (ä¸ %s ç›¸åŒ)", file.URL, existingURL)
			file.IsDuplicate = true
		} else {
			c.fileHashes[file.Hash] = file.URL
		}
	}
}

// performCrossModeDedupe æ‰§è¡Œè·¨æ¨¡å¼å»é‡
func (c *Crawler) performCrossModeDedupe() {
	c.mu.Lock()
	defer c.mu.Unlock()

	staticFiles := c.staticCrawler.GetJSFiles()
	dynamicFiles := c.dynamicCrawler.GetJSFiles()

	duplicateCount := 0

	// æ£€æŸ¥åŠ¨æ€çˆ¬å–çš„æ–‡ä»¶æ˜¯å¦ä¸é™æ€çˆ¬å–é‡å¤
	for _, dynFile := range dynamicFiles {
		for _, staticFile := range staticFiles {
			if dynFile.Hash == staticFile.Hash && !dynFile.IsDuplicate {
				utils.Debugf("è·¨æ¨¡å¼é‡å¤: %s (åŠ¨æ€) == %s (é™æ€)", dynFile.URL, staticFile.URL)
				dynFile.IsDuplicate = true
				duplicateCount++

				// åˆ é™¤é‡å¤çš„åŠ¨æ€çˆ¬å–æ–‡ä»¶
				if err := os.Remove(dynFile.FilePath); err != nil {
					utils.Warnf("åˆ é™¤é‡å¤æ–‡ä»¶å¤±è´¥ [%s]: %v", dynFile.FilePath, err)
				} else {
					utils.Debugf("å·²åˆ é™¤é‡å¤æ–‡ä»¶: %s", dynFile.FilePath)
				}
				break
			}
		}
	}

	if duplicateCount > 0 {
		utils.Infof("ğŸ”„ è·¨æ¨¡å¼å»é‡: åˆ é™¤äº† %d ä¸ªé‡å¤æ–‡ä»¶", duplicateCount)
	}
}

// mergeStats åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
func (c *Crawler) mergeStats() {
	c.mu.Lock()
	defer c.mu.Unlock()

	if c.staticCrawler != nil {
		staticStats := c.staticCrawler.GetStats()
		c.stats.VisitedURLs += staticStats.VisitedURLs
		c.stats.StaticFiles = staticStats.StaticFiles
		c.stats.TotalFiles += staticStats.StaticFiles
		c.stats.TotalSize += staticStats.TotalSize
		c.stats.FailedFiles += staticStats.FailedFiles
		c.stats.MapFiles += staticStats.MapFiles
	}

	if c.dynamicCrawler != nil {
		dynamicStats := c.dynamicCrawler.GetStats()
		c.stats.VisitedURLs += dynamicStats.VisitedURLs
		c.stats.DynamicFiles = dynamicStats.DynamicFiles
		c.stats.TotalFiles += dynamicStats.DynamicFiles
		c.stats.TotalSize += dynamicStats.TotalSize
		c.stats.FailedFiles += dynamicStats.FailedFiles
		c.stats.MapFiles += dynamicStats.MapFiles
	}

	// å»é™¤é‡å¤URLè®¡æ•°
	uniqueURLs := make(map[string]bool)
	if c.staticCrawler != nil {
		for _, file := range c.staticCrawler.GetJSFiles() {
			if !file.IsDuplicate {
				uniqueURLs[file.URL] = true
			}
		}
	}
	if c.dynamicCrawler != nil {
		for _, file := range c.dynamicCrawler.GetJSFiles() {
			if !file.IsDuplicate {
				uniqueURLs[file.URL] = true
			}
		}
	}

	c.stats.TotalFiles = len(uniqueURLs)
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (c *Crawler) GetStats() models.TaskStats {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return c.stats
}

// GetAllFiles è·å–æ‰€æœ‰ä¸‹è½½çš„æ–‡ä»¶(å»é‡å)
func (c *Crawler) GetAllFiles() []*models.JSFile {
	c.mu.RLock()
	defer c.mu.RUnlock()

	allFiles := make([]*models.JSFile, 0)

	if c.staticCrawler != nil {
		for _, file := range c.staticCrawler.GetJSFiles() {
			if !file.IsDuplicate {
				allFiles = append(allFiles, file)
			}
		}
	}

	if c.dynamicCrawler != nil {
		for _, file := range c.dynamicCrawler.GetJSFiles() {
			if !file.IsDuplicate {
				allFiles = append(allFiles, file)
			}
		}
	}

	return allFiles
}

// GetOutputDir è·å–è¾“å‡ºç›®å½•è·¯å¾„
func (c *Crawler) GetOutputDir() string {
	return filepath.Join(c.outputDir, c.domain)
}
